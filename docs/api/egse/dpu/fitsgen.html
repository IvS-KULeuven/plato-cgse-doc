<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>egse.dpu.fitsgen API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>egse.dpu.fitsgen</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import glob
import logging
import multiprocessing
import os
import pickle
import threading
from datetime import timedelta, datetime
from enum import Enum
from pathlib import Path, PosixPath
from typing import List, Mapping

import click
import invoke
import natsort
import numpy as np
import rich
import sys
import zmq
from astropy.io import fits
from h5py import File
from h5py._hl.attrs import AttributeManager
from itertools import chain
from persistqueue import Queue
from scipy.interpolate import interp1d

from egse import h5
from egse.config import find_file, find_files
from egse.control import time_in_ms
from egse.dpu import DPUMonitoring, get_expected_last_packet_flags
from egse.dpu.dpu_cs import is_dpu_cs_active
from egse.exceptions import Abort
from egse.fee import convert_ccd_order_value
from egse.fee import n_fee_mode, fee_side
from egse.fee.nfee import HousekeepingData
from egse.hk import get_housekeeping, HKError
from egse.obsid import ObservationIdentifier, LAB_SETUP_TEST, TEST_LAB, TEST_LAB_SETUP, obsid_from_storage
from egse.reg import RegisterMap
from egse.settings import Settings
from egse.setup import load_setup, Setup
from egse.spw import SpaceWirePacket
from egse.state import GlobalState
from egse.storage import is_storage_manager_active
from egse.storage.persistence import FITS, HDF5
from egse.synoptics import get_synoptics_table
from egse.system import time_since_epoch_1958, format_datetime, read_last_line
from egse.zmq_ser import bind_address, connect_address

LOGGER = logging.getLogger(__name__)

N_FEE_SETTINGS = Settings.load(&#34;N-FEE&#34;)
CCD_SETTINGS = Settings.load(&#34;CCD&#34;)
SITE = Settings.load(&#34;SITE&#34;)
CTRL_SETTINGS = Settings.load(&#34;FITS Generator Control Server&#34;)
STORAGE_SETTINGS = Settings.load(&#34;Storage Control Server&#34;)
DPU_SETTINGS = Settings.load(&#34;DPU&#34;)

TIMEOUT_RECV = 1.0  # seconds


def get_cycle_time(n_fee_state: Mapping, obsid=None, data_dir=None):
    &#34;&#34;&#34; Return the image cycle time.

    In the given N-FEE state parameters or register map, we check whether we are in internal or external sync:

        - Internal sync: Read the image cycle time from the given N-FEE state parameters or register map;
        - External sync: Get the image cycle time from the AEU (AWG2).  In case of off-line FITS generation (i.e. from
          the HDF5 files), the image cycle time (for the specified obsid) is taken from the AEU housekeeping (AWG2).
          In case of on-line FITS generation, the image cycle time is queried from the AEU AWG2.

    Args:
        - n_fee_state: N-FEE state parameters or register map.
        - obsid: Observation identifier for which the image cycle time is read from the AEU housekeeping.

    Returns: Image cycle time [s].
    &#34;&#34;&#34;

    # Internal sync -&gt; use sync period from the N-FEE state

    if n_fee_state[&#34;sync_sel&#34;] == 1:
        return n_fee_state[&#34;int_sync_period&#34;] / 1000.    # [ms] -&gt; [s]

    # External sync -&gt; use AEU sync pulses

    else:
        if obsid:
            try:
                return float(get_housekeeping(&#34;GAEU_EXT_CYCLE_TIME&#34;, obsid=obsid, data_dir=data_dir)[1])
            except HKError as exc:  # See GitHub issue #2025
                LOGGER.warning(&#34;No HK available for AWG2 (using default cycle time of 25s)&#34;, exc)
                return 25.0
        else:
            return None


def get_cgse_version(obsid=None, data_dir=None):
    &#34;&#34;&#34; Returns the version of the Common EGSE with which the FITS file was created.

    Args:
        - obsid: Observation identifier for which the version of the Common EGSE is read from the Configuration
                 Manager housekeeping.
    &#34;&#34;&#34;

    try:
        return None if obsid is None else get_housekeeping(&#34;CM_CGSE_VERSION&#34;, obsid=obsid, data_dir=data_dir)[1]
    except HKError:
        return None


class FITSGenerator:

    def __init__(self):
        &#34;&#34;&#34; Generation of FITS files from HDF5 files with SpW packets.

        In a separate thread, the DPU monitoring puts the name of new HDF5 files with SpW packets in the queue.  The
        FITS generator accesses this queue (FIFO) and stores the information in a FITS file.

        When there is a change in crucial parameters, the current FITS file (if any) will be closed and a new one will
        be created as soon as data packet start coming in (when the N-FEE is in full-image or full-image pattern mode).
        &#34;&#34;&#34;

        # Queue with the full path of the HDF5 files that still need to be processed.

        self.hdf5_filename_queue = Queue(f&#34;{os.environ[&#39;PLATO_DATA_STORAGE_LOCATION&#39;]}/{DPU_SETTINGS[&#39;HDF5_QUEUE&#39;]}&#34;)

        # Name of the FITS file currently being written
        # (None if the N-FEE is not in full-image mode or in full-image pattern mode)

        self.fits_images_filename = None
        self.fits_cube_filename = None

        # Name of the HDF5 file currently being processed

        self.hdf5_filename = None

        # The last obsid that was/is being processed

        self.last_processed_obsid = None

        # Keep track of what was the N-FEE mode and what were the crucial parameters at the previous long pulse
        # (When we have checked whether a change has been detected, these values will be overwritten with the new ones)

        self.ccd_mode_config = None
        self.v_start = None
        self.v_end = None
        self.h_end = None
        self.ccd_readout_order = None
        # self.sensor_sel = None
        self.rows_final_dump = None
        self.setup = GlobalState.setup

        self.config_slicing_num_cycles = 0   # Configured slicing parameter
        self.processed_num_cycles = 0           # HDF5 files with image data processed for current FITS file

        self.zcontext = zmq.Context.instance()
        self.monitoring_socket = self.zcontext.socket(zmq.PUB)
        self.monitoring_socket.bind(bind_address(CTRL_SETTINGS.PROTOCOL, CTRL_SETTINGS.MONITORING_PORT,))

        # self._quit_event = multiprocessing.Event()

        self.keep_processing_queue = True

        # The DPU monitoring should populate the queue in a separate thread

        self.dpu_monitoring_thread = threading.Thread(target=self.fill_queue)
        self.dpu_monitoring_thread.daemon = True
        self.dpu_monitoring_thread.start()

        # Processing the content of the queue should be done in a separate thread

        self.process_queue_thread = threading.Thread(target=self.process_queue)
        self.process_queue_thread.daemon = True
        self.process_queue_thread.start()

    def fill_queue(self):
        &#34;&#34;&#34; The DPU monitoring fills the queue.

        Each time an HDF5 file with SpW packets is closed, the DPU monitoring puts the full path of this file on the
        queue.
        &#34;&#34;&#34;

        while self.keep_processing_queue:

            with DPUMonitoring() as dpu_monitoring:

                while self.keep_processing_queue:

                    try:
                        hdf5_filename = dpu_monitoring.wait_for_hdf5_filename()
                        self.hdf5_filename_queue.put(hdf5_filename)

                    # If the DPU monitoring times out, try to re-establish a connection
                    # (in the future, we want to replace this with something like dpu_monitoring_reconnect())

                    except TimeoutError:
                        LOGGER.warning(&#34;DPU monitoring timeout&#34;)
                        break

    def run(self, commander, poller):
        &#34;&#34;&#34; Process the content of the queue.

        When there is a filename in the queue, take it from the queue:

            - If there is a change in crucial parameters, close the current FITS file (if any).
            - If there is a change in crucial parameter and the N-FEE is in full-image mode or in full-image pattern
              mode, or the N-FEE goes to full-image mode or full-image pattern mode, a new FITS file will be created.
            - The content of the HDF5 files will be extracted and passed to the FITS persistence layer as SpW packets.
        &#34;&#34;&#34;

        last_time = time_in_ms()

        try:
            while True:

                if _check_commander_status(commander, poller):

                    self.keep_processing_queue = False
                    break

                if time_in_ms() - last_time &gt;= 1000:
                    last_time = time_in_ms()

                    monitoring_info = {&#34;hdf5&#34;: self.hdf5_filename,
                                       &#34;fits&#34;: self.fits_cube_filename or self.fits_images_filename,
                                       &#34;last obsid (being) processed&#34;: self.last_processed_obsid}
                    pickle_string = pickle.dumps(monitoring_info)
                    self.monitoring_socket.send(pickle_string)

        except KeyboardInterrupt:
            click.echo(&#34;KeyboardInterrupt caught!&#34;)
            self.keep_processing_queue = False

        # Keyboard interrupt or stop command

        LOGGER.info(&#34;Shutting down FITS generation&#34;)
        commander.close(linger=0)
        # self.dpu_monitoring_thread.join()
        # self.process_queue_thread.join()

        self.monitoring_socket.close()
        # self.zcontext.term()

    def process_queue(self):

        location = os.environ.get(&#39;PLATO_DATA_STORAGE_LOCATION&#39;)

        previous_obsid = None

        while self.keep_processing_queue:

            # There is an HDF5 file ready for processing

            if not self.hdf5_filename_queue.empty():

                try:

                    # Get the first item in the queue (FIFO) and open it

                    hdf5_filename = self.hdf5_filename_queue.get()[0]
                    self.hdf5_filename = hdf5_filename

                    LOGGER.info(f&#34;Processing file {hdf5_filename}&#34;)

                    with h5.get_file(hdf5_filename, mode=&#34;r&#34;, locking=False) as hdf5_file:

                        LOGGER.info(f&#34;Opened file {hdf5_filename}&#34;)

                        # Check whether there is data in the HDF5
                        # (if there is no data in the HDF5 file, nothing has to be done and you can go to the next file)

                        try:

                            # Slicing

                            try:
                                slicing_num_cycles = hdf5_file[&#34;dpu&#34;].attrs[&#34;slicing_num_cycles&#34;]
                                if slicing_num_cycles != self.config_slicing_num_cycles:
                                    LOGGER.debug(f&#34;Slicing parameter changed: {self.config_slicing_num_cycles} &#34;
                                                 f&#34;-&gt; {slicing_num_cycles}&#34;)
                                    self.close_fits()
                                    self.config_slicing_num_cycles = slicing_num_cycles
                            except KeyError:
                                self.config_slicing_num_cycles = 0
                                LOGGER.debug(&#34;No slicing&#34;)

                            # Obsid

                            try:
                                obsid = hdf5_file[&#34;obsid&#34;][()].decode()
                                obsid = ObservationIdentifier.create_from_string(obsid, order=LAB_SETUP_TEST)

                                self.last_processed_obsid = obsid_from_storage(obsid, data_dir=location)
                            except (KeyError, ValueError):
                                # Either the obsid is not included in the HDF5 file or it is None
                                obsid = None
                            except AttributeError:
                                LOGGER.error(f&#34;No data present for obsid {str(obsid)} in the obs folder&#34;)
                                self.keep_processing_queue = False
                                break

                            # Loop over all groups in the current HDF5 file and check whether the &#34;data&#34; group is
                            # present

                            has_data = False

                            for group in h5.groups(hdf5_file):

                                if &#34;data&#34; in group.keys():

                                    has_data = True

                                    n_fee_state = group[&#34;data&#34;].attrs

                                    # Check whether there is a change in crucial parameters or in the N-FEE mode

                                    if self.crucial_parameter_change(n_fee_state):

                                        self.close_fits()

                                    if in_data_acquisition_mode(n_fee_state):

                                        if self.fits_images_filename is None:

                                            # Start writing to a new FITS file

                                            self.fits_images_filename = construct_images_filename(hdf5_filename, obsid)

                                            ccd_readout_order = convert_ccd_order_value(self.ccd_readout_order)

                                            prep = {
                                                &#34;v_start&#34;: self.v_start,
                                                &#34;v_end&#34;: self.v_end,
                                                &#34;h_end&#34;: self.h_end,
                                                &#34;rows_final_dump&#34;: self.rows_final_dump,
                                                &#34;ccd_mode_config&#34;: self.ccd_mode_config,
                                                &#34;ccd_readout_order&#34;: ccd_readout_order,  # CCD numbering [1-4]
                                                &#34;expected_last_packet_flags&#34;: get_expected_last_packet_flags(n_fee_state),
                                                &#34;obsid&#34;: str(obsid),
                                                &#34;cycle_time&#34;: get_cycle_time(n_fee_state, obsid=obsid),
                                                &#34;cgse_version&#34;: get_cgse_version(obsid=obsid),
                                                &#34;setup&#34;: self.setup
                                            }

                                            persistence = FITS(self.fits_images_filename, prep)
                                            persistence.open()

                                        # See https://github.com/IvS-KULeuven/plato-common-egse/issues/901
                                        # timecode = group[&#34;timecode&#34;]
                                        # spw_packet = SpaceWirePacket.create_packet(h5.get_data(timecode))

                                        timestamp = group[&#34;timecode&#34;].attrs[&#34;timestamp&#34;]
                                        persistence.create({&#34;Timestamp&#34;: timestamp})

                                        data = group[&#34;data&#34;]
                                        sorted_datasets = natsort.natsorted(data.items(), key=lambda x: x[0])

                                        persistence.expected_last_packet_flags = get_expected_last_packet_flags(
                                            n_fee_state)

                                        for identifier, dataset in sorted_datasets:

                                            spw_packet = SpaceWirePacket.create_packet(h5.get_data(dataset))
                                            persistence.create({f&#34;SpW packet {identifier}&#34;: spw_packet})

                            if has_data:
                                self.processed_num_cycles += 1

                                if self.config_slicing_num_cycles != 0 and \
                                        self.processed_num_cycles == self.config_slicing_num_cycles:
                                    self.close_fits()

                            else:

                                self.close_fits()
                                self.clear_crucial_parameters()

                            # When the previous HDF5 file still pertained to an observation and the current one doesn&#39;t,
                            # it means that the observation has just finished and all FITS files have been generated. It
                            # is only at this point that the synoptics can be included in the FITS headers.

                            if previous_obsid is not None and obsid is None:
                                add_synoptics(previous_obsid)
                                previous_obsid = obsid

                        except KeyError:
                            LOGGER.debug(&#34;KeyError occurred when accessing data in all groups of the HDF5 file.&#34;)

                except IndexError:
                    LOGGER.debug(&#34;Queue contained an emtpy entry&#34;)
                except RuntimeError as exc:
                    LOGGER.debug(f&#34;Unable to open HDF5 file: {exc}&#34;)

    def clear_crucial_parameters(self):
        &#34;&#34;&#34; Clear the crucial parameters.&#34;&#34;&#34;

        self.v_start = None
        self.v_end = None
        self.h_end = None
        self.rows_final_dump = None
        self.ccd_readout_order = None
        self.ccd_mode_config = None

    def close_fits(self):

        if self.fits_images_filename is not None:

            self.fits_cube_filename = construct_cube_filename(self.fits_images_filename)
            convert_to_cubes(self.fits_images_filename)
            self.fits_cube_filename = None

            # Stop writing to the current FITS file

            self.fits_images_filename = None

            # Reset the number of HDF5 files with image data processed for current FITS file

            self.processed_num_cycles = 0

    def crucial_parameter_change(self, n_fee_state: AttributeManager):
        &#34;&#34;&#34; Check for a change in crucial parameters.

        Crucial parameters are:

            - ccd_mode_config: readout mode;
            - v_start (int) and v_end(int): index of the first and the last row being transmitted;
            - h_end (int): index of the last serial readout of the readout register;
            - ccd_readout_order: CCDs that will be read out;
            # - sensor_sel: which  side(s) of the CCD(s) that will be read out;

        Args:
            - n_fee_stage: N-FEE stae parameters.

        Returns: True if a change in crucial parameters has been detected; False otherwise.
        &#34;&#34;&#34;

        ccd_mode_config = n_fee_state[&#34;ccd_mode_config&#34;]
        v_start = n_fee_state[&#34;v_start&#34;]
        v_end = n_fee_state[&#34;v_end&#34;]
        h_end = n_fee_state[&#34;h_end&#34;]
        ccd_readout_order = n_fee_state[&#34;ccd_readout_order&#34;]
        rows_final_dump = n_fee_state[&#34;n_final_dump&#34;]

        crucial_parameter_change = False

        if v_start != self.v_start:

            LOGGER.info(f&#34;Change in v_start: {self.v_start} -&gt; {v_start}&#34;)

            self.v_start = v_start
            crucial_parameter_change = True

        if v_end != self.v_end:

            LOGGER.info(f&#34;Change in v_end: {self.v_end} -&gt; {v_end}&#34;)

            self.v_end = v_end
            crucial_parameter_change = True

        if h_end != self.h_end:

            LOGGER.info(f&#34;Change in h_end: {self.h_end} -&gt; {h_end}&#34;)

            self.h_end = h_end
            crucial_parameter_change = True

        if rows_final_dump != self.rows_final_dump:

            LOGGER.info(f&#34;Change in rows_final_dump: {self.rows_final_dump} -&gt; {rows_final_dump}&#34;)

            self.rows_final_dump = rows_final_dump
            crucial_parameter_change = True

        if ccd_readout_order != self.ccd_readout_order:

            LOGGER.info(f&#34;Change in ccd_readout_order: {self.ccd_readout_order} -&gt; {ccd_readout_order}&#34;)

            self.ccd_readout_order = ccd_readout_order
            crucial_parameter_change = True

        if ccd_mode_config != self.ccd_mode_config:

            LOGGER.info(f&#34;Change in ccd_mode_config: {self.ccd_mode_config} -&gt; {ccd_mode_config}&#34;)

            self.ccd_mode_config = ccd_mode_config
            crucial_parameter_change = True

        return crucial_parameter_change


def convert_to_cubes(filename):
    &#34;&#34;&#34; Conversion of level-1 FITS files to level-2 FITS files.

    After the conversion, the flat-structure FITS file is removed.

    Args:
        - filename: Full path of the level-1 FITS file.
    &#34;&#34;&#34;

    cube_filename = construct_cube_filename(filename)
    LOGGER.info(f&#34;Converting to {cube_filename}&#34;)

    with fits.open(filename) as level1:

        primary_header = level1[&#34;PRIMARY&#34;].header

        selected_ccds = np.unique(primary_header[&#34;CCD_READOUT_ORDER&#34;][1:-1].split(&#34;, &#34;))  # str
        side_is_present = {ccd: {fee_side.E: 0, fee_side.F: 0} for ccd in selected_ccds}

        has_serial_overscan = primary_header[&#34;H_END&#34;] &gt;= \
            CCD_SETTINGS.LENGTH_SERIAL_PRESCAN + CCD_SETTINGS.NUM_COLUMNS // 2
        has_parallel_overscan = primary_header[&#34;V_END&#34;] &gt;= CCD_SETTINGS.NUM_ROWS

        # We are going to calculate the relative time since the very first exposure in the FITS file.  We don&#39;t know
        # here which CCD side of which CCD came in first, so we determine the start time here.

        start_time = time_since_epoch_1958(format_datetime(precision=6, width=9))   # Now (data will certainly be older)
        date_obs = None

        for ccd_number in selected_ccds:

            for ccd_side in fee_side:

                try:

                    time = level1[f&#34;IMAGE_{ccd_number}_{ccd_side.name[0]}&#34;, 0].header[&#34;DATE-OBS&#34;]

                    if time_since_epoch_1958(time) &lt; start_time:

                        start_time = time_since_epoch_1958(time)
                        date_obs = time

                    side_is_present[ccd_number][ccd_side] = True

                except KeyError:

                    side_is_present[ccd_number][ccd_side] = False

        primary_hdu = fits.PrimaryHDU()
        primary_header[&#34;DATE-OBS&#34;] = date_obs
        primary_header[&#34;LEVEL&#34;] = 2  # Cube structure
        primary_hdu.header = primary_header
        primary_hdu.writeto(cube_filename)

        for ccd_number in selected_ccds:

            for ccd_side in fee_side:

                if side_is_present[ccd_number][ccd_side]:

                    # Image

                    images = []
                    time_axis = np.array([])

                    exposure = 0

                    while True:

                        try:

                            slice = level1[f&#34;IMAGE_{ccd_number}_{ccd_side.name[0]}&#34;, exposure]

                            time = time_since_epoch_1958(slice.header[&#34;DATE-OBS&#34;])
                            time_axis = np.append(time_axis, time)

                            images.append(slice.data)

                            exposure += 1

                        except KeyError:

                            break

                    image_cube = np.stack(images)
                    del images

                    time_axis -= start_time
                    time_column = fits.Column(&#34;TIME&#34;, format=&#34;F&#34;, array=time_axis)
                    time_table = fits.BinTableHDU.from_columns([time_column])
                    time_table.header[&#34;EXTNAME&#34;] = f&#34;WCS-TAB_{ccd_number}_{ccd_side.name[0]}&#34;

                    fits.append(cube_filename, time_table.data, time_table.header)
                    fits.append(filename, time_table.data, time_table.header)

                    image_cube_header = level1[f&#34;IMAGE_{ccd_number}_{ccd_side.name[0]}&#34;, 0].header
                    image_cube_header[&#34;NAXIS&#34;] = (3, f&#34;Dimensionality of the image cube ({ccd_side.name[0]}-side)&#34;,)
                    image_cube_header[&#34;NAXIS3&#34;] = exposure
                    image_cube_header[&#34;CRPIX3&#34;] = 1
                    image_cube_header[&#34;CRVAL3&#34;] = start_time
                    image_cube_header[&#34;CTYPE3&#34;] = &#34;TIMETAB&#34;
                    image_cube_header[&#34;CUNIT3&#34;] = &#34;s&#34;
                    image_cube_header[&#34;PS3_0&#34;] = f&#34;WCS-TAB_{ccd_number}_{ccd_side.name[0]}&#34;
                    image_cube_header[&#34;PS3_1&#34;] = &#34;TIME&#34;

                    fits.append(cube_filename, image_cube, image_cube_header)

                    # Serial pre-scan

                    serial_prescans = []

                    exposure = 0

                    while True:

                        try:

                            serial_prescans.append(level1[f&#34;SPRE_{ccd_number}_{ccd_side.name[0]}&#34;, exposure].data)
                            exposure += 1

                        except KeyError:

                            break

                    serial_prescan_cube = np.stack(serial_prescans)
                    del serial_prescans

                    serial_prescan_cube_header = level1[f&#34;SPRE_{ccd_number}_{ccd_side.name[0]}&#34;, 0].header
                    serial_prescan_cube_header[&#34;NAXIS&#34;] = (3, f&#34;Dimensionality of the serial pre-scan cube ({ccd_side.name[0]}-side)&#34;,)
                    serial_prescan_cube_header[&#34;NAXIS3&#34;] = exposure
                    serial_prescan_cube_header[&#34;CRPIX3&#34;] = 1
                    serial_prescan_cube_header[&#34;CRVAL3&#34;] = start_time
                    serial_prescan_cube_header[&#34;CTYPE3&#34;] = &#34;TIMETAB&#34;
                    serial_prescan_cube_header[&#34;CUNIT3&#34;] = &#34;s&#34;
                    serial_prescan_cube_header[&#34;PS3_0&#34;] = f&#34;WCS-TAB_{ccd_number}_{ccd_side.name[0]}&#34;
                    serial_prescan_cube_header[&#34;PS3_1&#34;] = &#34;TIME&#34;

                    fits.append(cube_filename, serial_prescan_cube, serial_prescan_cube_header)

                    # Serial over-scan

                    if has_serial_overscan:

                        serial_overscans = []
                        exposure = 0

                        while True:

                            try:

                                serial_overscans.append(level1[f&#34;SOVER_{ccd_number}_{ccd_side.name[0]}&#34;, exposure].data)
                                exposure += 1

                            except KeyError:

                                break

                        serial_overscan_cube = np.stack(serial_overscans)
                        del serial_overscans

                        serial_overscan_cube_header = level1[f&#34;SOVER_{ccd_number}_{ccd_side.name[0]}&#34;, 0].header
                        serial_overscan_cube_header[&#34;NAXIS&#34;] = (3, f&#34;Dimensionality of the serial over-scan cube ({ccd_side.name[0]}-side)&#34;,)
                        serial_overscan_cube_header[&#34;NAXIS3&#34;] = exposure
                        serial_overscan_cube_header[&#34;CRPIX3&#34;] = 1
                        serial_overscan_cube_header[&#34;CRVAL3&#34;] = start_time
                        serial_overscan_cube_header[&#34;CTYPE3&#34;] = &#34;TIMETAB&#34;
                        serial_overscan_cube_header[&#34;CUNIT3&#34;] = &#34;s&#34;
                        serial_overscan_cube_header[&#34;PS3_0&#34;] = f&#34;WCS-TAB_{ccd_number}_{ccd_side.name[0]}&#34;
                        serial_overscan_cube_header[&#34;PS3_1&#34;] = &#34;TIME&#34;

                        fits.append(cube_filename, serial_overscan_cube, serial_overscan_cube_header)

                    # Parallel over-scan

                    if has_parallel_overscan:

                        parallel_overscans = []
                        exposure = 0

                        while True:

                            try:

                                parallel_overscans.append(level1[f&#34;POVER_{ccd_number}_{ccd_side.name[0]}&#34;, exposure].data)
                                exposure += 1

                            except KeyError:
                                break

                        parallel_overscan_cube = np.stack(parallel_overscans)
                        del parallel_overscans

                        parallel_overscan_cube_header = level1[f&#34;POVER_{ccd_number}_{ccd_side.name[0]}&#34;, 0].header
                        parallel_overscan_cube_header[&#34;NAXIS&#34;] = (3, f&#34;Dimensionality of the parallel over-scan cube ({ccd_side.name[0]}-side)&#34;,)
                        parallel_overscan_cube_header[&#34;NAXIS3&#34;] = exposure
                        parallel_overscan_cube_header[&#34;CRPIX3&#34;] = 1
                        parallel_overscan_cube_header[&#34;CRVAL3&#34;] = start_time
                        parallel_overscan_cube_header[&#34;CTYPE3&#34;] = &#34;TIMETAB&#34;
                        parallel_overscan_cube_header[&#34;CUNIT3&#34;] = &#34;s&#34;
                        parallel_overscan_cube_header[&#34;PS3_0&#34;] = f&#34;WCS-TAB_{ccd_number}_{ccd_side.name[0]}&#34;
                        parallel_overscan_cube_header[&#34;PS3_1&#34;] = &#34;TIME&#34;

                        fits.append(
                            cube_filename, parallel_overscan_cube, parallel_overscan_cube_header
                        )

    # Remove the level-1 FITS file

    LOGGER.info(f&#34;Removing flat-structure FITS file {filename}&#34;)
    os.remove(filename)


def is_incomplete(hdf5_file: File):
    &#34;&#34;&#34; Check whether the given HDF5 file is incomplete.

    The HDF5 files are created at the start of a cycle. The register map and (if applicable) the format version are
    stored at this point.  If an observation starts &#34;half way&#34; a cycle, the register map will not be present.

    Args:
        - hdf5_file: HDF5 file.

    Returns: True if the given HDF5 file is incomplete (i.e. if the register map is not stored); False otherwise.
    &#34;&#34;&#34;

    return &#34;register&#34; not in hdf5_file


def is_corrupt(hdf5_file: File):
    &#34;&#34;&#34; Check whether the given HDF5 file is corrupt.

    Args:
        - hdf5_file: HDF5 file.

    Returns: True if an error flag is set in one of the groups; False otherwise.
    &#34;&#34;&#34;

    for count in range(4):

        if f&#34;/{count}/hk&#34; in hdf5_file:

            hk_packet = SpaceWirePacket.create_packet(hdf5_file[f&#34;/{count}/hk&#34;][...])
            error_flags = HousekeepingData(hk_packet.data)[&#39;error_flags&#39;]

            if error_flags:
                return True

    return False


def any_crucial_parameters_changed(prep: dict, n_fee_state: Mapping):
    &#34;&#34;&#34; Check whether there is a change in crucial parameters.

    Return True if any of the following parameters changed with respect to the revious check: v_start, v_end, h_end,
    rows_final_dump, ccd_mode_config, and ccd_readout_order.

    Args:
        - prep (dict): Current values for the crucial parameters.
        - n_fee_state: N-FEE state parameters or register map.

    Returns: True if any of the values have changed, False otherwise.
    &#34;&#34;&#34;

    v_start = n_fee_state[&#39;v_start&#39;]
    v_end = n_fee_state[&#39;v_end&#39;]
    h_end = n_fee_state[&#39;h_end&#39;]
    rows_final_dump = n_fee_state[&#39;n_final_dump&#39;]
    ccd_mode_config = n_fee_state[&#39;ccd_mode_config&#39;]
    ccd_readout_order = n_fee_state[&#39;ccd_readout_order&#39;]
    ccd_readout_order = convert_ccd_order_value(ccd_readout_order)

    for x, y in dict(
        v_start=v_start, v_end=v_end, h_end=h_end, rows_final_dump=rows_final_dump,
        ccd_mode_config=ccd_mode_config, ccd_readout_order=ccd_readout_order,
    ).items():
        if prep.get(x) != y:
            LOGGER.debug(f&#34;{x=}, {prep.get(x)=}, {y=}&#34;)
            return True

    return False


def in_data_acquisition_mode(n_fee_state: Mapping):
    &#34;&#34;&#34; Check whether the N-FEE is in data acquisition mode.

    Args:
           - n_fee_state: N-FEE state parameters or register map.

    Returns: True if the N-FEE is in imaging mode (full-image (pattern) mode, windowing (pattern) mode, or
             parallel/serial trap pumping mode (1/2)) and the digitised data is transferred to the N-DPU.
    &#34;&#34;&#34;

    ccd_mode_config = n_fee_state[&#34;ccd_mode_config&#34;]
    digitise_en = n_fee_state[&#34;digitise_en&#34;]

    return ccd_mode_config in [n_fee_mode.FULL_IMAGE_MODE, n_fee_mode.FULL_IMAGE_PATTERN_MODE,
                               n_fee_mode.PARALLEL_TRAP_PUMPING_1_MODE, n_fee_mode.PARALLEL_TRAP_PUMPING_2_MODE,
                               n_fee_mode.SERIAL_TRAP_PUMPING_1_MODE, n_fee_mode.SERIAL_TRAP_PUMPING_2_MODE,
                               n_fee_mode.WINDOWING_PATTERN_MODE, n_fee_mode.WINDOWING_MODE] and digitise_en


def construct_cube_filename(fits_filename: str) -&gt; str:
    &#34;&#34;&#34; Construct the filename for the level-2 FITS file.

    The level-2 FITS file will have the data arranged in cubes, rather than in a flat structure.

    Args:
        - fits_filename: Filename for the level-1 FITS file.  The level-1 FITS files has the data arranged in a flat
                         structure.

    Returns: Filename for the level-2 FITS file.
    &#34;&#34;&#34;

    LOGGER.info(f&#34;Construct cube filename from {fits_filename}&#34;)

    # LOGGER.info(f&#34;Images: {&#39;images&#39; in fits_filename}&#34;)

    if &#34;images&#34; in fits_filename:
        return fits_filename.replace(&#34;images&#34;, &#34;cube&#34;)

    else:
        prefix, suffix = str(fits_filename).rsplit(&#39;_&#39;, 1)
        return f&#34;{prefix}_cube_{suffix}&#34;


def construct_images_filename(hdf5_filename: PosixPath, obsid: ObservationIdentifier = None, location=None):
    &#34;&#34;&#34; Construct the filename for the level-1 FITS file.


    The level-1 FITS files has the data arranged in a flat structure.

    Args:
        - identifier (str): Identifier for the source of the data, this string is usually what is sent in the `origin`
                            of the item dictionary.
        - ext (str): File extension: this depends on the persistence class that is used for storing the data.
        - obsid (ObservationIdentifier): Unique identifier for the observation (LAB_SETUP_TEST).
        - use_counter: Indicates whether or not a counter should be included in the filename.
        - location: Folder (with /daily and /obs sub-folders) in which the FITS files should be written (in a
                    dedicated directory in the /obs folder). If not specified, the `PLATO_DATA_STORAGE_LOCATION`
                    environment variable will be read out.

    Returns: Full path to the file as a `PurePath`.
    &#34;&#34;&#34;

    location = location or os.environ.get(&#39;PLATO_DATA_STORAGE_LOCATION&#39;)

    if obsid is None:

        timestamp, site_id, _, _, counter = str.split(str.split(str(hdf5_filename), &#34;.&#34;)[0], &#34;_&#34;)
        fits_filename = f&#34;{timestamp}_{site_id}_{N_FEE_SETTINGS.ORIGIN_CCD_DATA}_{counter}_images.{FITS.extension}&#34;

        location += &#34;/daily/&#34;

        return str(Path(location) / timestamp / fits_filename)

    else:

        # Make sure that the FITS file ends up in the correct sub-folder
        #   - oldest data: TEST_LAB_SETUP
        #   - more recent data: TEST_LAB

        obsid = obsid_from_storage(obsid, data_dir=location)

        timestamp = str.split(str(hdf5_filename).split(&#34;/&#34;)[-1], &#34;_&#34;)[0]
        location += &#34;/obs/&#34;

        if not os.path.isdir(f&#34;{location}/{obsid}&#34;):
            os.makedirs(f&#34;{location}/{obsid}&#34;)
        location += f&#34;{obsid}/&#34;

        # Determine the filename

        pattern = f&#34;{obsid}_{N_FEE_SETTINGS.ORIGIN_CCD_DATA}_*_{timestamp}_cube.{FITS.extension}&#34;
        counter = get_fits_counter(location, pattern)

        fits_filename = f&#34;{obsid}_{N_FEE_SETTINGS.ORIGIN_CCD_DATA}_{counter:05d}_{timestamp}_images.{FITS.extension}&#34;

        return str(Path(location) / fits_filename)


def get_fits_counter(location, pattern):
    &#34;&#34;&#34; Determine counter for a new FITS file at the given location and with the given pattern.

    Args:
        - location: Location where the FITS file should be stored.
        - pattern: Pattern for the filename.

    Returns: Value of the next counter; 1 if no previous files were found or if an error occurred.
    &#34;&#34;&#34;

    LOGGER.debug(f&#34;Pattern: {pattern=}&#34;)
    LOGGER.debug(f&#34;Location: {location=}&#34;)

    files = sorted(find_files(pattern=pattern, root=location))

    # No filenames found showing the given pattern -&gt; start counting at 1

    LOGGER.debug(f&#34;Number of matches: {len(files)=}&#34;)

    if len(files) == 0:
        return 1

    last_file = files[-1]

    counter = last_file.name.split(&#34;_&#34;)

    LOGGER.debug(f&#34;{counter = }&#34;)

    try:

        # Observation files have the following pattern:
        #  &lt;test ID&gt;_&lt;lab ID&gt;_N-FEE_CCD_&lt;counter&gt;_&lt;day YYYYmmdd&gt;_cube.fits

        counter = int(counter[-3]) + 1
        LOGGER.debug(f&#34;{counter = }&#34;)
        return counter

    except ValueError:

        LOGGER.warning(&#34;ValueError&#34;, exc_info=True)
        return 1


def create_fits_from_hdf5(files: List, location: str = None, setup: Setup = None):
    &#34;&#34;&#34; Off-line generation of FITS files from HDF5 files with SpW packets.

    When there is a change in crucial parameters, the current FITS file (if any) will be closed and a new one will be
    created as soon and HDF5 file with data content is encountered (when the N-FEE is in full-image or full-image
    pattern mode):

        - If there is a change in crucial parameters, close the current FITS file (if any).
        - If there is a change in crucial parameter and the N-FEE is in full-image mode or in full-image pattern
          mode, or the N-FEE goes to full-image mode or full-image pattern mode, a new FITS file will be created.
        - The content of the HDF5 files will be extracted and passed to the FITS persistence layer as SpW packets.

    In the older HDF5 files, only the register map is stored, which does not always reflect the actual N-FEE state.
    This is solved in the later version of the HDF5 files (format version &gt;= 2.0).  In these files, the current N-FEE
    state is stored in each of the data groups.

    It&#39;s possible that the first file in the list is incomplete, because it was already created by the time the
    current observation started.  That file

    Args:
        - files: List of filenames of the HDF5 files to use to create the FITS file.
        - location: Folder (with /daily and /obs sub-folders) in which the FITS files should be written (in a
                    dedicated directory in the /obs folder). If not specified, the `PLATO_DATA_STORAGE_LOCATION`
                    environment variable will be read out.
        - setup: Setup to retrieve information from.
    &#34;&#34;&#34;

    location = location or os.environ.get(&#39;PLATO_DATA_STORAGE_LOCATION&#39;)

    # Loop over the filenames.  When you encounter an HDF5 file, check its format version.

    for filename in files:

        filename = Path(filename)

        if filename.suffix == f&#34;.{HDF5.extension}&#34;:

            try:

                with h5.get_file(filename, mode=&#34;r&#34;, locking=False) as hdf5_file:

                    # It happens that some of the HDF5 files are incomplete.  These should not be considered to determine
                    # whether the register map (original format version) or the N-FEE state (format version &gt;= 2.0) to
                    # determine the state of the crucial parameters.

                    if is_incomplete(hdf5_file):    # or is_corrupt(hdf5_file):

                        files = files[1:]

                    else:

                        # The N-FEE state is stored in the data groups of the HDF5 files (format version &gt;= 2.0)

                        if &#34;versions&#34; in hdf5_file:

                            version_attrs = hdf5_file[&#34;versions&#34;][&#34;format_version&#34;].attrs

                            if version_attrs[&#34;major_version&#34;] == 2:
                                create_fits_from_hdf5_nfee_state(files, location=location, setup=setup)
                                break

                            else:

                                version = f&#34;{version_attrs[&#39;major_version&#39;]}.{version_attrs[&#39;minor_version&#39;]}&#34;

                                raise AttributeError(f&#34;HDF5 file format version {version} cannot be handled by the FITS generator&#34;)

                        # The register map is stored (globally) in the HDF5 files

                        else:
                            create_fits_from_hdf5_register_map(files, location=location, setup=setup)
                            break

            except RuntimeError as exc:
                LOGGER.debug(f&#34;Unable to open HDF5 file: {exc}&#34;)


def create_fits_from_hdf5_register_map(files: List, location: str = None, setup: Setup = None):
    &#34;&#34;&#34; Off-line generation of FITS files from HDF5 files with SpW packets.

    When there is a change in crucial parameters, the current FITS file (if any) will be closed and a new one will be
    created as soon and HDF5 file with data content is encountered (when the N-FEE is in full-image or full-image
    pattern mode):

        - If there is a change in crucial parameters, close the current FITS file (if any).
        - If there is a change in crucial parameter and the N-FEE is in full-image mode or in full-image pattern
          mode, or the N-FEE goes to full-image mode or full-image pattern mode, a new FITS file will be created.
        - The content of the HDF5 files will be extracted and passed to the FITS persistence layer as SpW packets.

    In the given HDF5 files, only the register map is stored, which does not always reflect the actual N-FEE state. As
    a result not all data may be present in the generated FITS files (e.g. because the register map says the N-FEE is
    already in dump mode) or the data might be split over more FITS files than expected (e.g. because the v_start and
    v_end parameters are already / not yet changed in the register map but not in the N-FEE state).

    Note that this problem is solved in the later version of the HDF5 files (format version &gt;= 2.0).

    Args:
        - files: List of filenames of the HDF5 files to use to create the FITS file.
        - location: Folder (with /daily and /obs sub-folders) in which the FITS files should be written (in a
                    dedicated directory in the /obs folder). If not specified, the `PLATO_DATA_STORAGE_LOCATION`
                    environment variable will be read out.
        - setup: Setup to retrieve information from.
    &#34;&#34;&#34;

    location = location or os.environ.get(&#39;PLATO_DATA_STORAGE_LOCATION&#39;)

    prep = {}
    fits_filename = None

    for filename in files:

        filename = Path(filename)

        if filename.suffix == &#39;.hdf5&#39;:

            print(f&#34;Processing {filename=!s}...&#34;)

            try:

                with h5.get_file(filename, mode=&#34;r&#34;, locking=False) as hdf5_file:

                    # if is_corrupt(hdf5_file):
                    #     LOGGER.warning(f&#34;Skipping {filename} (corrupt)&#34;)
                    #
                    # else:

                    if &#39;register&#39; not in hdf5_file:
                        LOGGER.warning(f&#34;No register map found for {filename=!s}, continue with next file..&#34;)
                        continue  # next HDF5 file

                    register_map = RegisterMap(&#34;N-FEE&#34;, memory_map=h5.get_data(hdf5_file[&#34;register&#34;]))

                    has_data = False

                    for group in h5.groups(hdf5_file):

                        if &#34;data&#34; in group.keys():

                            has_data = True

                            # Should a new FITS file be created?

                            if any_crucial_parameters_changed(prep, register_map):

                                if fits_filename:

                                    LOGGER.info(f&#34;Creating a FITS CUBE file ...&#34;)
                                    convert_to_cubes(fits_filename)
                                    fits_filename = None

                            if in_data_acquisition_mode(register_map):

                                if fits_filename is None:

                                    LOGGER.info(f&#34;A new FITS file will be created...&#34;)

                                    # Start writing to a new FITS file
                                    # Collect all information to sent to the FITS layer

                                    if &#34;obsid&#34; in hdf5_file:
                                        obsid = hdf5_file[&#34;obsid&#34;][()].decode()
                                        obsid = ObservationIdentifier.create_from_string(obsid, order=LAB_SETUP_TEST)
                                    else:
                                        obsid = None

                                    fits_filename = construct_images_filename(filename, obsid, location=location)
                                    LOGGER.info(f&#34;{fits_filename = !s}&#34;)

                                    ccd_readout_order = register_map[&#39;ccd_readout_order&#39;]
                                    ccd_readout_order = convert_ccd_order_value(ccd_readout_order)

                                    prep = {
                                        &#34;v_start&#34;: register_map[&#39;v_start&#39;],
                                        &#34;v_end&#34;: register_map[&#39;v_end&#39;],
                                        &#34;h_end&#34;: register_map[&#39;h_end&#39;],
                                        &#34;rows_final_dump&#34;: register_map[&#39;n_final_dump&#39;],
                                        &#34;ccd_mode_config&#34;: register_map[&#39;ccd_mode_config&#39;],
                                        &#34;ccd_readout_order&#34;: ccd_readout_order,  # CCD numbering [1-4]
                                        &#34;expected_last_packet_flags&#34;: get_expected_last_packet_flags(register_map),
                                        &#34;obsid&#34;: str(obsid) if obsid is not None else None,
                                        &#34;cycle_time&#34;: get_cycle_time(register_map, obsid=obsid, data_dir=location),
                                        &#34;cgse_version&#34;: get_cgse_version(obsid=obsid, data_dir=location),
                                        &#34;setup&#34;: setup
                                    }

                                    persistence = FITS(str(fits_filename), prep)
                                    persistence.open()

                                # See https://github.com/IvS-KULeuven/plato-common-egse/issues/901
                                # timecode = group[&#34;timecode&#34;]
                                # spw_packet = SpaceWirePacket.create_packet(h5.get_data(timecode))

                                timestamp = group[&#34;timecode&#34;].attrs[&#34;timestamp&#34;]
                                persistence.create({&#34;Timestamp&#34;: timestamp})

                                data = group[&#34;data&#34;]
                                sorted_datasets = natsort.natsorted(data.items(), key=lambda x: x[0])

                                persistence.expected_last_packet_flags = get_expected_last_packet_flags(register_map)

                                for identifier, dataset in sorted_datasets:

                                    spw_packet = SpaceWirePacket.create_packet(h5.get_data(dataset))
                                    # LOGGER.debug(f&#34;{spw_packet.type = !s}&#34;)
                                    persistence.create({f&#34;SpW packet {identifier}&#34;: spw_packet})

                    if not has_data:

                        if fits_filename:

                            LOGGER.info(f&#34;Creating a FITS CUBE file ...&#34;)
                            convert_to_cubes(fits_filename)
                            fits_filename = None

                        prep = clear_crucial_parameters(prep)

            except RuntimeError as exc:
                LOGGER.debug(f&#34;Unable to open HDF5 file: {exc}&#34;)
        else:
            print(f&#34;Skipping {filename=}&#34;)

    try:
        if fits_filename:
            LOGGER.info(f&#34;Creating a FITS CUBE file ...&#34;)
            convert_to_cubes(fits_filename)
    except OSError:
        # The last file in the list still contained data, so we reached the end of the list without creating a cube
        # FITS file yet
        pass


def create_fits_from_hdf5_nfee_state(files: List, location: str = None, setup: Setup = None):
    &#34;&#34;&#34; Off-line generation of FITS files from HDF5 files with SpW packets.

    When there is a change in crucial parameters, the current FITS file (if any) will be closed and a new one will be
    created as soon and HDF5 file with data content is encountered (when the N-FEE is in full-image or full-image
    pattern mode):

        - If there is a change in crucial parameters, close the current FITS file (if any).
        - If there is a change in crucial parameter and the N-FEE is in full-image mode or in full-image pattern
          mode, or the N-FEE goes to full-image mode or full-image pattern mode, a new FITS file will be created.
        - The content of the HDF5 files will be extracted and passed to the FITS persistence layer as SpW packets.

    In the given HDF5 files, the N-FEE state is saved in all data groups, reflecting the actual N-FEE state (i.e.
    solving the problem of the mismatch between the register map and the N-FEE state).

    Args:
        - files: List of filenames of the HDF5 files to use to create the FITS file.
        - location: Folder (with /daily and /obs sub-folders) in which the FITS files should be written (in a
                    dedicated directory in the /obs folder). If not specified, the `PLATO_DATA_STORAGE_LOCATION`
                    environment variable will be read out.
        - setup: Setup to retrieve information from.
    &#34;&#34;&#34;

    location = location or os.environ.get(&#39;PLATO_DATA_STORAGE_LOCATION&#39;)

    config_slicing_num_cycles = 0    # Configured slicing parameter
    processed_num_cycles = 0            # HDF5 files with image data processed for current FITS file

    prep = {}
    fits_filename = None

    for filename in files:

        filename = Path(filename)

        if filename.suffix == &#39;.hdf5&#39;:

            print(f&#34;Processing {filename=!s}...&#34;)

            try:

                with h5.get_file(filename, mode=&#34;r&#34;, locking=False) as hdf5_file:

                    # Slicing

                    try:
                        slicing_num_cycles = hdf5_file[&#34;dpu&#34;].attrs[&#34;slicing_num_cycles&#34;]
                        if slicing_num_cycles != config_slicing_num_cycles:
                            if fits_filename:
                                convert_to_cubes(fits_filename)
                                fits_filename = None
                            processed_num_cycles = 0
                            config_slicing_num_cycles = slicing_num_cycles
                    except KeyError:
                        config_slicing_num_cycles = 0

                    # if is_corrupt(hdf5_file):
                    #     LOGGER.warning(f&#34;Skipping {filename} (corrupt)&#34;)
                    #
                    # else:

                    has_data = False

                    for group in h5.groups(hdf5_file):

                        if &#34;data&#34; in group.keys():

                            has_data = True

                            n_fee_state = group[&#34;data&#34;].attrs

                            # Should a new FITS file be created?

                            if any_crucial_parameters_changed(prep, n_fee_state):

                                if fits_filename:

                                    LOGGER.info(f&#34;Creating a FITS CUBE file ...&#34;)
                                    convert_to_cubes(fits_filename)
                                    fits_filename = None
                                    processed_num_cycles = 0

                            if in_data_acquisition_mode(n_fee_state):

                                if fits_filename is None:

                                    LOGGER.info(f&#34;A new FITS file will be created...&#34;)

                                    # Start writing to a new FITS file
                                    # Collect all information to sent to the FITS layer

                                    if &#34;obsid&#34; in hdf5_file:
                                        obsid = hdf5_file[&#34;obsid&#34;][()].decode()
                                        obsid = ObservationIdentifier.create_from_string(obsid, order=LAB_SETUP_TEST)
                                    else:
                                        obsid = None

                                    fits_filename = construct_images_filename(filename, obsid, location=location)
                                    LOGGER.info(f&#34;{fits_filename = !s}&#34;)

                                    ccd_readout_order = n_fee_state[&#39;ccd_readout_order&#39;]
                                    ccd_readout_order = convert_ccd_order_value(ccd_readout_order)

                                    prep = {
                                        &#34;v_start&#34;: n_fee_state[&#39;v_start&#39;],
                                        &#34;v_end&#34;: n_fee_state[&#39;v_end&#39;],
                                        &#34;h_end&#34;: n_fee_state[&#39;h_end&#39;],
                                        &#34;rows_final_dump&#34;: n_fee_state[&#39;n_final_dump&#39;],
                                        &#34;ccd_mode_config&#34;: n_fee_state[&#39;ccd_mode_config&#39;],
                                        &#34;ccd_readout_order&#34;: ccd_readout_order,  # CCD numbering [1-4]
                                        &#34;expected_last_packet_flags&#34;: get_expected_last_packet_flags(n_fee_state),
                                        &#34;obsid&#34;: str(obsid) if obsid is not None else None,
                                        &#34;cycle_time&#34;: get_cycle_time(n_fee_state, obsid=obsid, data_dir=location),
                                        &#34;cgse_version&#34;: get_cgse_version(obsid=obsid, data_dir=location),
                                        &#34;setup&#34;: setup
                                    }

                                    persistence = FITS(str(fits_filename), prep)
                                    persistence.open()

                                # See https://github.com/IvS-KULeuven/plato-common-egse/issues/901
                                # timecode = group[&#34;timecode&#34;]
                                # spw_packet = SpaceWirePacket.create_packet(h5.get_data(timecode))

                                timestamp = group[&#34;timecode&#34;].attrs[&#34;timestamp&#34;]
                                persistence.create({&#34;Timestamp&#34;: timestamp})

                                data = group[&#34;data&#34;]
                                sorted_datasets = natsort.natsorted(data.items(), key=lambda x: x[0])

                                persistence.expected_last_packet_flags = get_expected_last_packet_flags(n_fee_state)

                                for identifier, dataset in sorted_datasets:

                                    spw_packet = SpaceWirePacket.create_packet(h5.get_data(dataset))
                                    # LOGGER.debug(f&#34;{spw_packet.type = !s}&#34;)
                                    persistence.create({f&#34;SpW packet {identifier}&#34;: spw_packet})

                    if has_data:
                        processed_num_cycles += 1

                        if fits_filename and config_slicing_num_cycles != 0 \
                                and processed_num_cycles == config_slicing_num_cycles:
                            convert_to_cubes(fits_filename)
                            fits_filename = None
                            processed_num_cycles = 0
                    else:

                        if fits_filename:
                            LOGGER.info(f&#34;Creating a FITS CUBE file ...&#34;)
                            convert_to_cubes(fits_filename)
                            fits_filename = None
                            processed_num_cycles = 0

                        prep = clear_crucial_parameters(prep)

            except RuntimeError as exc:
                LOGGER.debug(f&#34;Unable to open HDF5 file: {exc}&#34;)
        else:
            print(f&#34;skipping {filename=}&#34;)

    try:
        if fits_filename:
            LOGGER.info(f&#34;Creating a FITS CUBE file ...&#34;)
            convert_to_cubes(fits_filename)
    except OSError:
        # The last file in the list still contained data, so we reached the end of the list without creating a cube
        # FITS file yet
        pass


def clear_crucial_parameters(prep: dict):
    &#34;&#34;&#34; Clear the crucial parameters from the given dictionary.

    Args:
        - prep: Dictionary with crucial parameters.

    Returns: Dictionary with the cleared crucial parameters.
    &#34;&#34;&#34;

    prep[&#34;v_start&#34;] = None
    prep[&#34;v_end&#34;] = None
    prep[&#34;h_end&#34;] = None
    prep[&#34;rows_final_dump&#34;] = None
    prep[&#34;ccd_mode_config&#34;] = None
    prep[&#34;ccd_readout_order&#34;] = None

    return prep


class SynopticsFwdFill(tuple, Enum):
    &#34;&#34;&#34; Enumeration of the synoptics to forward fill.

    This is only applicable for the commanded source position.
    &#34;&#34;&#34;

    # Source position (commanded)

    THETA_CMD = (&#34;GSYN_CMD_THETA&#34;, &#34;Commanded source position theta [deg]&#34;)
    PHI_CMD = (&#34;GSYN_CMD_PHI&#34;, &#34;Commanded source position phi [deg]&#34;)


class SynopticsInterp1d(tuple, Enum):
    &#34;&#34;&#34; Enumeration of the synoptics to linearly interpolate.

    This is only applicable for:
        - calibrated TCS temperatures;
        - calibrated N-FEE temperatures (TOU + CCDs + and board sensors);
        - selection of TH DAQ(s) temperatures;
        - OGSE attenuation (relative intensity + FWC fraction for the OGSE);
        - actual source position.
    &#34;&#34;&#34;

    # TCS temperatures

    T_TRP1 = (&#34;GSYN_TRP1&#34;, &#34;Mean T for TOU TRP1 (TCS) [deg C]&#34;)
    T_TRP22 = (&#34;GSYN_TRP22&#34;, &#34;Mean T for FEE TRP22 (TCS) [deg C]&#34;)

    # TOU TRP PT1000 sensors (N-FEE)

    T_TRP5 = (&#34;GSYN_TRP5&#34;, &#34;Mean T for TRP5 (TOU baffle ring) [deg C]&#34;)
    T_TRP6 = (&#34;GSYN_TRP6&#34;, &#34;Mean T for TRP6 (FPA I/F) [deg C]&#34;)
    T_TRP8 = (&#34;GSYN_TRP8&#34;, &#34;Mean T for TRP8 (L3) [deg C]&#34;)
    T_TRP21 = (&#34;GSYN_TRP21&#34;, &#34;Mean T for TRP21 (TOU bipod +X bottom) [deg C]&#34;)
    T_TRP31 = (&#34;GSYN_TRP31&#34;, &#34;Mean T for TRP31 (TOU bipod -Y bottom) [deg C]&#34;)
    T_TRP41 = (&#34;GSYN_TRP41&#34;, &#34;Mean T for TRP41 (TOU bipod +Y bottom) [deg C]&#34;)

    # CCD PT100 sensors (N-FEE)

    T_CCD1 = (&#34;GSYN_CCD1&#34;, &#34;Mean T for CCD1 [deg C]&#34;)
    T_CCD2 = (&#34;GSYN_CCD2&#34;, &#34;Mean T for CCD2 [deg C]&#34;)
    T_CCD3 = (&#34;GSYN_CCD3&#34;, &#34;Mean T for CCD3 [deg C]&#34;)
    T_CCD4 = (&#34;GSYN_CCD4&#34;, &#34;Mean T for CCD4 [deg C]&#34;)

    # Board sensors: type PT1000 (N-FEE)

    T_PCB1 = (&#34;GSYN_NFEE_T_PCB1&#34;, &#34;Mean T for board sensor PCB1 [deg C]&#34;)
    T_PCB2 = (&#34;GSYN_NFEE_T_PCB2&#34;, &#34;Mean T for board sensor PCB2 [deg C]&#34;)
    T_PCB3 = (&#34;GSYN_NFEE_T_PCB3&#34;, &#34;Mean T for board sensor PCB3 [deg C]&#34;)
    T_PCB4 = (&#34;GSYN_NFEE_T_PCB4&#34;, &#34;Mean T for board sensor PCB4 [deg C]&#34;)

    # Board sensors: type ISL71590

    T_ADC = (&#34;GSYN_NFEE_T_ADC&#34;, &#34;Mean ADC board T [deg C]&#34;)
    T_CDS = (&#34;GSYN_NFEE_T_CDS&#34;, &#34;Mean CDS board T [deg C]&#34;)
    T_ANALOG = (&#34;GSYN_NFEE_T_ANALOG&#34;, &#34;Mean analog board T [deg C]&#34;)
    T_SKYSHROUD = (&#34;GSYN_SKYSHROUD&#34;, &#34;Mean front shroud T [deg C]&#34;)
    T_TEB_TOU = (&#34;GSYN_TEB_TOU&#34;, &#34;Mean TEB TOU T [deg C]&#34;)
    T_TEB_FEE = (&#34;GSYN_TEB_FEE&#34;, &#34;Mean TEB FEE T [deg C]&#34;)

    # Temperatures from the TH DAQ

    T_TRP2 = (&#34;GSYN_TRP2&#34;, &#34;Mean T for TRP2 (MaRi bipod +X I/F) [deg C]&#34;)
    T_TRP3 = (&#34;GSYN_TRP3&#34;, &#34;Mean T for TRP3 (MaRi bipod -Y I/F) [deg C]&#34;)
    T_TRP4 = (&#34;GSYN_TRP4&#34;, &#34;Mean T for TRP4 (MaRi bipod +Y I/F) [deg C]&#34;)

    T_TRP7 = (&#34;GSYN_TRP7&#34;, &#34;Mean T for TRP7 (thermal strap) [deg C]&#34;)
    T_TRP10 = (&#34;GSYN_TRP10&#34;, &#34;Mean T for TRP10 (FPA) [deg C]&#34;)

    # OGSE attenuation

    OGATT = (&#34;GSYN_OGSE_REL_INTENSITY&#34;, &#34;Relative OGSE intensity&#34;)
    OGFWC = (&#34;GSYN_OGSE_FWC_FRACTION&#34;, &#34;OGSE FWC fraction&#34;)

    # Source position (actual)

    THETA = (&#34;GSYN_ACT_THETA&#34;, &#34;Actual source position theta [deg]&#34;)
    PHI = (&#34;GSYN_ACT_PHI&#34;, &#34;Actual source position phi [deg]&#34;)


class SynopticsLeaveGaps(tuple, Enum):
    &#34;&#34;&#34; Enumeration of the synoptics not to fill the gaps for.

    This is only applicable for the status of the shutter (open/closed).  Note that there is no shutter in CSL, so we
    indicate that the shutter is always open there.
    &#34;&#34;&#34;

    OGSHTTR = (&#34;GSYN_OGSE_SHUTTER_OPEN&#34;, &#34;Is the shutter open?&#34;)


def get_fits_synoptics(obsid: str, data_dir=None) -&gt; dict:
    &#34;&#34;&#34; Retrieve the synoptics that need to be included in the FITS files for the given observation.

    The synoptics that need be be included in the FITS files are represented by the following enumerations:

        - SynopticsFwdFill: Use forward filling for the gaps -&gt; only at the beginning of the observation it is possible
          that there still are gaps (but it is unlikely that the data acquisition has already started then);
        - SynopticsInterp1d: Use linear interpolation to fill the gaps.  At the extremes, we use extrapolation;
        - SynopticsLeaveGaps: Don&#39;t fill the gaps.

    Args:
        - obsid: Observation identifier [TEST_LAB or TEST_LAB_SETUP].

    Returns: Dictionary with the synoptics that should go into the FITS files for the given observation.
    &#34;&#34;&#34;

    synoptics_table = get_synoptics_table(obsid, data_dir=data_dir)

    # We keep the original timestamps (when filling the gaps)

    timestamps = synoptics_table[&#34;timestamp&#34;].values
    for index in range(len(timestamps)):
        timestamps[index] = time_since_epoch_1958(timestamps[index])
    timestamps = timestamps.astype(float)

    synoptics = {&#34;timestamps&#34;: timestamps}  # Don&#39;t forget to include the timestamps to the returned dictionary

    # Linear interpolation

    for syn_enum in SynopticsInterp1d:

        syn_name = syn_enum.value[0]

        if syn_name in synoptics_table:

            # We need to filter out the NaNs or the interpolation will not work

            values = synoptics_table[syn_name].values
            selection = ~np.isnan(values)

            if np.any(selection):
                interpolation = interp1d(timestamps[np.where(selection)], values[np.where(selection)], kind=&#39;linear&#39;,
                                         fill_value=&#39;extrapolate&#39;)
                synoptics[syn_enum] = interpolation(timestamps)

    # Forward fill

    for syn_enum in SynopticsFwdFill:

        syn_name = syn_enum.value[0]

        if syn_name in synoptics_table:
            synoptics[syn_enum] = synoptics_table[syn_name].ffill()

    # Leave the gaps in

    for syn_enum in SynopticsLeaveGaps:

        syn_name = syn_enum.value[0]

        if syn_name in synoptics_table:
            synoptics[syn_enum] = synoptics_table[syn_name]

    return synoptics


def add_synoptics(obsid: str, data_dir=None):
    &#34;&#34;&#34; Add synoptics to the FITS headers for the given observation.

    When all FITS files have been produced for the given obsid, synoptics is added to the headers.  This is done in the
    following steps:
        - Determine which folder in the /obs directory comprises the HK and FITS files for the given obsid;
        - Read the synoptics for the given obsid (from said folder) into a pandas DataFrame;
        - Compose the list of FITS files for the given observation (from said folder);
        - For all of these FITS files, loop over the cubes it contains and:
                - Determine the time range covered by the cube;
                - Select the synoptics (from the pandas DataFrame) over that time range;
                - For the synoptical temperatures, source position (commanded + actual), and OGSE intensity: calculate
                  the average and add this to the header of the cube;
                - For the shutter: calculate the mean and add this to the header of the cube.

    Args:
        - obsid: Observation identifier [TEST_LAB or TEST_LAB_SETUP].
    &#34;&#34;&#34;

    data_dir = data_dir or os.environ.get(&#39;PLATO_DATA_STORAGE_LOCATION&#39;)

    obsid = obsid_from_storage(obsid, data_dir=data_dir)
    obs_dir = f&#34;{data_dir}/obs/{obsid}&#34;  # Where the HK and FITS files are stored

    synoptics = get_fits_synoptics(obsid, data_dir=data_dir)
    timestamps = synoptics[&#34;timestamps&#34;]    # Timestamps of the synoptics -&gt; compare with absolute time in FITS file

    # Compose the list of FITS files for the given obsid

    pattern = f&#34;{obsid}_{N_FEE_SETTINGS.ORIGIN_CCD_DATA}_*_*_cube.fits&#34;
    fits_filenames = sorted(find_files(pattern=pattern, root=obs_dir))

    # Loop over all FITS files (cubes) for the given obsid

    for fits_filename in fits_filenames:

        syn_info = {}

        # Loop over all image cubes

        with fits.open(fits_filename) as fits_file:

            start_time = time_since_epoch_1958(fits_file[&#34;PRIMARY&#34;].header[&#34;DATE-OBS&#34;])

            # Loop over both sides of all CCDs (not all of them might be in -&gt; hence the KeyError)

            for ccd_number in range(1, 5):

                for ccd_side in fee_side:

                    try:
                        # Absolute time = time at the start of the readout
                        #               = time at the end of the exposure
                        # -&gt; Extract relative time from the WCS-TAB and add the DATE-OBS (which is the time of the
                        #    1st frame in the FITS file; all times in the WCS-TAB are relative to this)

                        wcs_table_name = f&#34;WCS-TAB_{ccd_number}_{ccd_side.name[0]}&#34;     # Holds relative time
                        absolute_time = np.array(fits_file[wcs_table_name].data[&#34;TIME&#34;]) + start_time

                        # We don&#39;t care about the 1st frame of any CCD side, as the image is saturated anyway, and it is
                        # very difficult to determine the start of that exposure anyway
                        # -&gt; Simplest solution: indicate that the synoptics is unknown for those frames
                        # For all other frames:
                        #       - Determine when the readout for the previous frame started -&gt; start_previous_readout;
                        #       - Determine when the readout for the current frame started -&gt; start_current_readout;
                        #       - For each synoptics parameter, gather the values acquired in the timespan
                        #         [start_previous_readout, start_current_readout]
                        #       - For the numerical values: take the mean (skipping the NaNs)
                        #       - For the boolean values (i.c. status of the shutter):
                        #               - Only NaN selected -&gt; &#34;U&#34; (unknown)
                        #               - Both True &amp; False selected (potentially also NaNs) -&gt; &#34;M&#34; (mixed)
                        #               - Only True (potentially also NaNs) selected -&gt; &#34;T&#34; (True = shutter open)
                        #               - Only False (potentially also NaNs) selected -&gt; &#34;F&#34; (False = shutter closed)
                        #
                        # For each synoptical parameter, first determine all the values that need to be included in the
                        # current cube of the current FITS file (it is only when we have composed these arrays, that we
                        # can included them in a table in the FITS file)

                        fits_synoptics = {syn_enum: np.array([np.nan]) for syn_enum in chain(SynopticsFwdFill,
                                                                                             SynopticsInterp1d)}
                        fits_synoptics.update({syn_enum: np.array([&#34;U&#34;]) for syn_enum in SynopticsLeaveGaps})

                        for index in range(1, len(absolute_time)):

                            # Selection of the synoptics for the current frame: based on timestamps:
                            #   - Start (start_previous_readout): start of the readout of the previous exposure
                            #   - End (start_current_readout): start of the readout of the current exposure

                            start_previous_readout = absolute_time[index - 1]
                            start_current_readout = absolute_time[index]

                            selection = np.where(timestamps &gt;= start_previous_readout) \
                                        and np.where(timestamps &lt;= start_current_readout)[0]

                            # Average (skipping NaNs)

                            for syn_enum in chain(SynopticsFwdFill, SynopticsInterp1d):
                                try:
                                    selected_values = synoptics[syn_enum][selection]
                                    average_value = np.nanmean(selected_values)

                                    fits_synoptics[syn_enum] = np.append(fits_synoptics[syn_enum], average_value)
                                except KeyError:
                                    fits_synoptics[syn_enum] = np.append(fits_synoptics[syn_enum], np.nan)

                            for syn_enum in SynopticsLeaveGaps:
                                try:
                                    selected_values = synoptics[syn_enum][selection].astype(float)
                                    selection = ~np.isnan(selected_values)

                                    if not np.any(selection):   # No data -&gt; &#34;U&#34; (unknown)
                                        value = &#34;U&#34;
                                    else:   # Use &#34;T&#34; (True) / &#34;F&#34; (False) only when unique (otherwise: &#34;M&#34; (mixed))
                                        unique_values = np.unique(selected_values[selection])
                                        value = str(bool(unique_values[0]))[0] if len(unique_values) == 1 else &#34;M&#34;
                                    fits_synoptics[syn_enum] = np.append(fits_synoptics[syn_enum], value)
                                except KeyError:    # &#34;U&#34; (unknown)
                                    fits_synoptics[syn_enum] = np.append(fits_synoptics[syn_enum], &#34;U&#34;)

                            # At this point, we have for each synoptical parameter an array of the values that need to
                            # be included in the FITS file.  We now put all this information in a dedicated table and
                            # add it to the FITS file.

                            syn_columns = []

                            for syn_enum in chain(SynopticsFwdFill, SynopticsInterp1d, SynopticsLeaveGaps):
                                column_format = &#34;A&#34; if syn_enum == SynopticsLeaveGaps.OGSHTTR else &#34;F&#34;

                                syn_column = fits.Column(syn_enum.value[0], format=column_format,
                                                         array=fits_synoptics[syn_enum])
                                syn_columns.append(syn_column)

                            syn_table = fits.BinTableHDU.from_columns(syn_columns)
                            syn_table.header[&#34;EXTNAME&#34;] = f&#34;SYN-TAB_{ccd_number}_{ccd_side.name[0]}&#34;

                            # merged_columns = wcs_table.columns + syn_table.columns
                            # merged_table = fits.BinTableHDU.from_columns(merged_columns)

                            syn_info[syn_table.header[&#34;EXTNAME&#34;]] = (syn_table.data, syn_table.header)
                    except KeyError:
                        pass

        for data in syn_info.values():
            fits.append(str(fits_filename), data[0], data[1])

@click.group()
def cli():
    pass


@cli.command()
def start():
    multiprocessing.current_process().name = &#34;fitsgen&#34;

    # FIXME: Why is this line commented out?
    # start_http_server(CTRL_SETTINGS.METRICS_PORT)

    # The Storage Manager must be active (otherwise the HK cannot be stored)

    if not is_storage_manager_active():

        LOGGER.error(&#34;The Storage Manager is not running, start the core services before running the data acquisition.&#34;)
        return

    if not is_dpu_cs_active():
        LOGGER.critical(&#34;DPU Control Server must be running to be able to start the FITS generator.&#34;)
        return

    # register_to_storage_manager(ORIGIN, CSV, prep=dict(mode=&#39;a&#39;, column_names=[&#34;timestamp&#34;, &#34;FOV_ACT_THETA&#34;,
    #                                                                            &#34;FOV_ACT_PHI&#34;, &#34;FOV_CMD_THETA&#34;,
    #                                                                            &#34;FOV_CMD_PHI&#34;]))

    # Make sure you notice when a command has been issued to stop the FOV HK generation

    context = zmq.Context()

    endpoint = bind_address(CTRL_SETTINGS.PROTOCOL, CTRL_SETTINGS.COMMANDING_PORT)
    commander = context.socket(zmq.REP)
    commander.bind(endpoint)

    poller = zmq.Poller()
    poller.register(commander, zmq.POLLIN)

    fg = FITSGenerator()
    fg.run(commander, poller)


@cli.command()
def start_bg():

    invoke.run(&#34;fitsgen start&#34;, disown=True)


@cli.command()
def stop():
    &#34;&#34;&#34;Stop the FOV HK Control Server. &#34;&#34;&#34;

    # In the while True loop in the start command, _should_stop needs to force a break from the loop.When this happens
    # (and also when a keyboard interrupt has been caught), the monitoring socket needs to be closed (this needs to be
    # done in the TH - specific implementation of _start).  Unregistering from the Storage Manager is done
    # automatically.

    response = send_request(&#34;quit&#34;)

    if response == &#34;ACK&#34;:
        rich.print(&#34;FITS generation successfully terminated.&#34;)
    else:
        rich.print(f&#34;[red] ERROR: {response}&#34;)


def _check_commander_status(commander, poller) -&gt; bool:
    &#34;&#34;&#34; Check the status of the commander.

    Checks whether a command has been received by the given commander.

    Returns: True if a quit command was received; False otherwise.

    Args:
        - commander: Commanding socket for the FOV HK generation.
        - poller: Poller for the FOV HK generation.
    &#34;&#34;&#34;

    socks = dict(poller.poll(timeout=5000))   # Timeout of 5s

    if commander in socks:
        pickle_string = commander.recv()
        command = pickle.loads(pickle_string)

        if command.lower() == &#34;quit&#34;:

            commander.send(pickle.dumps(&#34;ACK&#34;))
            return True

        if command.lower() == &#34;status&#34;:
            response = dict(
                status=&#34;ACK&#34;,
                host=CTRL_SETTINGS.HOSTNAME,
                command_port=CTRL_SETTINGS.COMMANDING_PORT
            )
            commander.send(pickle.dumps(response))

        return False

    return False


@cli.command()
def status():
    &#34;&#34;&#34;Print the status of the FITS Generation Control Server.&#34;&#34;&#34;

    rich.print(&#34;FITS generation:&#34;)

    response = send_request(&#34;status&#34;)

    if response.get(&#34;status&#34;) == &#34;ACK&#34;:
        rich.print(&#34;  Status: [green]active&#34;)
        rich.print(f&#34;  Hostname: {response.get(&#39;host&#39;)}&#34;)
        rich.print(f&#34;  Commanding port: {response.get(&#39;command_port&#39;)}&#34;)
    else:
        rich.print(&#34;  Status: [red]not active&#34;)


def send_request(command_request: str):
    &#34;&#34;&#34;Sends a request to the FOV HK Control Server and waits for a response.

    Args:
        - command_request: Request.

    Returns: Response to the request.
    &#34;&#34;&#34;

    ctx = zmq.Context().instance()
    endpoint = connect_address(CTRL_SETTINGS.PROTOCOL, CTRL_SETTINGS.HOSTNAME, CTRL_SETTINGS.COMMANDING_PORT)
    socket = ctx.socket(zmq.REQ)
    socket.connect(endpoint)

    socket.send(pickle.dumps(command_request))
    rlist, _, _ = zmq.select([socket], [], [], timeout=TIMEOUT_RECV)

    if socket in rlist:
        response = socket.recv()
        response = pickle.loads(response)
    else:
        response = {&#34;error&#34;: &#34;Receive from ZeroMQ socket timed out for FITS generation Control Server.&#34;}
    socket.close(linger=0)

    return response


@cli.command()
@click.argument(&#39;files&#39;, type=str, nargs=-1)
@click.option(&#34;--location&#34;, type=str, is_flag=False, default=None, help=&#34;Set the root folder for the output (i.e. folder with /daily and /obs)&#34;)
@click.option(&#34;--setup_id&#34;, type=int, is_flag=False, default=None, help=&#34;Setup ID&#34;)
@click.option(&#34;--site_id&#34;, type=str, is_flag=False, default=None, help=&#34;Site ID&#34;)
def from_hdf5(files, location=None, setup_id=None, site_id=None):
    &#34;&#34;&#34; Generate the FITS files for the given list of HDF5 files.

    Args:
        - files: List of HDF5 filenames.
        - setup_id: Identifier of the setup that should be used.  When not specified, the setup loading in the
                    Configuration Manager will be used to retrieve information from.
        - site_id: Identifier for the test site.
    &#34;&#34;&#34;

    site_id = site_id or SITE.ID

    if setup_id is None:
        setup = GlobalState.setup
    else:
        setup = load_setup(setup_id=setup_id, site_id=site_id, from_disk=True)

    create_fits_from_hdf5(files, location=location, setup=setup)


@cli.command()
@click.argument(&#39;obsid&#39;, type=str)
@click.option(&#34;--location&#34;, type=str, is_flag=False, default=None, help=&#34;Set the root folder for the output (i.e. folder with /daily and /obs)&#34;)
def for_obsid(obsid, location=None):
    &#34;&#34;&#34; Generate the FITS files for the given obsid.

    The setup that was loaded in the Configuration Manager during the given observation, will be used to retrieve
    information from.

    Args:
        - obsid: Observation identifier [TEST_LAB or TEST_LAB_SETUP].
        - location: Folder (with /daily and /obs sub-folders) in which the FITS files should be written (in a
                    dedicated directory in the /obs folder). If not specified, the `PLATO_DATA_STORAGE_LOCATION`
                    environment variable will be read out.
    &#34;&#34;&#34;

    hdf5_filenames = get_hdf5_filenames_for_obsid(obsid, data_dir=location)
    setup = get_setup_for_obsid(obsid, data_dir=location)

    create_fits_from_hdf5(hdf5_filenames, location=location, setup=setup)
    add_synoptics(obsid, data_dir=location)


def get_hdf5_filenames_for_obsid(obsid: str, data_dir: str = None) -&gt; List:
    &#34;&#34;&#34; Return list of HDF5 filenames that contribute to the given obsid.

    The given obsid can be specified in either of these two formats: TEST_LAB or TEST_LAB_SETUP.  The obsid that is
    stored in the HDF5 files is of format LAB_SETUP_TEST.  In this method, we gather the list of HDF5 filenames for
    which the combination (TEST, SITE) matches with the (TEST, SITE) combination from the given obsid.  To do this, the
    list of relevant ODs is composed, based on the first and last timestamp in the DPU HK file (this file will always
    be present if data has been acquired).  Then all HDF5 files for these ODs are looped over and the obsid stored in
    these is compared with the given obsid.  In case of a match, the HDF5 filename is added to the list.

    Args:
        - obsid: Observation identifier [TEST_LAB or TEST_LAB_SETUP].
        - data_dir: Full path to the directory in which the data resides.  This is the folder with a sub-folder /daily,
                    in which the HDF5 files are stored.
    &#34;&#34;&#34;

    data_dir = data_dir or os.environ.get(&#39;PLATO_DATA_STORAGE_LOCATION&#39;)

    # Determine in which location (i.e. in the folder of which OD in the /daily sub-folder of the data directory)
    # the required HDF5 files are stored.  This sub-folder carries the OD [yyyymmdd] as name.

    od_list = get_od(obsid, data_dir)                # Obsid -&gt; OD
    LOGGER.info(f&#34;OD for obsid {obsid}: {od_list}&#34;)

    obs_hdf5_files = []

    for od in od_list:

        day_dir = Path(f&#34;{data_dir}/daily/{od}&#34;)    # Sub-folder with the data for that OD

        daily_hdf5_filenames = glob.glob(str(day_dir / f&#34;*.{HDF5.extension}&#34;))

        for hdf5_filename in sorted(daily_hdf5_filenames):

            try:
                with h5.get_file(hdf5_filename, mode=&#34;r&#34;, locking=False) as hdf5_file:

                    if &#34;/obsid&#34; in hdf5_file:

                        hdf5_obsid = h5.get_data(hdf5_file[&#34;/obsid&#34;]).item().decode()

                        if hdf5_obsid != &#34;None&#34;:
                            hdf5_obsid = ObservationIdentifier.create_from_string(
                                hdf5_obsid, LAB_SETUP_TEST).create_id(order=TEST_LAB)      # TEST_LAB

                            if hdf5_obsid in str(obsid):
                                obs_hdf5_files.append(hdf5_filename)

            except OSError as exc:
                LOGGER.error(f&#34;Couldn&#39;t open {hdf5_filename} ({exc=})&#34;)
            except RuntimeError as exc:
                LOGGER.debug(f&#34;Unable to open HDF5 file: {exc}&#34;)

    return obs_hdf5_files


def get_setup_for_obsid(obsid: str, data_dir: str = None) -&gt; Setup:
    &#34;&#34;&#34; Return the setup for the given obsid.

    The given obsid can be specified in either of these two formats: TEST_LAB or TEST_LAB_SETUP. In this method, we
    determine from the CM HK file (this file will always be present) for the given observation which is the setup that
    was used during the given obsid.  This file resides in the folder of the given obsid in the /obs directory, with the
    name (i.e. obsid) in the format TEST_SITE_SETUP or TEST_SITE (depending on how old the observation is).  The obsid
    that is used in the filename follows the same pattern, so the given obsid must be converted to that format.

    Args:
        - obsid: Observation identifier [TEST_LAB or TEST_LAB_SETUP].
        - data_dir: Full path to the directory in which the data resides.  This is the folder with a sub-folder /daily,
                    in which the HDF5 files are stored.

    Returns: Setup for the given obsid.
    &#34;&#34;&#34;

    data_dir = data_dir or os.environ[&#34;PLATO_DATA_STORAGE_LOCATION&#34;]
    obsid = obsid_from_storage(obsid, data_dir=data_dir)     # Convert the obsid to the correct format
    obs_dir = f&#34;{data_dir}/obs/{obsid}&#34;

    try:
        cm_hk = str(find_file(f&#34;{obsid}_CM_*.csv&#34;, root=obs_dir))
        last_line = read_last_line(cm_hk).split(&#34;,&#34;)
        site_id = last_line[1]
        setup_id = int(last_line[2])

        return load_setup(setup_id=setup_id, site_id=site_id, from_disk=True)
    except IndexError:
        raise Abort(f&#34;DPU was not running during obsid {obsid}: no data could be acquired&#34;)


def get_od(obsid: str, data_dir: str = None):
    &#34;&#34;&#34; Return list of OD(s) for the given obsid.

    The given obsid can be specified in either of these two formats: TEST_LAB or TEST_LAB_SETUP. In this method, we
    determine during which OD(s) the given obsid was executed.  To do this, the first and last timestamp from the DPU HK
    file (this file will always be present if data has been acquired) are extracted.  This file resides in the folder of
    the given obsid in the /obs directory, with the name (i.e. obsid) in the format TEST_SITE_SETUP or TEST_SITE
    (depending on how old the observation is).  The obsid that is used in the filename follows the same pattern, so the
    given obsid must be converted to that format.

    Args:
         - obsid: Observation identifier [TEST_LAB or TEST_LAB_SETUP].
         - data_dir: Full path to the directory in which the data resides.  This is the folder with a sub-folder /daily,
                    in which the HDF5 files are stored.

    Returns: List of observation day [yyyymmdd].
    &#34;&#34;&#34;

    data_dir = data_dir or os.environ[&#34;PLATO_DATA_STORAGE_LOCATION&#34;]
    obsid = obsid_from_storage(obsid, data_dir=data_dir)     # Convert the obsid to the correct format
    obs_dir = f&#34;{data_dir}/obs/{obsid}&#34;

    try:
        filename = str(find_file(f&#34;{obsid}_DPU_*.csv&#34;, root=obs_dir))

        od_start = datetime.strptime(filename.split(&#34;_&#34;)[-2], &#34;%Y%m%d&#34;)         # First OD (from filename)
        od_end = datetime.strptime(read_last_line(filename)[:10], &#34;%Y-%m-%d&#34;)   # Last OD (from last line)

        od = od_start
        delta = timedelta(days=1)
        od_list = []

        while od &lt;= od_end:

            od_list.append(od.strftime(&#34;%Y%m%d&#34;))

            od += delta

        return od_list
    except IndexError:
        raise Abort(f&#34;DPU was not running during obsid {obsid}: no data could be acquired&#34;)


def get_obsid(od: str, index: int, day_dir: str) -&gt; int:
    &#34;&#34;&#34; Return the obsid stored in the HDF5 file for the given OD and the given index.

    Args:
        - od: Observation day.
        - index: Index of the HDF5 file.
        - day_dir: Full path to the directory with the HDF5 files for the given OD.

    Returns: Obsid as stored in the HDF5 file for the given OD and the given index (LAB_SETUP_TEST).
    &#34;&#34;&#34;

    if index == 0:  # For the first file, no index is used
        hdf5_filename = f&#34;{day_dir}/{od}_{SITE.ID}_{N_FEE_SETTINGS.ORIGIN_SPW_DATA}.hdf5&#34;
    else:
        hdf5_filename = f&#34;{day_dir}/{od}_{SITE.ID}_{N_FEE_SETTINGS.ORIGIN_SPW_DATA}_{index:05d}.hdf5&#34;

    with h5.get_file(hdf5_filename, mode=&#34;r&#34;, locking=False) as hdf5_file:
        try:
            return int(hdf5_file[&#34;obsid&#34;][()].decode().split(&#34;_&#34;)[-1])
        except:
            return None


if __name__ == &#34;__main__&#34;:

    sys.exit(cli())</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="egse.dpu.fitsgen.add_synoptics"><code class="name flex">
<span>def <span class="ident">add_synoptics</span></span>(<span>obsid: str, data_dir=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Add synoptics to the FITS headers for the given observation.</p>
<p>When all FITS files have been produced for the given obsid, synoptics is added to the headers.
This is done in the
following steps:
- Determine which folder in the /obs directory comprises the HK and FITS files for the given obsid;
- Read the synoptics for the given obsid (from said folder) into a pandas DataFrame;
- Compose the list of FITS files for the given observation (from said folder);
- For all of these FITS files, loop over the cubes it contains and:
- Determine the time range covered by the cube;
- Select the synoptics (from the pandas DataFrame) over that time range;
- For the synoptical temperatures, source position (commanded + actual), and OGSE intensity: calculate
the average and add this to the header of the cube;
- For the shutter: calculate the mean and add this to the header of the cube.</p>
<h2 id="args">Args</h2>
<ul>
<li>obsid: Observation identifier [TEST_LAB or TEST_LAB_SETUP].</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_synoptics(obsid: str, data_dir=None):
    &#34;&#34;&#34; Add synoptics to the FITS headers for the given observation.

    When all FITS files have been produced for the given obsid, synoptics is added to the headers.  This is done in the
    following steps:
        - Determine which folder in the /obs directory comprises the HK and FITS files for the given obsid;
        - Read the synoptics for the given obsid (from said folder) into a pandas DataFrame;
        - Compose the list of FITS files for the given observation (from said folder);
        - For all of these FITS files, loop over the cubes it contains and:
                - Determine the time range covered by the cube;
                - Select the synoptics (from the pandas DataFrame) over that time range;
                - For the synoptical temperatures, source position (commanded + actual), and OGSE intensity: calculate
                  the average and add this to the header of the cube;
                - For the shutter: calculate the mean and add this to the header of the cube.

    Args:
        - obsid: Observation identifier [TEST_LAB or TEST_LAB_SETUP].
    &#34;&#34;&#34;

    data_dir = data_dir or os.environ.get(&#39;PLATO_DATA_STORAGE_LOCATION&#39;)

    obsid = obsid_from_storage(obsid, data_dir=data_dir)
    obs_dir = f&#34;{data_dir}/obs/{obsid}&#34;  # Where the HK and FITS files are stored

    synoptics = get_fits_synoptics(obsid, data_dir=data_dir)
    timestamps = synoptics[&#34;timestamps&#34;]    # Timestamps of the synoptics -&gt; compare with absolute time in FITS file

    # Compose the list of FITS files for the given obsid

    pattern = f&#34;{obsid}_{N_FEE_SETTINGS.ORIGIN_CCD_DATA}_*_*_cube.fits&#34;
    fits_filenames = sorted(find_files(pattern=pattern, root=obs_dir))

    # Loop over all FITS files (cubes) for the given obsid

    for fits_filename in fits_filenames:

        syn_info = {}

        # Loop over all image cubes

        with fits.open(fits_filename) as fits_file:

            start_time = time_since_epoch_1958(fits_file[&#34;PRIMARY&#34;].header[&#34;DATE-OBS&#34;])

            # Loop over both sides of all CCDs (not all of them might be in -&gt; hence the KeyError)

            for ccd_number in range(1, 5):

                for ccd_side in fee_side:

                    try:
                        # Absolute time = time at the start of the readout
                        #               = time at the end of the exposure
                        # -&gt; Extract relative time from the WCS-TAB and add the DATE-OBS (which is the time of the
                        #    1st frame in the FITS file; all times in the WCS-TAB are relative to this)

                        wcs_table_name = f&#34;WCS-TAB_{ccd_number}_{ccd_side.name[0]}&#34;     # Holds relative time
                        absolute_time = np.array(fits_file[wcs_table_name].data[&#34;TIME&#34;]) + start_time

                        # We don&#39;t care about the 1st frame of any CCD side, as the image is saturated anyway, and it is
                        # very difficult to determine the start of that exposure anyway
                        # -&gt; Simplest solution: indicate that the synoptics is unknown for those frames
                        # For all other frames:
                        #       - Determine when the readout for the previous frame started -&gt; start_previous_readout;
                        #       - Determine when the readout for the current frame started -&gt; start_current_readout;
                        #       - For each synoptics parameter, gather the values acquired in the timespan
                        #         [start_previous_readout, start_current_readout]
                        #       - For the numerical values: take the mean (skipping the NaNs)
                        #       - For the boolean values (i.c. status of the shutter):
                        #               - Only NaN selected -&gt; &#34;U&#34; (unknown)
                        #               - Both True &amp; False selected (potentially also NaNs) -&gt; &#34;M&#34; (mixed)
                        #               - Only True (potentially also NaNs) selected -&gt; &#34;T&#34; (True = shutter open)
                        #               - Only False (potentially also NaNs) selected -&gt; &#34;F&#34; (False = shutter closed)
                        #
                        # For each synoptical parameter, first determine all the values that need to be included in the
                        # current cube of the current FITS file (it is only when we have composed these arrays, that we
                        # can included them in a table in the FITS file)

                        fits_synoptics = {syn_enum: np.array([np.nan]) for syn_enum in chain(SynopticsFwdFill,
                                                                                             SynopticsInterp1d)}
                        fits_synoptics.update({syn_enum: np.array([&#34;U&#34;]) for syn_enum in SynopticsLeaveGaps})

                        for index in range(1, len(absolute_time)):

                            # Selection of the synoptics for the current frame: based on timestamps:
                            #   - Start (start_previous_readout): start of the readout of the previous exposure
                            #   - End (start_current_readout): start of the readout of the current exposure

                            start_previous_readout = absolute_time[index - 1]
                            start_current_readout = absolute_time[index]

                            selection = np.where(timestamps &gt;= start_previous_readout) \
                                        and np.where(timestamps &lt;= start_current_readout)[0]

                            # Average (skipping NaNs)

                            for syn_enum in chain(SynopticsFwdFill, SynopticsInterp1d):
                                try:
                                    selected_values = synoptics[syn_enum][selection]
                                    average_value = np.nanmean(selected_values)

                                    fits_synoptics[syn_enum] = np.append(fits_synoptics[syn_enum], average_value)
                                except KeyError:
                                    fits_synoptics[syn_enum] = np.append(fits_synoptics[syn_enum], np.nan)

                            for syn_enum in SynopticsLeaveGaps:
                                try:
                                    selected_values = synoptics[syn_enum][selection].astype(float)
                                    selection = ~np.isnan(selected_values)

                                    if not np.any(selection):   # No data -&gt; &#34;U&#34; (unknown)
                                        value = &#34;U&#34;
                                    else:   # Use &#34;T&#34; (True) / &#34;F&#34; (False) only when unique (otherwise: &#34;M&#34; (mixed))
                                        unique_values = np.unique(selected_values[selection])
                                        value = str(bool(unique_values[0]))[0] if len(unique_values) == 1 else &#34;M&#34;
                                    fits_synoptics[syn_enum] = np.append(fits_synoptics[syn_enum], value)
                                except KeyError:    # &#34;U&#34; (unknown)
                                    fits_synoptics[syn_enum] = np.append(fits_synoptics[syn_enum], &#34;U&#34;)

                            # At this point, we have for each synoptical parameter an array of the values that need to
                            # be included in the FITS file.  We now put all this information in a dedicated table and
                            # add it to the FITS file.

                            syn_columns = []

                            for syn_enum in chain(SynopticsFwdFill, SynopticsInterp1d, SynopticsLeaveGaps):
                                column_format = &#34;A&#34; if syn_enum == SynopticsLeaveGaps.OGSHTTR else &#34;F&#34;

                                syn_column = fits.Column(syn_enum.value[0], format=column_format,
                                                         array=fits_synoptics[syn_enum])
                                syn_columns.append(syn_column)

                            syn_table = fits.BinTableHDU.from_columns(syn_columns)
                            syn_table.header[&#34;EXTNAME&#34;] = f&#34;SYN-TAB_{ccd_number}_{ccd_side.name[0]}&#34;

                            # merged_columns = wcs_table.columns + syn_table.columns
                            # merged_table = fits.BinTableHDU.from_columns(merged_columns)

                            syn_info[syn_table.header[&#34;EXTNAME&#34;]] = (syn_table.data, syn_table.header)
                    except KeyError:
                        pass

        for data in syn_info.values():
            fits.append(str(fits_filename), data[0], data[1])</code></pre>
</details>
</dd>
<dt id="egse.dpu.fitsgen.any_crucial_parameters_changed"><code class="name flex">
<span>def <span class="ident">any_crucial_parameters_changed</span></span>(<span>prep: dict, n_fee_state: Mapping[~KT, +VT_co])</span>
</code></dt>
<dd>
<div class="desc"><p>Check whether there is a change in crucial parameters.</p>
<p>Return True if any of the following parameters changed with respect to the revious check: v_start, v_end, h_end,
rows_final_dump, ccd_mode_config, and ccd_readout_order.</p>
<h2 id="args">Args</h2>
<ul>
<li>prep (dict): Current values for the crucial parameters.</li>
<li>n_fee_state: N-FEE state parameters or register map.
Returns: True if any of the values have changed, False otherwise.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def any_crucial_parameters_changed(prep: dict, n_fee_state: Mapping):
    &#34;&#34;&#34; Check whether there is a change in crucial parameters.

    Return True if any of the following parameters changed with respect to the revious check: v_start, v_end, h_end,
    rows_final_dump, ccd_mode_config, and ccd_readout_order.

    Args:
        - prep (dict): Current values for the crucial parameters.
        - n_fee_state: N-FEE state parameters or register map.

    Returns: True if any of the values have changed, False otherwise.
    &#34;&#34;&#34;

    v_start = n_fee_state[&#39;v_start&#39;]
    v_end = n_fee_state[&#39;v_end&#39;]
    h_end = n_fee_state[&#39;h_end&#39;]
    rows_final_dump = n_fee_state[&#39;n_final_dump&#39;]
    ccd_mode_config = n_fee_state[&#39;ccd_mode_config&#39;]
    ccd_readout_order = n_fee_state[&#39;ccd_readout_order&#39;]
    ccd_readout_order = convert_ccd_order_value(ccd_readout_order)

    for x, y in dict(
        v_start=v_start, v_end=v_end, h_end=h_end, rows_final_dump=rows_final_dump,
        ccd_mode_config=ccd_mode_config, ccd_readout_order=ccd_readout_order,
    ).items():
        if prep.get(x) != y:
            LOGGER.debug(f&#34;{x=}, {prep.get(x)=}, {y=}&#34;)
            return True

    return False</code></pre>
</details>
</dd>
<dt id="egse.dpu.fitsgen.clear_crucial_parameters"><code class="name flex">
<span>def <span class="ident">clear_crucial_parameters</span></span>(<span>prep: dict)</span>
</code></dt>
<dd>
<div class="desc"><p>Clear the crucial parameters from the given dictionary.</p>
<h2 id="args">Args</h2>
<ul>
<li>prep: Dictionary with crucial parameters.
Returns: Dictionary with the cleared crucial parameters.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clear_crucial_parameters(prep: dict):
    &#34;&#34;&#34; Clear the crucial parameters from the given dictionary.

    Args:
        - prep: Dictionary with crucial parameters.

    Returns: Dictionary with the cleared crucial parameters.
    &#34;&#34;&#34;

    prep[&#34;v_start&#34;] = None
    prep[&#34;v_end&#34;] = None
    prep[&#34;h_end&#34;] = None
    prep[&#34;rows_final_dump&#34;] = None
    prep[&#34;ccd_mode_config&#34;] = None
    prep[&#34;ccd_readout_order&#34;] = None

    return prep</code></pre>
</details>
</dd>
<dt id="egse.dpu.fitsgen.construct_cube_filename"><code class="name flex">
<span>def <span class="ident">construct_cube_filename</span></span>(<span>fits_filename: str) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Construct the filename for the level-2 FITS file.</p>
<p>The level-2 FITS file will have the data arranged in cubes, rather than in a flat structure.</p>
<h2 id="args">Args</h2>
<ul>
<li>fits_filename: Filename for the level-1 FITS file.
The level-1 FITS files has the data arranged in a flat
structure.
Returns: Filename for the level-2 FITS file.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def construct_cube_filename(fits_filename: str) -&gt; str:
    &#34;&#34;&#34; Construct the filename for the level-2 FITS file.

    The level-2 FITS file will have the data arranged in cubes, rather than in a flat structure.

    Args:
        - fits_filename: Filename for the level-1 FITS file.  The level-1 FITS files has the data arranged in a flat
                         structure.

    Returns: Filename for the level-2 FITS file.
    &#34;&#34;&#34;

    LOGGER.info(f&#34;Construct cube filename from {fits_filename}&#34;)

    # LOGGER.info(f&#34;Images: {&#39;images&#39; in fits_filename}&#34;)

    if &#34;images&#34; in fits_filename:
        return fits_filename.replace(&#34;images&#34;, &#34;cube&#34;)

    else:
        prefix, suffix = str(fits_filename).rsplit(&#39;_&#39;, 1)
        return f&#34;{prefix}_cube_{suffix}&#34;</code></pre>
</details>
</dd>
<dt id="egse.dpu.fitsgen.construct_images_filename"><code class="name flex">
<span>def <span class="ident">construct_images_filename</span></span>(<span>hdf5_filename: pathlib.PosixPath, obsid: <a title="egse.obsid.ObservationIdentifier" href="../obsid.html#egse.obsid.ObservationIdentifier">ObservationIdentifier</a> = None, location=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Construct the filename for the level-1 FITS file.</p>
<p>The level-1 FITS files has the data arranged in a flat structure.</p>
<h2 id="args">Args</h2>
<ul>
<li>identifier (str): Identifier for the source of the data, this string is usually what is sent in the <code>origin</code>
of the item dictionary.</li>
<li>ext (str): File extension: this depends on the persistence class that is used for storing the data.</li>
<li>obsid (ObservationIdentifier): Unique identifier for the observation (LAB_SETUP_TEST).</li>
<li>use_counter: Indicates whether or not a counter should be included in the filename.</li>
<li>location: Folder (with /daily and /obs sub-folders) in which the FITS files should be written (in a
dedicated directory in the /obs folder). If not specified, the <code>PLATO_DATA_STORAGE_LOCATION</code>
environment variable will be read out.
Returns: Full path to the file as a <code>PurePath</code>.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def construct_images_filename(hdf5_filename: PosixPath, obsid: ObservationIdentifier = None, location=None):
    &#34;&#34;&#34; Construct the filename for the level-1 FITS file.


    The level-1 FITS files has the data arranged in a flat structure.

    Args:
        - identifier (str): Identifier for the source of the data, this string is usually what is sent in the `origin`
                            of the item dictionary.
        - ext (str): File extension: this depends on the persistence class that is used for storing the data.
        - obsid (ObservationIdentifier): Unique identifier for the observation (LAB_SETUP_TEST).
        - use_counter: Indicates whether or not a counter should be included in the filename.
        - location: Folder (with /daily and /obs sub-folders) in which the FITS files should be written (in a
                    dedicated directory in the /obs folder). If not specified, the `PLATO_DATA_STORAGE_LOCATION`
                    environment variable will be read out.

    Returns: Full path to the file as a `PurePath`.
    &#34;&#34;&#34;

    location = location or os.environ.get(&#39;PLATO_DATA_STORAGE_LOCATION&#39;)

    if obsid is None:

        timestamp, site_id, _, _, counter = str.split(str.split(str(hdf5_filename), &#34;.&#34;)[0], &#34;_&#34;)
        fits_filename = f&#34;{timestamp}_{site_id}_{N_FEE_SETTINGS.ORIGIN_CCD_DATA}_{counter}_images.{FITS.extension}&#34;

        location += &#34;/daily/&#34;

        return str(Path(location) / timestamp / fits_filename)

    else:

        # Make sure that the FITS file ends up in the correct sub-folder
        #   - oldest data: TEST_LAB_SETUP
        #   - more recent data: TEST_LAB

        obsid = obsid_from_storage(obsid, data_dir=location)

        timestamp = str.split(str(hdf5_filename).split(&#34;/&#34;)[-1], &#34;_&#34;)[0]
        location += &#34;/obs/&#34;

        if not os.path.isdir(f&#34;{location}/{obsid}&#34;):
            os.makedirs(f&#34;{location}/{obsid}&#34;)
        location += f&#34;{obsid}/&#34;

        # Determine the filename

        pattern = f&#34;{obsid}_{N_FEE_SETTINGS.ORIGIN_CCD_DATA}_*_{timestamp}_cube.{FITS.extension}&#34;
        counter = get_fits_counter(location, pattern)

        fits_filename = f&#34;{obsid}_{N_FEE_SETTINGS.ORIGIN_CCD_DATA}_{counter:05d}_{timestamp}_images.{FITS.extension}&#34;

        return str(Path(location) / fits_filename)</code></pre>
</details>
</dd>
<dt id="egse.dpu.fitsgen.convert_to_cubes"><code class="name flex">
<span>def <span class="ident">convert_to_cubes</span></span>(<span>filename)</span>
</code></dt>
<dd>
<div class="desc"><p>Conversion of level-1 FITS files to level-2 FITS files.</p>
<p>After the conversion, the flat-structure FITS file is removed.</p>
<h2 id="args">Args</h2>
<ul>
<li>filename: Full path of the level-1 FITS file.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def convert_to_cubes(filename):
    &#34;&#34;&#34; Conversion of level-1 FITS files to level-2 FITS files.

    After the conversion, the flat-structure FITS file is removed.

    Args:
        - filename: Full path of the level-1 FITS file.
    &#34;&#34;&#34;

    cube_filename = construct_cube_filename(filename)
    LOGGER.info(f&#34;Converting to {cube_filename}&#34;)

    with fits.open(filename) as level1:

        primary_header = level1[&#34;PRIMARY&#34;].header

        selected_ccds = np.unique(primary_header[&#34;CCD_READOUT_ORDER&#34;][1:-1].split(&#34;, &#34;))  # str
        side_is_present = {ccd: {fee_side.E: 0, fee_side.F: 0} for ccd in selected_ccds}

        has_serial_overscan = primary_header[&#34;H_END&#34;] &gt;= \
            CCD_SETTINGS.LENGTH_SERIAL_PRESCAN + CCD_SETTINGS.NUM_COLUMNS // 2
        has_parallel_overscan = primary_header[&#34;V_END&#34;] &gt;= CCD_SETTINGS.NUM_ROWS

        # We are going to calculate the relative time since the very first exposure in the FITS file.  We don&#39;t know
        # here which CCD side of which CCD came in first, so we determine the start time here.

        start_time = time_since_epoch_1958(format_datetime(precision=6, width=9))   # Now (data will certainly be older)
        date_obs = None

        for ccd_number in selected_ccds:

            for ccd_side in fee_side:

                try:

                    time = level1[f&#34;IMAGE_{ccd_number}_{ccd_side.name[0]}&#34;, 0].header[&#34;DATE-OBS&#34;]

                    if time_since_epoch_1958(time) &lt; start_time:

                        start_time = time_since_epoch_1958(time)
                        date_obs = time

                    side_is_present[ccd_number][ccd_side] = True

                except KeyError:

                    side_is_present[ccd_number][ccd_side] = False

        primary_hdu = fits.PrimaryHDU()
        primary_header[&#34;DATE-OBS&#34;] = date_obs
        primary_header[&#34;LEVEL&#34;] = 2  # Cube structure
        primary_hdu.header = primary_header
        primary_hdu.writeto(cube_filename)

        for ccd_number in selected_ccds:

            for ccd_side in fee_side:

                if side_is_present[ccd_number][ccd_side]:

                    # Image

                    images = []
                    time_axis = np.array([])

                    exposure = 0

                    while True:

                        try:

                            slice = level1[f&#34;IMAGE_{ccd_number}_{ccd_side.name[0]}&#34;, exposure]

                            time = time_since_epoch_1958(slice.header[&#34;DATE-OBS&#34;])
                            time_axis = np.append(time_axis, time)

                            images.append(slice.data)

                            exposure += 1

                        except KeyError:

                            break

                    image_cube = np.stack(images)
                    del images

                    time_axis -= start_time
                    time_column = fits.Column(&#34;TIME&#34;, format=&#34;F&#34;, array=time_axis)
                    time_table = fits.BinTableHDU.from_columns([time_column])
                    time_table.header[&#34;EXTNAME&#34;] = f&#34;WCS-TAB_{ccd_number}_{ccd_side.name[0]}&#34;

                    fits.append(cube_filename, time_table.data, time_table.header)
                    fits.append(filename, time_table.data, time_table.header)

                    image_cube_header = level1[f&#34;IMAGE_{ccd_number}_{ccd_side.name[0]}&#34;, 0].header
                    image_cube_header[&#34;NAXIS&#34;] = (3, f&#34;Dimensionality of the image cube ({ccd_side.name[0]}-side)&#34;,)
                    image_cube_header[&#34;NAXIS3&#34;] = exposure
                    image_cube_header[&#34;CRPIX3&#34;] = 1
                    image_cube_header[&#34;CRVAL3&#34;] = start_time
                    image_cube_header[&#34;CTYPE3&#34;] = &#34;TIMETAB&#34;
                    image_cube_header[&#34;CUNIT3&#34;] = &#34;s&#34;
                    image_cube_header[&#34;PS3_0&#34;] = f&#34;WCS-TAB_{ccd_number}_{ccd_side.name[0]}&#34;
                    image_cube_header[&#34;PS3_1&#34;] = &#34;TIME&#34;

                    fits.append(cube_filename, image_cube, image_cube_header)

                    # Serial pre-scan

                    serial_prescans = []

                    exposure = 0

                    while True:

                        try:

                            serial_prescans.append(level1[f&#34;SPRE_{ccd_number}_{ccd_side.name[0]}&#34;, exposure].data)
                            exposure += 1

                        except KeyError:

                            break

                    serial_prescan_cube = np.stack(serial_prescans)
                    del serial_prescans

                    serial_prescan_cube_header = level1[f&#34;SPRE_{ccd_number}_{ccd_side.name[0]}&#34;, 0].header
                    serial_prescan_cube_header[&#34;NAXIS&#34;] = (3, f&#34;Dimensionality of the serial pre-scan cube ({ccd_side.name[0]}-side)&#34;,)
                    serial_prescan_cube_header[&#34;NAXIS3&#34;] = exposure
                    serial_prescan_cube_header[&#34;CRPIX3&#34;] = 1
                    serial_prescan_cube_header[&#34;CRVAL3&#34;] = start_time
                    serial_prescan_cube_header[&#34;CTYPE3&#34;] = &#34;TIMETAB&#34;
                    serial_prescan_cube_header[&#34;CUNIT3&#34;] = &#34;s&#34;
                    serial_prescan_cube_header[&#34;PS3_0&#34;] = f&#34;WCS-TAB_{ccd_number}_{ccd_side.name[0]}&#34;
                    serial_prescan_cube_header[&#34;PS3_1&#34;] = &#34;TIME&#34;

                    fits.append(cube_filename, serial_prescan_cube, serial_prescan_cube_header)

                    # Serial over-scan

                    if has_serial_overscan:

                        serial_overscans = []
                        exposure = 0

                        while True:

                            try:

                                serial_overscans.append(level1[f&#34;SOVER_{ccd_number}_{ccd_side.name[0]}&#34;, exposure].data)
                                exposure += 1

                            except KeyError:

                                break

                        serial_overscan_cube = np.stack(serial_overscans)
                        del serial_overscans

                        serial_overscan_cube_header = level1[f&#34;SOVER_{ccd_number}_{ccd_side.name[0]}&#34;, 0].header
                        serial_overscan_cube_header[&#34;NAXIS&#34;] = (3, f&#34;Dimensionality of the serial over-scan cube ({ccd_side.name[0]}-side)&#34;,)
                        serial_overscan_cube_header[&#34;NAXIS3&#34;] = exposure
                        serial_overscan_cube_header[&#34;CRPIX3&#34;] = 1
                        serial_overscan_cube_header[&#34;CRVAL3&#34;] = start_time
                        serial_overscan_cube_header[&#34;CTYPE3&#34;] = &#34;TIMETAB&#34;
                        serial_overscan_cube_header[&#34;CUNIT3&#34;] = &#34;s&#34;
                        serial_overscan_cube_header[&#34;PS3_0&#34;] = f&#34;WCS-TAB_{ccd_number}_{ccd_side.name[0]}&#34;
                        serial_overscan_cube_header[&#34;PS3_1&#34;] = &#34;TIME&#34;

                        fits.append(cube_filename, serial_overscan_cube, serial_overscan_cube_header)

                    # Parallel over-scan

                    if has_parallel_overscan:

                        parallel_overscans = []
                        exposure = 0

                        while True:

                            try:

                                parallel_overscans.append(level1[f&#34;POVER_{ccd_number}_{ccd_side.name[0]}&#34;, exposure].data)
                                exposure += 1

                            except KeyError:
                                break

                        parallel_overscan_cube = np.stack(parallel_overscans)
                        del parallel_overscans

                        parallel_overscan_cube_header = level1[f&#34;POVER_{ccd_number}_{ccd_side.name[0]}&#34;, 0].header
                        parallel_overscan_cube_header[&#34;NAXIS&#34;] = (3, f&#34;Dimensionality of the parallel over-scan cube ({ccd_side.name[0]}-side)&#34;,)
                        parallel_overscan_cube_header[&#34;NAXIS3&#34;] = exposure
                        parallel_overscan_cube_header[&#34;CRPIX3&#34;] = 1
                        parallel_overscan_cube_header[&#34;CRVAL3&#34;] = start_time
                        parallel_overscan_cube_header[&#34;CTYPE3&#34;] = &#34;TIMETAB&#34;
                        parallel_overscan_cube_header[&#34;CUNIT3&#34;] = &#34;s&#34;
                        parallel_overscan_cube_header[&#34;PS3_0&#34;] = f&#34;WCS-TAB_{ccd_number}_{ccd_side.name[0]}&#34;
                        parallel_overscan_cube_header[&#34;PS3_1&#34;] = &#34;TIME&#34;

                        fits.append(
                            cube_filename, parallel_overscan_cube, parallel_overscan_cube_header
                        )

    # Remove the level-1 FITS file

    LOGGER.info(f&#34;Removing flat-structure FITS file {filename}&#34;)
    os.remove(filename)</code></pre>
</details>
</dd>
<dt id="egse.dpu.fitsgen.create_fits_from_hdf5"><code class="name flex">
<span>def <span class="ident">create_fits_from_hdf5</span></span>(<span>files: List[~T], location: str = None, setup: <a title="egse.setup.Setup" href="../setup.html#egse.setup.Setup">Setup</a> = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Off-line generation of FITS files from HDF5 files with SpW packets.</p>
<p>When there is a change in crucial parameters, the current FITS file (if any) will be closed and a new one will be
created as soon and HDF5 file with data content is encountered (when the N-FEE is in full-image or full-image
pattern mode):</p>
<pre><code>- If there is a change in crucial parameters, close the current FITS file (if any).
- If there is a change in crucial parameter and the N-FEE is in full-image mode or in full-image pattern
  mode, or the N-FEE goes to full-image mode or full-image pattern mode, a new FITS file will be created.
- The content of the HDF5 files will be extracted and passed to the FITS persistence layer as SpW packets.
</code></pre>
<p>In the older HDF5 files, only the register map is stored, which does not always reflect the actual N-FEE state.
This is solved in the later version of the HDF5 files (format version &gt;= 2.0).
In these files, the current N-FEE
state is stored in each of the data groups.</p>
<p>It's possible that the first file in the list is incomplete, because it was already created by the time the
current observation started.
That file</p>
<h2 id="args">Args</h2>
<ul>
<li>files: List of filenames of the HDF5 files to use to create the FITS file.</li>
<li>location: Folder (with /daily and /obs sub-folders) in which the FITS files should be written (in a
dedicated directory in the /obs folder). If not specified, the <code>PLATO_DATA_STORAGE_LOCATION</code>
environment variable will be read out.</li>
<li>setup: Setup to retrieve information from.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_fits_from_hdf5(files: List, location: str = None, setup: Setup = None):
    &#34;&#34;&#34; Off-line generation of FITS files from HDF5 files with SpW packets.

    When there is a change in crucial parameters, the current FITS file (if any) will be closed and a new one will be
    created as soon and HDF5 file with data content is encountered (when the N-FEE is in full-image or full-image
    pattern mode):

        - If there is a change in crucial parameters, close the current FITS file (if any).
        - If there is a change in crucial parameter and the N-FEE is in full-image mode or in full-image pattern
          mode, or the N-FEE goes to full-image mode or full-image pattern mode, a new FITS file will be created.
        - The content of the HDF5 files will be extracted and passed to the FITS persistence layer as SpW packets.

    In the older HDF5 files, only the register map is stored, which does not always reflect the actual N-FEE state.
    This is solved in the later version of the HDF5 files (format version &gt;= 2.0).  In these files, the current N-FEE
    state is stored in each of the data groups.

    It&#39;s possible that the first file in the list is incomplete, because it was already created by the time the
    current observation started.  That file

    Args:
        - files: List of filenames of the HDF5 files to use to create the FITS file.
        - location: Folder (with /daily and /obs sub-folders) in which the FITS files should be written (in a
                    dedicated directory in the /obs folder). If not specified, the `PLATO_DATA_STORAGE_LOCATION`
                    environment variable will be read out.
        - setup: Setup to retrieve information from.
    &#34;&#34;&#34;

    location = location or os.environ.get(&#39;PLATO_DATA_STORAGE_LOCATION&#39;)

    # Loop over the filenames.  When you encounter an HDF5 file, check its format version.

    for filename in files:

        filename = Path(filename)

        if filename.suffix == f&#34;.{HDF5.extension}&#34;:

            try:

                with h5.get_file(filename, mode=&#34;r&#34;, locking=False) as hdf5_file:

                    # It happens that some of the HDF5 files are incomplete.  These should not be considered to determine
                    # whether the register map (original format version) or the N-FEE state (format version &gt;= 2.0) to
                    # determine the state of the crucial parameters.

                    if is_incomplete(hdf5_file):    # or is_corrupt(hdf5_file):

                        files = files[1:]

                    else:

                        # The N-FEE state is stored in the data groups of the HDF5 files (format version &gt;= 2.0)

                        if &#34;versions&#34; in hdf5_file:

                            version_attrs = hdf5_file[&#34;versions&#34;][&#34;format_version&#34;].attrs

                            if version_attrs[&#34;major_version&#34;] == 2:
                                create_fits_from_hdf5_nfee_state(files, location=location, setup=setup)
                                break

                            else:

                                version = f&#34;{version_attrs[&#39;major_version&#39;]}.{version_attrs[&#39;minor_version&#39;]}&#34;

                                raise AttributeError(f&#34;HDF5 file format version {version} cannot be handled by the FITS generator&#34;)

                        # The register map is stored (globally) in the HDF5 files

                        else:
                            create_fits_from_hdf5_register_map(files, location=location, setup=setup)
                            break

            except RuntimeError as exc:
                LOGGER.debug(f&#34;Unable to open HDF5 file: {exc}&#34;)</code></pre>
</details>
</dd>
<dt id="egse.dpu.fitsgen.create_fits_from_hdf5_nfee_state"><code class="name flex">
<span>def <span class="ident">create_fits_from_hdf5_nfee_state</span></span>(<span>files: List[~T], location: str = None, setup: <a title="egse.setup.Setup" href="../setup.html#egse.setup.Setup">Setup</a> = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Off-line generation of FITS files from HDF5 files with SpW packets.</p>
<p>When there is a change in crucial parameters, the current FITS file (if any) will be closed and a new one will be
created as soon and HDF5 file with data content is encountered (when the N-FEE is in full-image or full-image
pattern mode):</p>
<pre><code>- If there is a change in crucial parameters, close the current FITS file (if any).
- If there is a change in crucial parameter and the N-FEE is in full-image mode or in full-image pattern
  mode, or the N-FEE goes to full-image mode or full-image pattern mode, a new FITS file will be created.
- The content of the HDF5 files will be extracted and passed to the FITS persistence layer as SpW packets.
</code></pre>
<p>In the given HDF5 files, the N-FEE state is saved in all data groups, reflecting the actual N-FEE state (i.e.
solving the problem of the mismatch between the register map and the N-FEE state).</p>
<h2 id="args">Args</h2>
<ul>
<li>files: List of filenames of the HDF5 files to use to create the FITS file.</li>
<li>location: Folder (with /daily and /obs sub-folders) in which the FITS files should be written (in a
dedicated directory in the /obs folder). If not specified, the <code>PLATO_DATA_STORAGE_LOCATION</code>
environment variable will be read out.</li>
<li>setup: Setup to retrieve information from.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_fits_from_hdf5_nfee_state(files: List, location: str = None, setup: Setup = None):
    &#34;&#34;&#34; Off-line generation of FITS files from HDF5 files with SpW packets.

    When there is a change in crucial parameters, the current FITS file (if any) will be closed and a new one will be
    created as soon and HDF5 file with data content is encountered (when the N-FEE is in full-image or full-image
    pattern mode):

        - If there is a change in crucial parameters, close the current FITS file (if any).
        - If there is a change in crucial parameter and the N-FEE is in full-image mode or in full-image pattern
          mode, or the N-FEE goes to full-image mode or full-image pattern mode, a new FITS file will be created.
        - The content of the HDF5 files will be extracted and passed to the FITS persistence layer as SpW packets.

    In the given HDF5 files, the N-FEE state is saved in all data groups, reflecting the actual N-FEE state (i.e.
    solving the problem of the mismatch between the register map and the N-FEE state).

    Args:
        - files: List of filenames of the HDF5 files to use to create the FITS file.
        - location: Folder (with /daily and /obs sub-folders) in which the FITS files should be written (in a
                    dedicated directory in the /obs folder). If not specified, the `PLATO_DATA_STORAGE_LOCATION`
                    environment variable will be read out.
        - setup: Setup to retrieve information from.
    &#34;&#34;&#34;

    location = location or os.environ.get(&#39;PLATO_DATA_STORAGE_LOCATION&#39;)

    config_slicing_num_cycles = 0    # Configured slicing parameter
    processed_num_cycles = 0            # HDF5 files with image data processed for current FITS file

    prep = {}
    fits_filename = None

    for filename in files:

        filename = Path(filename)

        if filename.suffix == &#39;.hdf5&#39;:

            print(f&#34;Processing {filename=!s}...&#34;)

            try:

                with h5.get_file(filename, mode=&#34;r&#34;, locking=False) as hdf5_file:

                    # Slicing

                    try:
                        slicing_num_cycles = hdf5_file[&#34;dpu&#34;].attrs[&#34;slicing_num_cycles&#34;]
                        if slicing_num_cycles != config_slicing_num_cycles:
                            if fits_filename:
                                convert_to_cubes(fits_filename)
                                fits_filename = None
                            processed_num_cycles = 0
                            config_slicing_num_cycles = slicing_num_cycles
                    except KeyError:
                        config_slicing_num_cycles = 0

                    # if is_corrupt(hdf5_file):
                    #     LOGGER.warning(f&#34;Skipping {filename} (corrupt)&#34;)
                    #
                    # else:

                    has_data = False

                    for group in h5.groups(hdf5_file):

                        if &#34;data&#34; in group.keys():

                            has_data = True

                            n_fee_state = group[&#34;data&#34;].attrs

                            # Should a new FITS file be created?

                            if any_crucial_parameters_changed(prep, n_fee_state):

                                if fits_filename:

                                    LOGGER.info(f&#34;Creating a FITS CUBE file ...&#34;)
                                    convert_to_cubes(fits_filename)
                                    fits_filename = None
                                    processed_num_cycles = 0

                            if in_data_acquisition_mode(n_fee_state):

                                if fits_filename is None:

                                    LOGGER.info(f&#34;A new FITS file will be created...&#34;)

                                    # Start writing to a new FITS file
                                    # Collect all information to sent to the FITS layer

                                    if &#34;obsid&#34; in hdf5_file:
                                        obsid = hdf5_file[&#34;obsid&#34;][()].decode()
                                        obsid = ObservationIdentifier.create_from_string(obsid, order=LAB_SETUP_TEST)
                                    else:
                                        obsid = None

                                    fits_filename = construct_images_filename(filename, obsid, location=location)
                                    LOGGER.info(f&#34;{fits_filename = !s}&#34;)

                                    ccd_readout_order = n_fee_state[&#39;ccd_readout_order&#39;]
                                    ccd_readout_order = convert_ccd_order_value(ccd_readout_order)

                                    prep = {
                                        &#34;v_start&#34;: n_fee_state[&#39;v_start&#39;],
                                        &#34;v_end&#34;: n_fee_state[&#39;v_end&#39;],
                                        &#34;h_end&#34;: n_fee_state[&#39;h_end&#39;],
                                        &#34;rows_final_dump&#34;: n_fee_state[&#39;n_final_dump&#39;],
                                        &#34;ccd_mode_config&#34;: n_fee_state[&#39;ccd_mode_config&#39;],
                                        &#34;ccd_readout_order&#34;: ccd_readout_order,  # CCD numbering [1-4]
                                        &#34;expected_last_packet_flags&#34;: get_expected_last_packet_flags(n_fee_state),
                                        &#34;obsid&#34;: str(obsid) if obsid is not None else None,
                                        &#34;cycle_time&#34;: get_cycle_time(n_fee_state, obsid=obsid, data_dir=location),
                                        &#34;cgse_version&#34;: get_cgse_version(obsid=obsid, data_dir=location),
                                        &#34;setup&#34;: setup
                                    }

                                    persistence = FITS(str(fits_filename), prep)
                                    persistence.open()

                                # See https://github.com/IvS-KULeuven/plato-common-egse/issues/901
                                # timecode = group[&#34;timecode&#34;]
                                # spw_packet = SpaceWirePacket.create_packet(h5.get_data(timecode))

                                timestamp = group[&#34;timecode&#34;].attrs[&#34;timestamp&#34;]
                                persistence.create({&#34;Timestamp&#34;: timestamp})

                                data = group[&#34;data&#34;]
                                sorted_datasets = natsort.natsorted(data.items(), key=lambda x: x[0])

                                persistence.expected_last_packet_flags = get_expected_last_packet_flags(n_fee_state)

                                for identifier, dataset in sorted_datasets:

                                    spw_packet = SpaceWirePacket.create_packet(h5.get_data(dataset))
                                    # LOGGER.debug(f&#34;{spw_packet.type = !s}&#34;)
                                    persistence.create({f&#34;SpW packet {identifier}&#34;: spw_packet})

                    if has_data:
                        processed_num_cycles += 1

                        if fits_filename and config_slicing_num_cycles != 0 \
                                and processed_num_cycles == config_slicing_num_cycles:
                            convert_to_cubes(fits_filename)
                            fits_filename = None
                            processed_num_cycles = 0
                    else:

                        if fits_filename:
                            LOGGER.info(f&#34;Creating a FITS CUBE file ...&#34;)
                            convert_to_cubes(fits_filename)
                            fits_filename = None
                            processed_num_cycles = 0

                        prep = clear_crucial_parameters(prep)

            except RuntimeError as exc:
                LOGGER.debug(f&#34;Unable to open HDF5 file: {exc}&#34;)
        else:
            print(f&#34;skipping {filename=}&#34;)

    try:
        if fits_filename:
            LOGGER.info(f&#34;Creating a FITS CUBE file ...&#34;)
            convert_to_cubes(fits_filename)
    except OSError:
        # The last file in the list still contained data, so we reached the end of the list without creating a cube
        # FITS file yet
        pass</code></pre>
</details>
</dd>
<dt id="egse.dpu.fitsgen.create_fits_from_hdf5_register_map"><code class="name flex">
<span>def <span class="ident">create_fits_from_hdf5_register_map</span></span>(<span>files: List[~T], location: str = None, setup: <a title="egse.setup.Setup" href="../setup.html#egse.setup.Setup">Setup</a> = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Off-line generation of FITS files from HDF5 files with SpW packets.</p>
<p>When there is a change in crucial parameters, the current FITS file (if any) will be closed and a new one will be
created as soon and HDF5 file with data content is encountered (when the N-FEE is in full-image or full-image
pattern mode):</p>
<pre><code>- If there is a change in crucial parameters, close the current FITS file (if any).
- If there is a change in crucial parameter and the N-FEE is in full-image mode or in full-image pattern
  mode, or the N-FEE goes to full-image mode or full-image pattern mode, a new FITS file will be created.
- The content of the HDF5 files will be extracted and passed to the FITS persistence layer as SpW packets.
</code></pre>
<p>In the given HDF5 files, only the register map is stored, which does not always reflect the actual N-FEE state. As
a result not all data may be present in the generated FITS files (e.g. because the register map says the N-FEE is
already in dump mode) or the data might be split over more FITS files than expected (e.g. because the v_start and
v_end parameters are already / not yet changed in the register map but not in the N-FEE state).</p>
<p>Note that this problem is solved in the later version of the HDF5 files (format version &gt;= 2.0).</p>
<h2 id="args">Args</h2>
<ul>
<li>files: List of filenames of the HDF5 files to use to create the FITS file.</li>
<li>location: Folder (with /daily and /obs sub-folders) in which the FITS files should be written (in a
dedicated directory in the /obs folder). If not specified, the <code>PLATO_DATA_STORAGE_LOCATION</code>
environment variable will be read out.</li>
<li>setup: Setup to retrieve information from.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_fits_from_hdf5_register_map(files: List, location: str = None, setup: Setup = None):
    &#34;&#34;&#34; Off-line generation of FITS files from HDF5 files with SpW packets.

    When there is a change in crucial parameters, the current FITS file (if any) will be closed and a new one will be
    created as soon and HDF5 file with data content is encountered (when the N-FEE is in full-image or full-image
    pattern mode):

        - If there is a change in crucial parameters, close the current FITS file (if any).
        - If there is a change in crucial parameter and the N-FEE is in full-image mode or in full-image pattern
          mode, or the N-FEE goes to full-image mode or full-image pattern mode, a new FITS file will be created.
        - The content of the HDF5 files will be extracted and passed to the FITS persistence layer as SpW packets.

    In the given HDF5 files, only the register map is stored, which does not always reflect the actual N-FEE state. As
    a result not all data may be present in the generated FITS files (e.g. because the register map says the N-FEE is
    already in dump mode) or the data might be split over more FITS files than expected (e.g. because the v_start and
    v_end parameters are already / not yet changed in the register map but not in the N-FEE state).

    Note that this problem is solved in the later version of the HDF5 files (format version &gt;= 2.0).

    Args:
        - files: List of filenames of the HDF5 files to use to create the FITS file.
        - location: Folder (with /daily and /obs sub-folders) in which the FITS files should be written (in a
                    dedicated directory in the /obs folder). If not specified, the `PLATO_DATA_STORAGE_LOCATION`
                    environment variable will be read out.
        - setup: Setup to retrieve information from.
    &#34;&#34;&#34;

    location = location or os.environ.get(&#39;PLATO_DATA_STORAGE_LOCATION&#39;)

    prep = {}
    fits_filename = None

    for filename in files:

        filename = Path(filename)

        if filename.suffix == &#39;.hdf5&#39;:

            print(f&#34;Processing {filename=!s}...&#34;)

            try:

                with h5.get_file(filename, mode=&#34;r&#34;, locking=False) as hdf5_file:

                    # if is_corrupt(hdf5_file):
                    #     LOGGER.warning(f&#34;Skipping {filename} (corrupt)&#34;)
                    #
                    # else:

                    if &#39;register&#39; not in hdf5_file:
                        LOGGER.warning(f&#34;No register map found for {filename=!s}, continue with next file..&#34;)
                        continue  # next HDF5 file

                    register_map = RegisterMap(&#34;N-FEE&#34;, memory_map=h5.get_data(hdf5_file[&#34;register&#34;]))

                    has_data = False

                    for group in h5.groups(hdf5_file):

                        if &#34;data&#34; in group.keys():

                            has_data = True

                            # Should a new FITS file be created?

                            if any_crucial_parameters_changed(prep, register_map):

                                if fits_filename:

                                    LOGGER.info(f&#34;Creating a FITS CUBE file ...&#34;)
                                    convert_to_cubes(fits_filename)
                                    fits_filename = None

                            if in_data_acquisition_mode(register_map):

                                if fits_filename is None:

                                    LOGGER.info(f&#34;A new FITS file will be created...&#34;)

                                    # Start writing to a new FITS file
                                    # Collect all information to sent to the FITS layer

                                    if &#34;obsid&#34; in hdf5_file:
                                        obsid = hdf5_file[&#34;obsid&#34;][()].decode()
                                        obsid = ObservationIdentifier.create_from_string(obsid, order=LAB_SETUP_TEST)
                                    else:
                                        obsid = None

                                    fits_filename = construct_images_filename(filename, obsid, location=location)
                                    LOGGER.info(f&#34;{fits_filename = !s}&#34;)

                                    ccd_readout_order = register_map[&#39;ccd_readout_order&#39;]
                                    ccd_readout_order = convert_ccd_order_value(ccd_readout_order)

                                    prep = {
                                        &#34;v_start&#34;: register_map[&#39;v_start&#39;],
                                        &#34;v_end&#34;: register_map[&#39;v_end&#39;],
                                        &#34;h_end&#34;: register_map[&#39;h_end&#39;],
                                        &#34;rows_final_dump&#34;: register_map[&#39;n_final_dump&#39;],
                                        &#34;ccd_mode_config&#34;: register_map[&#39;ccd_mode_config&#39;],
                                        &#34;ccd_readout_order&#34;: ccd_readout_order,  # CCD numbering [1-4]
                                        &#34;expected_last_packet_flags&#34;: get_expected_last_packet_flags(register_map),
                                        &#34;obsid&#34;: str(obsid) if obsid is not None else None,
                                        &#34;cycle_time&#34;: get_cycle_time(register_map, obsid=obsid, data_dir=location),
                                        &#34;cgse_version&#34;: get_cgse_version(obsid=obsid, data_dir=location),
                                        &#34;setup&#34;: setup
                                    }

                                    persistence = FITS(str(fits_filename), prep)
                                    persistence.open()

                                # See https://github.com/IvS-KULeuven/plato-common-egse/issues/901
                                # timecode = group[&#34;timecode&#34;]
                                # spw_packet = SpaceWirePacket.create_packet(h5.get_data(timecode))

                                timestamp = group[&#34;timecode&#34;].attrs[&#34;timestamp&#34;]
                                persistence.create({&#34;Timestamp&#34;: timestamp})

                                data = group[&#34;data&#34;]
                                sorted_datasets = natsort.natsorted(data.items(), key=lambda x: x[0])

                                persistence.expected_last_packet_flags = get_expected_last_packet_flags(register_map)

                                for identifier, dataset in sorted_datasets:

                                    spw_packet = SpaceWirePacket.create_packet(h5.get_data(dataset))
                                    # LOGGER.debug(f&#34;{spw_packet.type = !s}&#34;)
                                    persistence.create({f&#34;SpW packet {identifier}&#34;: spw_packet})

                    if not has_data:

                        if fits_filename:

                            LOGGER.info(f&#34;Creating a FITS CUBE file ...&#34;)
                            convert_to_cubes(fits_filename)
                            fits_filename = None

                        prep = clear_crucial_parameters(prep)

            except RuntimeError as exc:
                LOGGER.debug(f&#34;Unable to open HDF5 file: {exc}&#34;)
        else:
            print(f&#34;Skipping {filename=}&#34;)

    try:
        if fits_filename:
            LOGGER.info(f&#34;Creating a FITS CUBE file ...&#34;)
            convert_to_cubes(fits_filename)
    except OSError:
        # The last file in the list still contained data, so we reached the end of the list without creating a cube
        # FITS file yet
        pass</code></pre>
</details>
</dd>
<dt id="egse.dpu.fitsgen.get_cgse_version"><code class="name flex">
<span>def <span class="ident">get_cgse_version</span></span>(<span>obsid=None, data_dir=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the version of the Common EGSE with which the FITS file was created.</p>
<h2 id="args">Args</h2>
<ul>
<li>obsid: Observation identifier for which the version of the Common EGSE is read from the Configuration
Manager housekeeping.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_cgse_version(obsid=None, data_dir=None):
    &#34;&#34;&#34; Returns the version of the Common EGSE with which the FITS file was created.

    Args:
        - obsid: Observation identifier for which the version of the Common EGSE is read from the Configuration
                 Manager housekeeping.
    &#34;&#34;&#34;

    try:
        return None if obsid is None else get_housekeeping(&#34;CM_CGSE_VERSION&#34;, obsid=obsid, data_dir=data_dir)[1]
    except HKError:
        return None</code></pre>
</details>
</dd>
<dt id="egse.dpu.fitsgen.get_cycle_time"><code class="name flex">
<span>def <span class="ident">get_cycle_time</span></span>(<span>n_fee_state: Mapping[~KT, +VT_co], obsid=None, data_dir=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the image cycle time.</p>
<p>In the given N-FEE state parameters or register map, we check whether we are in internal or external sync:</p>
<pre><code>- Internal sync: Read the image cycle time from the given N-FEE state parameters or register map;
- External sync: Get the image cycle time from the AEU (AWG2).  In case of off-line FITS generation (i.e. from
  the HDF5 files), the image cycle time (for the specified obsid) is taken from the AEU housekeeping (AWG2).
  In case of on-line FITS generation, the image cycle time is queried from the AEU AWG2.
</code></pre>
<h2 id="args">Args</h2>
<ul>
<li>n_fee_state: N-FEE state parameters or register map.</li>
<li>obsid: Observation identifier for which the image cycle time is read from the AEU housekeeping.
Returns: Image cycle time [s].</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_cycle_time(n_fee_state: Mapping, obsid=None, data_dir=None):
    &#34;&#34;&#34; Return the image cycle time.

    In the given N-FEE state parameters or register map, we check whether we are in internal or external sync:

        - Internal sync: Read the image cycle time from the given N-FEE state parameters or register map;
        - External sync: Get the image cycle time from the AEU (AWG2).  In case of off-line FITS generation (i.e. from
          the HDF5 files), the image cycle time (for the specified obsid) is taken from the AEU housekeeping (AWG2).
          In case of on-line FITS generation, the image cycle time is queried from the AEU AWG2.

    Args:
        - n_fee_state: N-FEE state parameters or register map.
        - obsid: Observation identifier for which the image cycle time is read from the AEU housekeeping.

    Returns: Image cycle time [s].
    &#34;&#34;&#34;

    # Internal sync -&gt; use sync period from the N-FEE state

    if n_fee_state[&#34;sync_sel&#34;] == 1:
        return n_fee_state[&#34;int_sync_period&#34;] / 1000.    # [ms] -&gt; [s]

    # External sync -&gt; use AEU sync pulses

    else:
        if obsid:
            try:
                return float(get_housekeeping(&#34;GAEU_EXT_CYCLE_TIME&#34;, obsid=obsid, data_dir=data_dir)[1])
            except HKError as exc:  # See GitHub issue #2025
                LOGGER.warning(&#34;No HK available for AWG2 (using default cycle time of 25s)&#34;, exc)
                return 25.0
        else:
            return None</code></pre>
</details>
</dd>
<dt id="egse.dpu.fitsgen.get_fits_counter"><code class="name flex">
<span>def <span class="ident">get_fits_counter</span></span>(<span>location, pattern)</span>
</code></dt>
<dd>
<div class="desc"><p>Determine counter for a new FITS file at the given location and with the given pattern.</p>
<h2 id="args">Args</h2>
<ul>
<li>location: Location where the FITS file should be stored.</li>
<li>pattern: Pattern for the filename.
Returns: Value of the next counter; 1 if no previous files were found or if an error occurred.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_fits_counter(location, pattern):
    &#34;&#34;&#34; Determine counter for a new FITS file at the given location and with the given pattern.

    Args:
        - location: Location where the FITS file should be stored.
        - pattern: Pattern for the filename.

    Returns: Value of the next counter; 1 if no previous files were found or if an error occurred.
    &#34;&#34;&#34;

    LOGGER.debug(f&#34;Pattern: {pattern=}&#34;)
    LOGGER.debug(f&#34;Location: {location=}&#34;)

    files = sorted(find_files(pattern=pattern, root=location))

    # No filenames found showing the given pattern -&gt; start counting at 1

    LOGGER.debug(f&#34;Number of matches: {len(files)=}&#34;)

    if len(files) == 0:
        return 1

    last_file = files[-1]

    counter = last_file.name.split(&#34;_&#34;)

    LOGGER.debug(f&#34;{counter = }&#34;)

    try:

        # Observation files have the following pattern:
        #  &lt;test ID&gt;_&lt;lab ID&gt;_N-FEE_CCD_&lt;counter&gt;_&lt;day YYYYmmdd&gt;_cube.fits

        counter = int(counter[-3]) + 1
        LOGGER.debug(f&#34;{counter = }&#34;)
        return counter

    except ValueError:

        LOGGER.warning(&#34;ValueError&#34;, exc_info=True)
        return 1</code></pre>
</details>
</dd>
<dt id="egse.dpu.fitsgen.get_fits_synoptics"><code class="name flex">
<span>def <span class="ident">get_fits_synoptics</span></span>(<span>obsid: str, data_dir=None) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Retrieve the synoptics that need to be included in the FITS files for the given observation.</p>
<p>The synoptics that need be be included in the FITS files are represented by the following enumerations:</p>
<pre><code>- SynopticsFwdFill: Use forward filling for the gaps -&gt; only at the beginning of the observation it is possible
  that there still are gaps (but it is unlikely that the data acquisition has already started then);
- SynopticsInterp1d: Use linear interpolation to fill the gaps.  At the extremes, we use extrapolation;
- SynopticsLeaveGaps: Don't fill the gaps.
</code></pre>
<h2 id="args">Args</h2>
<ul>
<li>obsid: Observation identifier [TEST_LAB or TEST_LAB_SETUP].
Returns: Dictionary with the synoptics that should go into the FITS files for the given observation.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_fits_synoptics(obsid: str, data_dir=None) -&gt; dict:
    &#34;&#34;&#34; Retrieve the synoptics that need to be included in the FITS files for the given observation.

    The synoptics that need be be included in the FITS files are represented by the following enumerations:

        - SynopticsFwdFill: Use forward filling for the gaps -&gt; only at the beginning of the observation it is possible
          that there still are gaps (but it is unlikely that the data acquisition has already started then);
        - SynopticsInterp1d: Use linear interpolation to fill the gaps.  At the extremes, we use extrapolation;
        - SynopticsLeaveGaps: Don&#39;t fill the gaps.

    Args:
        - obsid: Observation identifier [TEST_LAB or TEST_LAB_SETUP].

    Returns: Dictionary with the synoptics that should go into the FITS files for the given observation.
    &#34;&#34;&#34;

    synoptics_table = get_synoptics_table(obsid, data_dir=data_dir)

    # We keep the original timestamps (when filling the gaps)

    timestamps = synoptics_table[&#34;timestamp&#34;].values
    for index in range(len(timestamps)):
        timestamps[index] = time_since_epoch_1958(timestamps[index])
    timestamps = timestamps.astype(float)

    synoptics = {&#34;timestamps&#34;: timestamps}  # Don&#39;t forget to include the timestamps to the returned dictionary

    # Linear interpolation

    for syn_enum in SynopticsInterp1d:

        syn_name = syn_enum.value[0]

        if syn_name in synoptics_table:

            # We need to filter out the NaNs or the interpolation will not work

            values = synoptics_table[syn_name].values
            selection = ~np.isnan(values)

            if np.any(selection):
                interpolation = interp1d(timestamps[np.where(selection)], values[np.where(selection)], kind=&#39;linear&#39;,
                                         fill_value=&#39;extrapolate&#39;)
                synoptics[syn_enum] = interpolation(timestamps)

    # Forward fill

    for syn_enum in SynopticsFwdFill:

        syn_name = syn_enum.value[0]

        if syn_name in synoptics_table:
            synoptics[syn_enum] = synoptics_table[syn_name].ffill()

    # Leave the gaps in

    for syn_enum in SynopticsLeaveGaps:

        syn_name = syn_enum.value[0]

        if syn_name in synoptics_table:
            synoptics[syn_enum] = synoptics_table[syn_name]

    return synoptics</code></pre>
</details>
</dd>
<dt id="egse.dpu.fitsgen.get_hdf5_filenames_for_obsid"><code class="name flex">
<span>def <span class="ident">get_hdf5_filenames_for_obsid</span></span>(<span>obsid: str, data_dir: str = None) ‑> List[~T]</span>
</code></dt>
<dd>
<div class="desc"><p>Return list of HDF5 filenames that contribute to the given obsid.</p>
<p>The given obsid can be specified in either of these two formats: TEST_LAB or TEST_LAB_SETUP.
The obsid that is
stored in the HDF5 files is of format LAB_SETUP_TEST.
In this method, we gather the list of HDF5 filenames for
which the combination (TEST, SITE) matches with the (TEST, SITE) combination from the given obsid.
To do this, the
list of relevant ODs is composed, based on the first and last timestamp in the DPU HK file (this file will always
be present if data has been acquired).
Then all HDF5 files for these ODs are looped over and the obsid stored in
these is compared with the given obsid.
In case of a match, the HDF5 filename is added to the list.</p>
<h2 id="args">Args</h2>
<ul>
<li>obsid: Observation identifier [TEST_LAB or TEST_LAB_SETUP].</li>
<li>data_dir: Full path to the directory in which the data resides.
This is the folder with a sub-folder /daily,
in which the HDF5 files are stored.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_hdf5_filenames_for_obsid(obsid: str, data_dir: str = None) -&gt; List:
    &#34;&#34;&#34; Return list of HDF5 filenames that contribute to the given obsid.

    The given obsid can be specified in either of these two formats: TEST_LAB or TEST_LAB_SETUP.  The obsid that is
    stored in the HDF5 files is of format LAB_SETUP_TEST.  In this method, we gather the list of HDF5 filenames for
    which the combination (TEST, SITE) matches with the (TEST, SITE) combination from the given obsid.  To do this, the
    list of relevant ODs is composed, based on the first and last timestamp in the DPU HK file (this file will always
    be present if data has been acquired).  Then all HDF5 files for these ODs are looped over and the obsid stored in
    these is compared with the given obsid.  In case of a match, the HDF5 filename is added to the list.

    Args:
        - obsid: Observation identifier [TEST_LAB or TEST_LAB_SETUP].
        - data_dir: Full path to the directory in which the data resides.  This is the folder with a sub-folder /daily,
                    in which the HDF5 files are stored.
    &#34;&#34;&#34;

    data_dir = data_dir or os.environ.get(&#39;PLATO_DATA_STORAGE_LOCATION&#39;)

    # Determine in which location (i.e. in the folder of which OD in the /daily sub-folder of the data directory)
    # the required HDF5 files are stored.  This sub-folder carries the OD [yyyymmdd] as name.

    od_list = get_od(obsid, data_dir)                # Obsid -&gt; OD
    LOGGER.info(f&#34;OD for obsid {obsid}: {od_list}&#34;)

    obs_hdf5_files = []

    for od in od_list:

        day_dir = Path(f&#34;{data_dir}/daily/{od}&#34;)    # Sub-folder with the data for that OD

        daily_hdf5_filenames = glob.glob(str(day_dir / f&#34;*.{HDF5.extension}&#34;))

        for hdf5_filename in sorted(daily_hdf5_filenames):

            try:
                with h5.get_file(hdf5_filename, mode=&#34;r&#34;, locking=False) as hdf5_file:

                    if &#34;/obsid&#34; in hdf5_file:

                        hdf5_obsid = h5.get_data(hdf5_file[&#34;/obsid&#34;]).item().decode()

                        if hdf5_obsid != &#34;None&#34;:
                            hdf5_obsid = ObservationIdentifier.create_from_string(
                                hdf5_obsid, LAB_SETUP_TEST).create_id(order=TEST_LAB)      # TEST_LAB

                            if hdf5_obsid in str(obsid):
                                obs_hdf5_files.append(hdf5_filename)

            except OSError as exc:
                LOGGER.error(f&#34;Couldn&#39;t open {hdf5_filename} ({exc=})&#34;)
            except RuntimeError as exc:
                LOGGER.debug(f&#34;Unable to open HDF5 file: {exc}&#34;)

    return obs_hdf5_files</code></pre>
</details>
</dd>
<dt id="egse.dpu.fitsgen.get_obsid"><code class="name flex">
<span>def <span class="ident">get_obsid</span></span>(<span>od: str, index: int, day_dir: str) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Return the obsid stored in the HDF5 file for the given OD and the given index.</p>
<h2 id="args">Args</h2>
<ul>
<li>od: Observation day.</li>
<li>index: Index of the HDF5 file.</li>
<li>day_dir: Full path to the directory with the HDF5 files for the given OD.
Returns: Obsid as stored in the HDF5 file for the given OD and the given index (LAB_SETUP_TEST).</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_obsid(od: str, index: int, day_dir: str) -&gt; int:
    &#34;&#34;&#34; Return the obsid stored in the HDF5 file for the given OD and the given index.

    Args:
        - od: Observation day.
        - index: Index of the HDF5 file.
        - day_dir: Full path to the directory with the HDF5 files for the given OD.

    Returns: Obsid as stored in the HDF5 file for the given OD and the given index (LAB_SETUP_TEST).
    &#34;&#34;&#34;

    if index == 0:  # For the first file, no index is used
        hdf5_filename = f&#34;{day_dir}/{od}_{SITE.ID}_{N_FEE_SETTINGS.ORIGIN_SPW_DATA}.hdf5&#34;
    else:
        hdf5_filename = f&#34;{day_dir}/{od}_{SITE.ID}_{N_FEE_SETTINGS.ORIGIN_SPW_DATA}_{index:05d}.hdf5&#34;

    with h5.get_file(hdf5_filename, mode=&#34;r&#34;, locking=False) as hdf5_file:
        try:
            return int(hdf5_file[&#34;obsid&#34;][()].decode().split(&#34;_&#34;)[-1])
        except:
            return None</code></pre>
</details>
</dd>
<dt id="egse.dpu.fitsgen.get_od"><code class="name flex">
<span>def <span class="ident">get_od</span></span>(<span>obsid: str, data_dir: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Return list of OD(s) for the given obsid.</p>
<p>The given obsid can be specified in either of these two formats: TEST_LAB or TEST_LAB_SETUP. In this method, we
determine during which OD(s) the given obsid was executed.
To do this, the first and last timestamp from the DPU HK
file (this file will always be present if data has been acquired) are extracted.
This file resides in the folder of
the given obsid in the /obs directory, with the name (i.e. obsid) in the format TEST_SITE_SETUP or TEST_SITE
(depending on how old the observation is).
The obsid that is used in the filename follows the same pattern, so the
given obsid must be converted to that format.</p>
<h2 id="args">Args</h2>
<ul>
<li>obsid: Observation identifier [TEST_LAB or TEST_LAB_SETUP].</li>
<li>data_dir: Full path to the directory in which the data resides.
This is the folder with a sub-folder /daily,
in which the HDF5 files are stored.
Returns: List of observation day [yyyymmdd].</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_od(obsid: str, data_dir: str = None):
    &#34;&#34;&#34; Return list of OD(s) for the given obsid.

    The given obsid can be specified in either of these two formats: TEST_LAB or TEST_LAB_SETUP. In this method, we
    determine during which OD(s) the given obsid was executed.  To do this, the first and last timestamp from the DPU HK
    file (this file will always be present if data has been acquired) are extracted.  This file resides in the folder of
    the given obsid in the /obs directory, with the name (i.e. obsid) in the format TEST_SITE_SETUP or TEST_SITE
    (depending on how old the observation is).  The obsid that is used in the filename follows the same pattern, so the
    given obsid must be converted to that format.

    Args:
         - obsid: Observation identifier [TEST_LAB or TEST_LAB_SETUP].
         - data_dir: Full path to the directory in which the data resides.  This is the folder with a sub-folder /daily,
                    in which the HDF5 files are stored.

    Returns: List of observation day [yyyymmdd].
    &#34;&#34;&#34;

    data_dir = data_dir or os.environ[&#34;PLATO_DATA_STORAGE_LOCATION&#34;]
    obsid = obsid_from_storage(obsid, data_dir=data_dir)     # Convert the obsid to the correct format
    obs_dir = f&#34;{data_dir}/obs/{obsid}&#34;

    try:
        filename = str(find_file(f&#34;{obsid}_DPU_*.csv&#34;, root=obs_dir))

        od_start = datetime.strptime(filename.split(&#34;_&#34;)[-2], &#34;%Y%m%d&#34;)         # First OD (from filename)
        od_end = datetime.strptime(read_last_line(filename)[:10], &#34;%Y-%m-%d&#34;)   # Last OD (from last line)

        od = od_start
        delta = timedelta(days=1)
        od_list = []

        while od &lt;= od_end:

            od_list.append(od.strftime(&#34;%Y%m%d&#34;))

            od += delta

        return od_list
    except IndexError:
        raise Abort(f&#34;DPU was not running during obsid {obsid}: no data could be acquired&#34;)</code></pre>
</details>
</dd>
<dt id="egse.dpu.fitsgen.get_setup_for_obsid"><code class="name flex">
<span>def <span class="ident">get_setup_for_obsid</span></span>(<span>obsid: str, data_dir: str = None) ‑> <a title="egse.setup.Setup" href="../setup.html#egse.setup.Setup">Setup</a></span>
</code></dt>
<dd>
<div class="desc"><p>Return the setup for the given obsid.</p>
<p>The given obsid can be specified in either of these two formats: TEST_LAB or TEST_LAB_SETUP. In this method, we
determine from the CM HK file (this file will always be present) for the given observation which is the setup that
was used during the given obsid.
This file resides in the folder of the given obsid in the /obs directory, with the
name (i.e. obsid) in the format TEST_SITE_SETUP or TEST_SITE (depending on how old the observation is).
The obsid
that is used in the filename follows the same pattern, so the given obsid must be converted to that format.</p>
<h2 id="args">Args</h2>
<ul>
<li>obsid: Observation identifier [TEST_LAB or TEST_LAB_SETUP].</li>
<li>data_dir: Full path to the directory in which the data resides.
This is the folder with a sub-folder /daily,
in which the HDF5 files are stored.
Returns: Setup for the given obsid.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_setup_for_obsid(obsid: str, data_dir: str = None) -&gt; Setup:
    &#34;&#34;&#34; Return the setup for the given obsid.

    The given obsid can be specified in either of these two formats: TEST_LAB or TEST_LAB_SETUP. In this method, we
    determine from the CM HK file (this file will always be present) for the given observation which is the setup that
    was used during the given obsid.  This file resides in the folder of the given obsid in the /obs directory, with the
    name (i.e. obsid) in the format TEST_SITE_SETUP or TEST_SITE (depending on how old the observation is).  The obsid
    that is used in the filename follows the same pattern, so the given obsid must be converted to that format.

    Args:
        - obsid: Observation identifier [TEST_LAB or TEST_LAB_SETUP].
        - data_dir: Full path to the directory in which the data resides.  This is the folder with a sub-folder /daily,
                    in which the HDF5 files are stored.

    Returns: Setup for the given obsid.
    &#34;&#34;&#34;

    data_dir = data_dir or os.environ[&#34;PLATO_DATA_STORAGE_LOCATION&#34;]
    obsid = obsid_from_storage(obsid, data_dir=data_dir)     # Convert the obsid to the correct format
    obs_dir = f&#34;{data_dir}/obs/{obsid}&#34;

    try:
        cm_hk = str(find_file(f&#34;{obsid}_CM_*.csv&#34;, root=obs_dir))
        last_line = read_last_line(cm_hk).split(&#34;,&#34;)
        site_id = last_line[1]
        setup_id = int(last_line[2])

        return load_setup(setup_id=setup_id, site_id=site_id, from_disk=True)
    except IndexError:
        raise Abort(f&#34;DPU was not running during obsid {obsid}: no data could be acquired&#34;)</code></pre>
</details>
</dd>
<dt id="egse.dpu.fitsgen.in_data_acquisition_mode"><code class="name flex">
<span>def <span class="ident">in_data_acquisition_mode</span></span>(<span>n_fee_state: Mapping[~KT, +VT_co])</span>
</code></dt>
<dd>
<div class="desc"><p>Check whether the N-FEE is in data acquisition mode.</p>
<h2 id="args">Args</h2>
<ul>
<li>n_fee_state: N-FEE state parameters or register map.
Returns: True if the N-FEE is in imaging mode (full-image (pattern) mode, windowing (pattern) mode, or
parallel/serial trap pumping mode (1/2)) and the digitised data is transferred to the N-DPU.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def in_data_acquisition_mode(n_fee_state: Mapping):
    &#34;&#34;&#34; Check whether the N-FEE is in data acquisition mode.

    Args:
           - n_fee_state: N-FEE state parameters or register map.

    Returns: True if the N-FEE is in imaging mode (full-image (pattern) mode, windowing (pattern) mode, or
             parallel/serial trap pumping mode (1/2)) and the digitised data is transferred to the N-DPU.
    &#34;&#34;&#34;

    ccd_mode_config = n_fee_state[&#34;ccd_mode_config&#34;]
    digitise_en = n_fee_state[&#34;digitise_en&#34;]

    return ccd_mode_config in [n_fee_mode.FULL_IMAGE_MODE, n_fee_mode.FULL_IMAGE_PATTERN_MODE,
                               n_fee_mode.PARALLEL_TRAP_PUMPING_1_MODE, n_fee_mode.PARALLEL_TRAP_PUMPING_2_MODE,
                               n_fee_mode.SERIAL_TRAP_PUMPING_1_MODE, n_fee_mode.SERIAL_TRAP_PUMPING_2_MODE,
                               n_fee_mode.WINDOWING_PATTERN_MODE, n_fee_mode.WINDOWING_MODE] and digitise_en</code></pre>
</details>
</dd>
<dt id="egse.dpu.fitsgen.is_corrupt"><code class="name flex">
<span>def <span class="ident">is_corrupt</span></span>(<span>hdf5_file: h5py._hl.files.File)</span>
</code></dt>
<dd>
<div class="desc"><p>Check whether the given HDF5 file is corrupt.</p>
<h2 id="args">Args</h2>
<ul>
<li>hdf5_file: HDF5 file.
Returns: True if an error flag is set in one of the groups; False otherwise.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_corrupt(hdf5_file: File):
    &#34;&#34;&#34; Check whether the given HDF5 file is corrupt.

    Args:
        - hdf5_file: HDF5 file.

    Returns: True if an error flag is set in one of the groups; False otherwise.
    &#34;&#34;&#34;

    for count in range(4):

        if f&#34;/{count}/hk&#34; in hdf5_file:

            hk_packet = SpaceWirePacket.create_packet(hdf5_file[f&#34;/{count}/hk&#34;][...])
            error_flags = HousekeepingData(hk_packet.data)[&#39;error_flags&#39;]

            if error_flags:
                return True

    return False</code></pre>
</details>
</dd>
<dt id="egse.dpu.fitsgen.is_incomplete"><code class="name flex">
<span>def <span class="ident">is_incomplete</span></span>(<span>hdf5_file: h5py._hl.files.File)</span>
</code></dt>
<dd>
<div class="desc"><p>Check whether the given HDF5 file is incomplete.</p>
<p>The HDF5 files are created at the start of a cycle. The register map and (if applicable) the format version are
stored at this point.
If an observation starts "half way" a cycle, the register map will not be present.</p>
<h2 id="args">Args</h2>
<ul>
<li>hdf5_file: HDF5 file.
Returns: True if the given HDF5 file is incomplete (i.e. if the register map is not stored); False otherwise.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_incomplete(hdf5_file: File):
    &#34;&#34;&#34; Check whether the given HDF5 file is incomplete.

    The HDF5 files are created at the start of a cycle. The register map and (if applicable) the format version are
    stored at this point.  If an observation starts &#34;half way&#34; a cycle, the register map will not be present.

    Args:
        - hdf5_file: HDF5 file.

    Returns: True if the given HDF5 file is incomplete (i.e. if the register map is not stored); False otherwise.
    &#34;&#34;&#34;

    return &#34;register&#34; not in hdf5_file</code></pre>
</details>
</dd>
<dt id="egse.dpu.fitsgen.send_request"><code class="name flex">
<span>def <span class="ident">send_request</span></span>(<span>command_request: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Sends a request to the FOV HK Control Server and waits for a response.</p>
<h2 id="args">Args</h2>
<ul>
<li>command_request: Request.
Returns: Response to the request.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def send_request(command_request: str):
    &#34;&#34;&#34;Sends a request to the FOV HK Control Server and waits for a response.

    Args:
        - command_request: Request.

    Returns: Response to the request.
    &#34;&#34;&#34;

    ctx = zmq.Context().instance()
    endpoint = connect_address(CTRL_SETTINGS.PROTOCOL, CTRL_SETTINGS.HOSTNAME, CTRL_SETTINGS.COMMANDING_PORT)
    socket = ctx.socket(zmq.REQ)
    socket.connect(endpoint)

    socket.send(pickle.dumps(command_request))
    rlist, _, _ = zmq.select([socket], [], [], timeout=TIMEOUT_RECV)

    if socket in rlist:
        response = socket.recv()
        response = pickle.loads(response)
    else:
        response = {&#34;error&#34;: &#34;Receive from ZeroMQ socket timed out for FITS generation Control Server.&#34;}
    socket.close(linger=0)

    return response</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="egse.dpu.fitsgen.FITSGenerator"><code class="flex name class">
<span>class <span class="ident">FITSGenerator</span></span>
</code></dt>
<dd>
<div class="desc"><p>Generation of FITS files from HDF5 files with SpW packets.</p>
<p>In a separate thread, the DPU monitoring puts the name of new HDF5 files with SpW packets in the queue.
The
FITS generator accesses this queue (FIFO) and stores the information in a FITS file.</p>
<p>When there is a change in crucial parameters, the current FITS file (if any) will be closed and a new one will
be created as soon as data packet start coming in (when the N-FEE is in full-image or full-image pattern mode).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FITSGenerator:

    def __init__(self):
        &#34;&#34;&#34; Generation of FITS files from HDF5 files with SpW packets.

        In a separate thread, the DPU monitoring puts the name of new HDF5 files with SpW packets in the queue.  The
        FITS generator accesses this queue (FIFO) and stores the information in a FITS file.

        When there is a change in crucial parameters, the current FITS file (if any) will be closed and a new one will
        be created as soon as data packet start coming in (when the N-FEE is in full-image or full-image pattern mode).
        &#34;&#34;&#34;

        # Queue with the full path of the HDF5 files that still need to be processed.

        self.hdf5_filename_queue = Queue(f&#34;{os.environ[&#39;PLATO_DATA_STORAGE_LOCATION&#39;]}/{DPU_SETTINGS[&#39;HDF5_QUEUE&#39;]}&#34;)

        # Name of the FITS file currently being written
        # (None if the N-FEE is not in full-image mode or in full-image pattern mode)

        self.fits_images_filename = None
        self.fits_cube_filename = None

        # Name of the HDF5 file currently being processed

        self.hdf5_filename = None

        # The last obsid that was/is being processed

        self.last_processed_obsid = None

        # Keep track of what was the N-FEE mode and what were the crucial parameters at the previous long pulse
        # (When we have checked whether a change has been detected, these values will be overwritten with the new ones)

        self.ccd_mode_config = None
        self.v_start = None
        self.v_end = None
        self.h_end = None
        self.ccd_readout_order = None
        # self.sensor_sel = None
        self.rows_final_dump = None
        self.setup = GlobalState.setup

        self.config_slicing_num_cycles = 0   # Configured slicing parameter
        self.processed_num_cycles = 0           # HDF5 files with image data processed for current FITS file

        self.zcontext = zmq.Context.instance()
        self.monitoring_socket = self.zcontext.socket(zmq.PUB)
        self.monitoring_socket.bind(bind_address(CTRL_SETTINGS.PROTOCOL, CTRL_SETTINGS.MONITORING_PORT,))

        # self._quit_event = multiprocessing.Event()

        self.keep_processing_queue = True

        # The DPU monitoring should populate the queue in a separate thread

        self.dpu_monitoring_thread = threading.Thread(target=self.fill_queue)
        self.dpu_monitoring_thread.daemon = True
        self.dpu_monitoring_thread.start()

        # Processing the content of the queue should be done in a separate thread

        self.process_queue_thread = threading.Thread(target=self.process_queue)
        self.process_queue_thread.daemon = True
        self.process_queue_thread.start()

    def fill_queue(self):
        &#34;&#34;&#34; The DPU monitoring fills the queue.

        Each time an HDF5 file with SpW packets is closed, the DPU monitoring puts the full path of this file on the
        queue.
        &#34;&#34;&#34;

        while self.keep_processing_queue:

            with DPUMonitoring() as dpu_monitoring:

                while self.keep_processing_queue:

                    try:
                        hdf5_filename = dpu_monitoring.wait_for_hdf5_filename()
                        self.hdf5_filename_queue.put(hdf5_filename)

                    # If the DPU monitoring times out, try to re-establish a connection
                    # (in the future, we want to replace this with something like dpu_monitoring_reconnect())

                    except TimeoutError:
                        LOGGER.warning(&#34;DPU monitoring timeout&#34;)
                        break

    def run(self, commander, poller):
        &#34;&#34;&#34; Process the content of the queue.

        When there is a filename in the queue, take it from the queue:

            - If there is a change in crucial parameters, close the current FITS file (if any).
            - If there is a change in crucial parameter and the N-FEE is in full-image mode or in full-image pattern
              mode, or the N-FEE goes to full-image mode or full-image pattern mode, a new FITS file will be created.
            - The content of the HDF5 files will be extracted and passed to the FITS persistence layer as SpW packets.
        &#34;&#34;&#34;

        last_time = time_in_ms()

        try:
            while True:

                if _check_commander_status(commander, poller):

                    self.keep_processing_queue = False
                    break

                if time_in_ms() - last_time &gt;= 1000:
                    last_time = time_in_ms()

                    monitoring_info = {&#34;hdf5&#34;: self.hdf5_filename,
                                       &#34;fits&#34;: self.fits_cube_filename or self.fits_images_filename,
                                       &#34;last obsid (being) processed&#34;: self.last_processed_obsid}
                    pickle_string = pickle.dumps(monitoring_info)
                    self.monitoring_socket.send(pickle_string)

        except KeyboardInterrupt:
            click.echo(&#34;KeyboardInterrupt caught!&#34;)
            self.keep_processing_queue = False

        # Keyboard interrupt or stop command

        LOGGER.info(&#34;Shutting down FITS generation&#34;)
        commander.close(linger=0)
        # self.dpu_monitoring_thread.join()
        # self.process_queue_thread.join()

        self.monitoring_socket.close()
        # self.zcontext.term()

    def process_queue(self):

        location = os.environ.get(&#39;PLATO_DATA_STORAGE_LOCATION&#39;)

        previous_obsid = None

        while self.keep_processing_queue:

            # There is an HDF5 file ready for processing

            if not self.hdf5_filename_queue.empty():

                try:

                    # Get the first item in the queue (FIFO) and open it

                    hdf5_filename = self.hdf5_filename_queue.get()[0]
                    self.hdf5_filename = hdf5_filename

                    LOGGER.info(f&#34;Processing file {hdf5_filename}&#34;)

                    with h5.get_file(hdf5_filename, mode=&#34;r&#34;, locking=False) as hdf5_file:

                        LOGGER.info(f&#34;Opened file {hdf5_filename}&#34;)

                        # Check whether there is data in the HDF5
                        # (if there is no data in the HDF5 file, nothing has to be done and you can go to the next file)

                        try:

                            # Slicing

                            try:
                                slicing_num_cycles = hdf5_file[&#34;dpu&#34;].attrs[&#34;slicing_num_cycles&#34;]
                                if slicing_num_cycles != self.config_slicing_num_cycles:
                                    LOGGER.debug(f&#34;Slicing parameter changed: {self.config_slicing_num_cycles} &#34;
                                                 f&#34;-&gt; {slicing_num_cycles}&#34;)
                                    self.close_fits()
                                    self.config_slicing_num_cycles = slicing_num_cycles
                            except KeyError:
                                self.config_slicing_num_cycles = 0
                                LOGGER.debug(&#34;No slicing&#34;)

                            # Obsid

                            try:
                                obsid = hdf5_file[&#34;obsid&#34;][()].decode()
                                obsid = ObservationIdentifier.create_from_string(obsid, order=LAB_SETUP_TEST)

                                self.last_processed_obsid = obsid_from_storage(obsid, data_dir=location)
                            except (KeyError, ValueError):
                                # Either the obsid is not included in the HDF5 file or it is None
                                obsid = None
                            except AttributeError:
                                LOGGER.error(f&#34;No data present for obsid {str(obsid)} in the obs folder&#34;)
                                self.keep_processing_queue = False
                                break

                            # Loop over all groups in the current HDF5 file and check whether the &#34;data&#34; group is
                            # present

                            has_data = False

                            for group in h5.groups(hdf5_file):

                                if &#34;data&#34; in group.keys():

                                    has_data = True

                                    n_fee_state = group[&#34;data&#34;].attrs

                                    # Check whether there is a change in crucial parameters or in the N-FEE mode

                                    if self.crucial_parameter_change(n_fee_state):

                                        self.close_fits()

                                    if in_data_acquisition_mode(n_fee_state):

                                        if self.fits_images_filename is None:

                                            # Start writing to a new FITS file

                                            self.fits_images_filename = construct_images_filename(hdf5_filename, obsid)

                                            ccd_readout_order = convert_ccd_order_value(self.ccd_readout_order)

                                            prep = {
                                                &#34;v_start&#34;: self.v_start,
                                                &#34;v_end&#34;: self.v_end,
                                                &#34;h_end&#34;: self.h_end,
                                                &#34;rows_final_dump&#34;: self.rows_final_dump,
                                                &#34;ccd_mode_config&#34;: self.ccd_mode_config,
                                                &#34;ccd_readout_order&#34;: ccd_readout_order,  # CCD numbering [1-4]
                                                &#34;expected_last_packet_flags&#34;: get_expected_last_packet_flags(n_fee_state),
                                                &#34;obsid&#34;: str(obsid),
                                                &#34;cycle_time&#34;: get_cycle_time(n_fee_state, obsid=obsid),
                                                &#34;cgse_version&#34;: get_cgse_version(obsid=obsid),
                                                &#34;setup&#34;: self.setup
                                            }

                                            persistence = FITS(self.fits_images_filename, prep)
                                            persistence.open()

                                        # See https://github.com/IvS-KULeuven/plato-common-egse/issues/901
                                        # timecode = group[&#34;timecode&#34;]
                                        # spw_packet = SpaceWirePacket.create_packet(h5.get_data(timecode))

                                        timestamp = group[&#34;timecode&#34;].attrs[&#34;timestamp&#34;]
                                        persistence.create({&#34;Timestamp&#34;: timestamp})

                                        data = group[&#34;data&#34;]
                                        sorted_datasets = natsort.natsorted(data.items(), key=lambda x: x[0])

                                        persistence.expected_last_packet_flags = get_expected_last_packet_flags(
                                            n_fee_state)

                                        for identifier, dataset in sorted_datasets:

                                            spw_packet = SpaceWirePacket.create_packet(h5.get_data(dataset))
                                            persistence.create({f&#34;SpW packet {identifier}&#34;: spw_packet})

                            if has_data:
                                self.processed_num_cycles += 1

                                if self.config_slicing_num_cycles != 0 and \
                                        self.processed_num_cycles == self.config_slicing_num_cycles:
                                    self.close_fits()

                            else:

                                self.close_fits()
                                self.clear_crucial_parameters()

                            # When the previous HDF5 file still pertained to an observation and the current one doesn&#39;t,
                            # it means that the observation has just finished and all FITS files have been generated. It
                            # is only at this point that the synoptics can be included in the FITS headers.

                            if previous_obsid is not None and obsid is None:
                                add_synoptics(previous_obsid)
                                previous_obsid = obsid

                        except KeyError:
                            LOGGER.debug(&#34;KeyError occurred when accessing data in all groups of the HDF5 file.&#34;)

                except IndexError:
                    LOGGER.debug(&#34;Queue contained an emtpy entry&#34;)
                except RuntimeError as exc:
                    LOGGER.debug(f&#34;Unable to open HDF5 file: {exc}&#34;)

    def clear_crucial_parameters(self):
        &#34;&#34;&#34; Clear the crucial parameters.&#34;&#34;&#34;

        self.v_start = None
        self.v_end = None
        self.h_end = None
        self.rows_final_dump = None
        self.ccd_readout_order = None
        self.ccd_mode_config = None

    def close_fits(self):

        if self.fits_images_filename is not None:

            self.fits_cube_filename = construct_cube_filename(self.fits_images_filename)
            convert_to_cubes(self.fits_images_filename)
            self.fits_cube_filename = None

            # Stop writing to the current FITS file

            self.fits_images_filename = None

            # Reset the number of HDF5 files with image data processed for current FITS file

            self.processed_num_cycles = 0

    def crucial_parameter_change(self, n_fee_state: AttributeManager):
        &#34;&#34;&#34; Check for a change in crucial parameters.

        Crucial parameters are:

            - ccd_mode_config: readout mode;
            - v_start (int) and v_end(int): index of the first and the last row being transmitted;
            - h_end (int): index of the last serial readout of the readout register;
            - ccd_readout_order: CCDs that will be read out;
            # - sensor_sel: which  side(s) of the CCD(s) that will be read out;

        Args:
            - n_fee_stage: N-FEE stae parameters.

        Returns: True if a change in crucial parameters has been detected; False otherwise.
        &#34;&#34;&#34;

        ccd_mode_config = n_fee_state[&#34;ccd_mode_config&#34;]
        v_start = n_fee_state[&#34;v_start&#34;]
        v_end = n_fee_state[&#34;v_end&#34;]
        h_end = n_fee_state[&#34;h_end&#34;]
        ccd_readout_order = n_fee_state[&#34;ccd_readout_order&#34;]
        rows_final_dump = n_fee_state[&#34;n_final_dump&#34;]

        crucial_parameter_change = False

        if v_start != self.v_start:

            LOGGER.info(f&#34;Change in v_start: {self.v_start} -&gt; {v_start}&#34;)

            self.v_start = v_start
            crucial_parameter_change = True

        if v_end != self.v_end:

            LOGGER.info(f&#34;Change in v_end: {self.v_end} -&gt; {v_end}&#34;)

            self.v_end = v_end
            crucial_parameter_change = True

        if h_end != self.h_end:

            LOGGER.info(f&#34;Change in h_end: {self.h_end} -&gt; {h_end}&#34;)

            self.h_end = h_end
            crucial_parameter_change = True

        if rows_final_dump != self.rows_final_dump:

            LOGGER.info(f&#34;Change in rows_final_dump: {self.rows_final_dump} -&gt; {rows_final_dump}&#34;)

            self.rows_final_dump = rows_final_dump
            crucial_parameter_change = True

        if ccd_readout_order != self.ccd_readout_order:

            LOGGER.info(f&#34;Change in ccd_readout_order: {self.ccd_readout_order} -&gt; {ccd_readout_order}&#34;)

            self.ccd_readout_order = ccd_readout_order
            crucial_parameter_change = True

        if ccd_mode_config != self.ccd_mode_config:

            LOGGER.info(f&#34;Change in ccd_mode_config: {self.ccd_mode_config} -&gt; {ccd_mode_config}&#34;)

            self.ccd_mode_config = ccd_mode_config
            crucial_parameter_change = True

        return crucial_parameter_change</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="egse.dpu.fitsgen.FITSGenerator.clear_crucial_parameters"><code class="name flex">
<span>def <span class="ident">clear_crucial_parameters</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Clear the crucial parameters.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clear_crucial_parameters(self):
    &#34;&#34;&#34; Clear the crucial parameters.&#34;&#34;&#34;

    self.v_start = None
    self.v_end = None
    self.h_end = None
    self.rows_final_dump = None
    self.ccd_readout_order = None
    self.ccd_mode_config = None</code></pre>
</details>
</dd>
<dt id="egse.dpu.fitsgen.FITSGenerator.close_fits"><code class="name flex">
<span>def <span class="ident">close_fits</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def close_fits(self):

    if self.fits_images_filename is not None:

        self.fits_cube_filename = construct_cube_filename(self.fits_images_filename)
        convert_to_cubes(self.fits_images_filename)
        self.fits_cube_filename = None

        # Stop writing to the current FITS file

        self.fits_images_filename = None

        # Reset the number of HDF5 files with image data processed for current FITS file

        self.processed_num_cycles = 0</code></pre>
</details>
</dd>
<dt id="egse.dpu.fitsgen.FITSGenerator.crucial_parameter_change"><code class="name flex">
<span>def <span class="ident">crucial_parameter_change</span></span>(<span>self, n_fee_state: h5py._hl.attrs.AttributeManager)</span>
</code></dt>
<dd>
<div class="desc"><p>Check for a change in crucial parameters.</p>
<p>Crucial parameters are:</p>
<pre><code>- ccd_mode_config: readout mode;
- v_start (int) and v_end(int): index of the first and the last row being transmitted;
- h_end (int): index of the last serial readout of the readout register;
- ccd_readout_order: CCDs that will be read out;
# - sensor_sel: which  side(s) of the CCD(s) that will be read out;
</code></pre>
<h2 id="args">Args</h2>
<ul>
<li>n_fee_stage: N-FEE stae parameters.
Returns: True if a change in crucial parameters has been detected; False otherwise.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def crucial_parameter_change(self, n_fee_state: AttributeManager):
    &#34;&#34;&#34; Check for a change in crucial parameters.

    Crucial parameters are:

        - ccd_mode_config: readout mode;
        - v_start (int) and v_end(int): index of the first and the last row being transmitted;
        - h_end (int): index of the last serial readout of the readout register;
        - ccd_readout_order: CCDs that will be read out;
        # - sensor_sel: which  side(s) of the CCD(s) that will be read out;

    Args:
        - n_fee_stage: N-FEE stae parameters.

    Returns: True if a change in crucial parameters has been detected; False otherwise.
    &#34;&#34;&#34;

    ccd_mode_config = n_fee_state[&#34;ccd_mode_config&#34;]
    v_start = n_fee_state[&#34;v_start&#34;]
    v_end = n_fee_state[&#34;v_end&#34;]
    h_end = n_fee_state[&#34;h_end&#34;]
    ccd_readout_order = n_fee_state[&#34;ccd_readout_order&#34;]
    rows_final_dump = n_fee_state[&#34;n_final_dump&#34;]

    crucial_parameter_change = False

    if v_start != self.v_start:

        LOGGER.info(f&#34;Change in v_start: {self.v_start} -&gt; {v_start}&#34;)

        self.v_start = v_start
        crucial_parameter_change = True

    if v_end != self.v_end:

        LOGGER.info(f&#34;Change in v_end: {self.v_end} -&gt; {v_end}&#34;)

        self.v_end = v_end
        crucial_parameter_change = True

    if h_end != self.h_end:

        LOGGER.info(f&#34;Change in h_end: {self.h_end} -&gt; {h_end}&#34;)

        self.h_end = h_end
        crucial_parameter_change = True

    if rows_final_dump != self.rows_final_dump:

        LOGGER.info(f&#34;Change in rows_final_dump: {self.rows_final_dump} -&gt; {rows_final_dump}&#34;)

        self.rows_final_dump = rows_final_dump
        crucial_parameter_change = True

    if ccd_readout_order != self.ccd_readout_order:

        LOGGER.info(f&#34;Change in ccd_readout_order: {self.ccd_readout_order} -&gt; {ccd_readout_order}&#34;)

        self.ccd_readout_order = ccd_readout_order
        crucial_parameter_change = True

    if ccd_mode_config != self.ccd_mode_config:

        LOGGER.info(f&#34;Change in ccd_mode_config: {self.ccd_mode_config} -&gt; {ccd_mode_config}&#34;)

        self.ccd_mode_config = ccd_mode_config
        crucial_parameter_change = True

    return crucial_parameter_change</code></pre>
</details>
</dd>
<dt id="egse.dpu.fitsgen.FITSGenerator.fill_queue"><code class="name flex">
<span>def <span class="ident">fill_queue</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>The DPU monitoring fills the queue.</p>
<p>Each time an HDF5 file with SpW packets is closed, the DPU monitoring puts the full path of this file on the
queue.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fill_queue(self):
    &#34;&#34;&#34; The DPU monitoring fills the queue.

    Each time an HDF5 file with SpW packets is closed, the DPU monitoring puts the full path of this file on the
    queue.
    &#34;&#34;&#34;

    while self.keep_processing_queue:

        with DPUMonitoring() as dpu_monitoring:

            while self.keep_processing_queue:

                try:
                    hdf5_filename = dpu_monitoring.wait_for_hdf5_filename()
                    self.hdf5_filename_queue.put(hdf5_filename)

                # If the DPU monitoring times out, try to re-establish a connection
                # (in the future, we want to replace this with something like dpu_monitoring_reconnect())

                except TimeoutError:
                    LOGGER.warning(&#34;DPU monitoring timeout&#34;)
                    break</code></pre>
</details>
</dd>
<dt id="egse.dpu.fitsgen.FITSGenerator.process_queue"><code class="name flex">
<span>def <span class="ident">process_queue</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_queue(self):

    location = os.environ.get(&#39;PLATO_DATA_STORAGE_LOCATION&#39;)

    previous_obsid = None

    while self.keep_processing_queue:

        # There is an HDF5 file ready for processing

        if not self.hdf5_filename_queue.empty():

            try:

                # Get the first item in the queue (FIFO) and open it

                hdf5_filename = self.hdf5_filename_queue.get()[0]
                self.hdf5_filename = hdf5_filename

                LOGGER.info(f&#34;Processing file {hdf5_filename}&#34;)

                with h5.get_file(hdf5_filename, mode=&#34;r&#34;, locking=False) as hdf5_file:

                    LOGGER.info(f&#34;Opened file {hdf5_filename}&#34;)

                    # Check whether there is data in the HDF5
                    # (if there is no data in the HDF5 file, nothing has to be done and you can go to the next file)

                    try:

                        # Slicing

                        try:
                            slicing_num_cycles = hdf5_file[&#34;dpu&#34;].attrs[&#34;slicing_num_cycles&#34;]
                            if slicing_num_cycles != self.config_slicing_num_cycles:
                                LOGGER.debug(f&#34;Slicing parameter changed: {self.config_slicing_num_cycles} &#34;
                                             f&#34;-&gt; {slicing_num_cycles}&#34;)
                                self.close_fits()
                                self.config_slicing_num_cycles = slicing_num_cycles
                        except KeyError:
                            self.config_slicing_num_cycles = 0
                            LOGGER.debug(&#34;No slicing&#34;)

                        # Obsid

                        try:
                            obsid = hdf5_file[&#34;obsid&#34;][()].decode()
                            obsid = ObservationIdentifier.create_from_string(obsid, order=LAB_SETUP_TEST)

                            self.last_processed_obsid = obsid_from_storage(obsid, data_dir=location)
                        except (KeyError, ValueError):
                            # Either the obsid is not included in the HDF5 file or it is None
                            obsid = None
                        except AttributeError:
                            LOGGER.error(f&#34;No data present for obsid {str(obsid)} in the obs folder&#34;)
                            self.keep_processing_queue = False
                            break

                        # Loop over all groups in the current HDF5 file and check whether the &#34;data&#34; group is
                        # present

                        has_data = False

                        for group in h5.groups(hdf5_file):

                            if &#34;data&#34; in group.keys():

                                has_data = True

                                n_fee_state = group[&#34;data&#34;].attrs

                                # Check whether there is a change in crucial parameters or in the N-FEE mode

                                if self.crucial_parameter_change(n_fee_state):

                                    self.close_fits()

                                if in_data_acquisition_mode(n_fee_state):

                                    if self.fits_images_filename is None:

                                        # Start writing to a new FITS file

                                        self.fits_images_filename = construct_images_filename(hdf5_filename, obsid)

                                        ccd_readout_order = convert_ccd_order_value(self.ccd_readout_order)

                                        prep = {
                                            &#34;v_start&#34;: self.v_start,
                                            &#34;v_end&#34;: self.v_end,
                                            &#34;h_end&#34;: self.h_end,
                                            &#34;rows_final_dump&#34;: self.rows_final_dump,
                                            &#34;ccd_mode_config&#34;: self.ccd_mode_config,
                                            &#34;ccd_readout_order&#34;: ccd_readout_order,  # CCD numbering [1-4]
                                            &#34;expected_last_packet_flags&#34;: get_expected_last_packet_flags(n_fee_state),
                                            &#34;obsid&#34;: str(obsid),
                                            &#34;cycle_time&#34;: get_cycle_time(n_fee_state, obsid=obsid),
                                            &#34;cgse_version&#34;: get_cgse_version(obsid=obsid),
                                            &#34;setup&#34;: self.setup
                                        }

                                        persistence = FITS(self.fits_images_filename, prep)
                                        persistence.open()

                                    # See https://github.com/IvS-KULeuven/plato-common-egse/issues/901
                                    # timecode = group[&#34;timecode&#34;]
                                    # spw_packet = SpaceWirePacket.create_packet(h5.get_data(timecode))

                                    timestamp = group[&#34;timecode&#34;].attrs[&#34;timestamp&#34;]
                                    persistence.create({&#34;Timestamp&#34;: timestamp})

                                    data = group[&#34;data&#34;]
                                    sorted_datasets = natsort.natsorted(data.items(), key=lambda x: x[0])

                                    persistence.expected_last_packet_flags = get_expected_last_packet_flags(
                                        n_fee_state)

                                    for identifier, dataset in sorted_datasets:

                                        spw_packet = SpaceWirePacket.create_packet(h5.get_data(dataset))
                                        persistence.create({f&#34;SpW packet {identifier}&#34;: spw_packet})

                        if has_data:
                            self.processed_num_cycles += 1

                            if self.config_slicing_num_cycles != 0 and \
                                    self.processed_num_cycles == self.config_slicing_num_cycles:
                                self.close_fits()

                        else:

                            self.close_fits()
                            self.clear_crucial_parameters()

                        # When the previous HDF5 file still pertained to an observation and the current one doesn&#39;t,
                        # it means that the observation has just finished and all FITS files have been generated. It
                        # is only at this point that the synoptics can be included in the FITS headers.

                        if previous_obsid is not None and obsid is None:
                            add_synoptics(previous_obsid)
                            previous_obsid = obsid

                    except KeyError:
                        LOGGER.debug(&#34;KeyError occurred when accessing data in all groups of the HDF5 file.&#34;)

            except IndexError:
                LOGGER.debug(&#34;Queue contained an emtpy entry&#34;)
            except RuntimeError as exc:
                LOGGER.debug(f&#34;Unable to open HDF5 file: {exc}&#34;)</code></pre>
</details>
</dd>
<dt id="egse.dpu.fitsgen.FITSGenerator.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self, commander, poller)</span>
</code></dt>
<dd>
<div class="desc"><p>Process the content of the queue.</p>
<p>When there is a filename in the queue, take it from the queue:</p>
<pre><code>- If there is a change in crucial parameters, close the current FITS file (if any).
- If there is a change in crucial parameter and the N-FEE is in full-image mode or in full-image pattern
  mode, or the N-FEE goes to full-image mode or full-image pattern mode, a new FITS file will be created.
- The content of the HDF5 files will be extracted and passed to the FITS persistence layer as SpW packets.
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(self, commander, poller):
    &#34;&#34;&#34; Process the content of the queue.

    When there is a filename in the queue, take it from the queue:

        - If there is a change in crucial parameters, close the current FITS file (if any).
        - If there is a change in crucial parameter and the N-FEE is in full-image mode or in full-image pattern
          mode, or the N-FEE goes to full-image mode or full-image pattern mode, a new FITS file will be created.
        - The content of the HDF5 files will be extracted and passed to the FITS persistence layer as SpW packets.
    &#34;&#34;&#34;

    last_time = time_in_ms()

    try:
        while True:

            if _check_commander_status(commander, poller):

                self.keep_processing_queue = False
                break

            if time_in_ms() - last_time &gt;= 1000:
                last_time = time_in_ms()

                monitoring_info = {&#34;hdf5&#34;: self.hdf5_filename,
                                   &#34;fits&#34;: self.fits_cube_filename or self.fits_images_filename,
                                   &#34;last obsid (being) processed&#34;: self.last_processed_obsid}
                pickle_string = pickle.dumps(monitoring_info)
                self.monitoring_socket.send(pickle_string)

    except KeyboardInterrupt:
        click.echo(&#34;KeyboardInterrupt caught!&#34;)
        self.keep_processing_queue = False

    # Keyboard interrupt or stop command

    LOGGER.info(&#34;Shutting down FITS generation&#34;)
    commander.close(linger=0)
    # self.dpu_monitoring_thread.join()
    # self.process_queue_thread.join()

    self.monitoring_socket.close()
    # self.zcontext.term()</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsFwdFill"><code class="flex name class">
<span>class <span class="ident">SynopticsFwdFill</span></span>
<span>(</span><span>value, names=None, *, module=None, qualname=None, type=None, start=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Enumeration of the synoptics to forward fill.</p>
<p>This is only applicable for the commanded source position.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SynopticsFwdFill(tuple, Enum):
    &#34;&#34;&#34; Enumeration of the synoptics to forward fill.

    This is only applicable for the commanded source position.
    &#34;&#34;&#34;

    # Source position (commanded)

    THETA_CMD = (&#34;GSYN_CMD_THETA&#34;, &#34;Commanded source position theta [deg]&#34;)
    PHI_CMD = (&#34;GSYN_CMD_PHI&#34;, &#34;Commanded source position phi [deg]&#34;)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.tuple</li>
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="egse.dpu.fitsgen.SynopticsFwdFill.PHI_CMD"><code class="name">var <span class="ident">PHI_CMD</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsFwdFill.THETA_CMD"><code class="name">var <span class="ident">THETA_CMD</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsInterp1d"><code class="flex name class">
<span>class <span class="ident">SynopticsInterp1d</span></span>
<span>(</span><span>value, names=None, *, module=None, qualname=None, type=None, start=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Enumeration of the synoptics to linearly interpolate.</p>
<p>This is only applicable for:
- calibrated TCS temperatures;
- calibrated N-FEE temperatures (TOU + CCDs + and board sensors);
- selection of TH DAQ(s) temperatures;
- OGSE attenuation (relative intensity + FWC fraction for the OGSE);
- actual source position.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SynopticsInterp1d(tuple, Enum):
    &#34;&#34;&#34; Enumeration of the synoptics to linearly interpolate.

    This is only applicable for:
        - calibrated TCS temperatures;
        - calibrated N-FEE temperatures (TOU + CCDs + and board sensors);
        - selection of TH DAQ(s) temperatures;
        - OGSE attenuation (relative intensity + FWC fraction for the OGSE);
        - actual source position.
    &#34;&#34;&#34;

    # TCS temperatures

    T_TRP1 = (&#34;GSYN_TRP1&#34;, &#34;Mean T for TOU TRP1 (TCS) [deg C]&#34;)
    T_TRP22 = (&#34;GSYN_TRP22&#34;, &#34;Mean T for FEE TRP22 (TCS) [deg C]&#34;)

    # TOU TRP PT1000 sensors (N-FEE)

    T_TRP5 = (&#34;GSYN_TRP5&#34;, &#34;Mean T for TRP5 (TOU baffle ring) [deg C]&#34;)
    T_TRP6 = (&#34;GSYN_TRP6&#34;, &#34;Mean T for TRP6 (FPA I/F) [deg C]&#34;)
    T_TRP8 = (&#34;GSYN_TRP8&#34;, &#34;Mean T for TRP8 (L3) [deg C]&#34;)
    T_TRP21 = (&#34;GSYN_TRP21&#34;, &#34;Mean T for TRP21 (TOU bipod +X bottom) [deg C]&#34;)
    T_TRP31 = (&#34;GSYN_TRP31&#34;, &#34;Mean T for TRP31 (TOU bipod -Y bottom) [deg C]&#34;)
    T_TRP41 = (&#34;GSYN_TRP41&#34;, &#34;Mean T for TRP41 (TOU bipod +Y bottom) [deg C]&#34;)

    # CCD PT100 sensors (N-FEE)

    T_CCD1 = (&#34;GSYN_CCD1&#34;, &#34;Mean T for CCD1 [deg C]&#34;)
    T_CCD2 = (&#34;GSYN_CCD2&#34;, &#34;Mean T for CCD2 [deg C]&#34;)
    T_CCD3 = (&#34;GSYN_CCD3&#34;, &#34;Mean T for CCD3 [deg C]&#34;)
    T_CCD4 = (&#34;GSYN_CCD4&#34;, &#34;Mean T for CCD4 [deg C]&#34;)

    # Board sensors: type PT1000 (N-FEE)

    T_PCB1 = (&#34;GSYN_NFEE_T_PCB1&#34;, &#34;Mean T for board sensor PCB1 [deg C]&#34;)
    T_PCB2 = (&#34;GSYN_NFEE_T_PCB2&#34;, &#34;Mean T for board sensor PCB2 [deg C]&#34;)
    T_PCB3 = (&#34;GSYN_NFEE_T_PCB3&#34;, &#34;Mean T for board sensor PCB3 [deg C]&#34;)
    T_PCB4 = (&#34;GSYN_NFEE_T_PCB4&#34;, &#34;Mean T for board sensor PCB4 [deg C]&#34;)

    # Board sensors: type ISL71590

    T_ADC = (&#34;GSYN_NFEE_T_ADC&#34;, &#34;Mean ADC board T [deg C]&#34;)
    T_CDS = (&#34;GSYN_NFEE_T_CDS&#34;, &#34;Mean CDS board T [deg C]&#34;)
    T_ANALOG = (&#34;GSYN_NFEE_T_ANALOG&#34;, &#34;Mean analog board T [deg C]&#34;)
    T_SKYSHROUD = (&#34;GSYN_SKYSHROUD&#34;, &#34;Mean front shroud T [deg C]&#34;)
    T_TEB_TOU = (&#34;GSYN_TEB_TOU&#34;, &#34;Mean TEB TOU T [deg C]&#34;)
    T_TEB_FEE = (&#34;GSYN_TEB_FEE&#34;, &#34;Mean TEB FEE T [deg C]&#34;)

    # Temperatures from the TH DAQ

    T_TRP2 = (&#34;GSYN_TRP2&#34;, &#34;Mean T for TRP2 (MaRi bipod +X I/F) [deg C]&#34;)
    T_TRP3 = (&#34;GSYN_TRP3&#34;, &#34;Mean T for TRP3 (MaRi bipod -Y I/F) [deg C]&#34;)
    T_TRP4 = (&#34;GSYN_TRP4&#34;, &#34;Mean T for TRP4 (MaRi bipod +Y I/F) [deg C]&#34;)

    T_TRP7 = (&#34;GSYN_TRP7&#34;, &#34;Mean T for TRP7 (thermal strap) [deg C]&#34;)
    T_TRP10 = (&#34;GSYN_TRP10&#34;, &#34;Mean T for TRP10 (FPA) [deg C]&#34;)

    # OGSE attenuation

    OGATT = (&#34;GSYN_OGSE_REL_INTENSITY&#34;, &#34;Relative OGSE intensity&#34;)
    OGFWC = (&#34;GSYN_OGSE_FWC_FRACTION&#34;, &#34;OGSE FWC fraction&#34;)

    # Source position (actual)

    THETA = (&#34;GSYN_ACT_THETA&#34;, &#34;Actual source position theta [deg]&#34;)
    PHI = (&#34;GSYN_ACT_PHI&#34;, &#34;Actual source position phi [deg]&#34;)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.tuple</li>
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="egse.dpu.fitsgen.SynopticsInterp1d.OGATT"><code class="name">var <span class="ident">OGATT</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsInterp1d.OGFWC"><code class="name">var <span class="ident">OGFWC</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsInterp1d.PHI"><code class="name">var <span class="ident">PHI</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsInterp1d.THETA"><code class="name">var <span class="ident">THETA</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsInterp1d.T_ADC"><code class="name">var <span class="ident">T_ADC</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsInterp1d.T_ANALOG"><code class="name">var <span class="ident">T_ANALOG</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsInterp1d.T_CCD1"><code class="name">var <span class="ident">T_CCD1</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsInterp1d.T_CCD2"><code class="name">var <span class="ident">T_CCD2</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsInterp1d.T_CCD3"><code class="name">var <span class="ident">T_CCD3</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsInterp1d.T_CCD4"><code class="name">var <span class="ident">T_CCD4</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsInterp1d.T_CDS"><code class="name">var <span class="ident">T_CDS</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsInterp1d.T_PCB1"><code class="name">var <span class="ident">T_PCB1</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsInterp1d.T_PCB2"><code class="name">var <span class="ident">T_PCB2</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsInterp1d.T_PCB3"><code class="name">var <span class="ident">T_PCB3</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsInterp1d.T_PCB4"><code class="name">var <span class="ident">T_PCB4</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsInterp1d.T_SKYSHROUD"><code class="name">var <span class="ident">T_SKYSHROUD</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsInterp1d.T_TEB_FEE"><code class="name">var <span class="ident">T_TEB_FEE</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsInterp1d.T_TEB_TOU"><code class="name">var <span class="ident">T_TEB_TOU</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsInterp1d.T_TRP1"><code class="name">var <span class="ident">T_TRP1</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsInterp1d.T_TRP10"><code class="name">var <span class="ident">T_TRP10</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsInterp1d.T_TRP2"><code class="name">var <span class="ident">T_TRP2</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsInterp1d.T_TRP21"><code class="name">var <span class="ident">T_TRP21</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsInterp1d.T_TRP22"><code class="name">var <span class="ident">T_TRP22</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsInterp1d.T_TRP3"><code class="name">var <span class="ident">T_TRP3</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsInterp1d.T_TRP31"><code class="name">var <span class="ident">T_TRP31</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsInterp1d.T_TRP4"><code class="name">var <span class="ident">T_TRP4</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsInterp1d.T_TRP41"><code class="name">var <span class="ident">T_TRP41</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsInterp1d.T_TRP5"><code class="name">var <span class="ident">T_TRP5</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsInterp1d.T_TRP6"><code class="name">var <span class="ident">T_TRP6</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsInterp1d.T_TRP7"><code class="name">var <span class="ident">T_TRP7</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsInterp1d.T_TRP8"><code class="name">var <span class="ident">T_TRP8</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="egse.dpu.fitsgen.SynopticsLeaveGaps"><code class="flex name class">
<span>class <span class="ident">SynopticsLeaveGaps</span></span>
<span>(</span><span>value, names=None, *, module=None, qualname=None, type=None, start=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Enumeration of the synoptics not to fill the gaps for.</p>
<p>This is only applicable for the status of the shutter (open/closed).
Note that there is no shutter in CSL, so we
indicate that the shutter is always open there.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SynopticsLeaveGaps(tuple, Enum):
    &#34;&#34;&#34; Enumeration of the synoptics not to fill the gaps for.

    This is only applicable for the status of the shutter (open/closed).  Note that there is no shutter in CSL, so we
    indicate that the shutter is always open there.
    &#34;&#34;&#34;

    OGSHTTR = (&#34;GSYN_OGSE_SHUTTER_OPEN&#34;, &#34;Is the shutter open?&#34;)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.tuple</li>
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="egse.dpu.fitsgen.SynopticsLeaveGaps.OGSHTTR"><code class="name">var <span class="ident">OGSHTTR</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="egse.dpu" href="index.html">egse.dpu</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="egse.dpu.fitsgen.add_synoptics" href="#egse.dpu.fitsgen.add_synoptics">add_synoptics</a></code></li>
<li><code><a title="egse.dpu.fitsgen.any_crucial_parameters_changed" href="#egse.dpu.fitsgen.any_crucial_parameters_changed">any_crucial_parameters_changed</a></code></li>
<li><code><a title="egse.dpu.fitsgen.clear_crucial_parameters" href="#egse.dpu.fitsgen.clear_crucial_parameters">clear_crucial_parameters</a></code></li>
<li><code><a title="egse.dpu.fitsgen.construct_cube_filename" href="#egse.dpu.fitsgen.construct_cube_filename">construct_cube_filename</a></code></li>
<li><code><a title="egse.dpu.fitsgen.construct_images_filename" href="#egse.dpu.fitsgen.construct_images_filename">construct_images_filename</a></code></li>
<li><code><a title="egse.dpu.fitsgen.convert_to_cubes" href="#egse.dpu.fitsgen.convert_to_cubes">convert_to_cubes</a></code></li>
<li><code><a title="egse.dpu.fitsgen.create_fits_from_hdf5" href="#egse.dpu.fitsgen.create_fits_from_hdf5">create_fits_from_hdf5</a></code></li>
<li><code><a title="egse.dpu.fitsgen.create_fits_from_hdf5_nfee_state" href="#egse.dpu.fitsgen.create_fits_from_hdf5_nfee_state">create_fits_from_hdf5_nfee_state</a></code></li>
<li><code><a title="egse.dpu.fitsgen.create_fits_from_hdf5_register_map" href="#egse.dpu.fitsgen.create_fits_from_hdf5_register_map">create_fits_from_hdf5_register_map</a></code></li>
<li><code><a title="egse.dpu.fitsgen.get_cgse_version" href="#egse.dpu.fitsgen.get_cgse_version">get_cgse_version</a></code></li>
<li><code><a title="egse.dpu.fitsgen.get_cycle_time" href="#egse.dpu.fitsgen.get_cycle_time">get_cycle_time</a></code></li>
<li><code><a title="egse.dpu.fitsgen.get_fits_counter" href="#egse.dpu.fitsgen.get_fits_counter">get_fits_counter</a></code></li>
<li><code><a title="egse.dpu.fitsgen.get_fits_synoptics" href="#egse.dpu.fitsgen.get_fits_synoptics">get_fits_synoptics</a></code></li>
<li><code><a title="egse.dpu.fitsgen.get_hdf5_filenames_for_obsid" href="#egse.dpu.fitsgen.get_hdf5_filenames_for_obsid">get_hdf5_filenames_for_obsid</a></code></li>
<li><code><a title="egse.dpu.fitsgen.get_obsid" href="#egse.dpu.fitsgen.get_obsid">get_obsid</a></code></li>
<li><code><a title="egse.dpu.fitsgen.get_od" href="#egse.dpu.fitsgen.get_od">get_od</a></code></li>
<li><code><a title="egse.dpu.fitsgen.get_setup_for_obsid" href="#egse.dpu.fitsgen.get_setup_for_obsid">get_setup_for_obsid</a></code></li>
<li><code><a title="egse.dpu.fitsgen.in_data_acquisition_mode" href="#egse.dpu.fitsgen.in_data_acquisition_mode">in_data_acquisition_mode</a></code></li>
<li><code><a title="egse.dpu.fitsgen.is_corrupt" href="#egse.dpu.fitsgen.is_corrupt">is_corrupt</a></code></li>
<li><code><a title="egse.dpu.fitsgen.is_incomplete" href="#egse.dpu.fitsgen.is_incomplete">is_incomplete</a></code></li>
<li><code><a title="egse.dpu.fitsgen.send_request" href="#egse.dpu.fitsgen.send_request">send_request</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="egse.dpu.fitsgen.FITSGenerator" href="#egse.dpu.fitsgen.FITSGenerator">FITSGenerator</a></code></h4>
<ul class="">
<li><code><a title="egse.dpu.fitsgen.FITSGenerator.clear_crucial_parameters" href="#egse.dpu.fitsgen.FITSGenerator.clear_crucial_parameters">clear_crucial_parameters</a></code></li>
<li><code><a title="egse.dpu.fitsgen.FITSGenerator.close_fits" href="#egse.dpu.fitsgen.FITSGenerator.close_fits">close_fits</a></code></li>
<li><code><a title="egse.dpu.fitsgen.FITSGenerator.crucial_parameter_change" href="#egse.dpu.fitsgen.FITSGenerator.crucial_parameter_change">crucial_parameter_change</a></code></li>
<li><code><a title="egse.dpu.fitsgen.FITSGenerator.fill_queue" href="#egse.dpu.fitsgen.FITSGenerator.fill_queue">fill_queue</a></code></li>
<li><code><a title="egse.dpu.fitsgen.FITSGenerator.process_queue" href="#egse.dpu.fitsgen.FITSGenerator.process_queue">process_queue</a></code></li>
<li><code><a title="egse.dpu.fitsgen.FITSGenerator.run" href="#egse.dpu.fitsgen.FITSGenerator.run">run</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="egse.dpu.fitsgen.SynopticsFwdFill" href="#egse.dpu.fitsgen.SynopticsFwdFill">SynopticsFwdFill</a></code></h4>
<ul class="">
<li><code><a title="egse.dpu.fitsgen.SynopticsFwdFill.PHI_CMD" href="#egse.dpu.fitsgen.SynopticsFwdFill.PHI_CMD">PHI_CMD</a></code></li>
<li><code><a title="egse.dpu.fitsgen.SynopticsFwdFill.THETA_CMD" href="#egse.dpu.fitsgen.SynopticsFwdFill.THETA_CMD">THETA_CMD</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="egse.dpu.fitsgen.SynopticsInterp1d" href="#egse.dpu.fitsgen.SynopticsInterp1d">SynopticsInterp1d</a></code></h4>
<ul class="two-column">
<li><code><a title="egse.dpu.fitsgen.SynopticsInterp1d.OGATT" href="#egse.dpu.fitsgen.SynopticsInterp1d.OGATT">OGATT</a></code></li>
<li><code><a title="egse.dpu.fitsgen.SynopticsInterp1d.OGFWC" href="#egse.dpu.fitsgen.SynopticsInterp1d.OGFWC">OGFWC</a></code></li>
<li><code><a title="egse.dpu.fitsgen.SynopticsInterp1d.PHI" href="#egse.dpu.fitsgen.SynopticsInterp1d.PHI">PHI</a></code></li>
<li><code><a title="egse.dpu.fitsgen.SynopticsInterp1d.THETA" href="#egse.dpu.fitsgen.SynopticsInterp1d.THETA">THETA</a></code></li>
<li><code><a title="egse.dpu.fitsgen.SynopticsInterp1d.T_ADC" href="#egse.dpu.fitsgen.SynopticsInterp1d.T_ADC">T_ADC</a></code></li>
<li><code><a title="egse.dpu.fitsgen.SynopticsInterp1d.T_ANALOG" href="#egse.dpu.fitsgen.SynopticsInterp1d.T_ANALOG">T_ANALOG</a></code></li>
<li><code><a title="egse.dpu.fitsgen.SynopticsInterp1d.T_CCD1" href="#egse.dpu.fitsgen.SynopticsInterp1d.T_CCD1">T_CCD1</a></code></li>
<li><code><a title="egse.dpu.fitsgen.SynopticsInterp1d.T_CCD2" href="#egse.dpu.fitsgen.SynopticsInterp1d.T_CCD2">T_CCD2</a></code></li>
<li><code><a title="egse.dpu.fitsgen.SynopticsInterp1d.T_CCD3" href="#egse.dpu.fitsgen.SynopticsInterp1d.T_CCD3">T_CCD3</a></code></li>
<li><code><a title="egse.dpu.fitsgen.SynopticsInterp1d.T_CCD4" href="#egse.dpu.fitsgen.SynopticsInterp1d.T_CCD4">T_CCD4</a></code></li>
<li><code><a title="egse.dpu.fitsgen.SynopticsInterp1d.T_CDS" href="#egse.dpu.fitsgen.SynopticsInterp1d.T_CDS">T_CDS</a></code></li>
<li><code><a title="egse.dpu.fitsgen.SynopticsInterp1d.T_PCB1" href="#egse.dpu.fitsgen.SynopticsInterp1d.T_PCB1">T_PCB1</a></code></li>
<li><code><a title="egse.dpu.fitsgen.SynopticsInterp1d.T_PCB2" href="#egse.dpu.fitsgen.SynopticsInterp1d.T_PCB2">T_PCB2</a></code></li>
<li><code><a title="egse.dpu.fitsgen.SynopticsInterp1d.T_PCB3" href="#egse.dpu.fitsgen.SynopticsInterp1d.T_PCB3">T_PCB3</a></code></li>
<li><code><a title="egse.dpu.fitsgen.SynopticsInterp1d.T_PCB4" href="#egse.dpu.fitsgen.SynopticsInterp1d.T_PCB4">T_PCB4</a></code></li>
<li><code><a title="egse.dpu.fitsgen.SynopticsInterp1d.T_SKYSHROUD" href="#egse.dpu.fitsgen.SynopticsInterp1d.T_SKYSHROUD">T_SKYSHROUD</a></code></li>
<li><code><a title="egse.dpu.fitsgen.SynopticsInterp1d.T_TEB_FEE" href="#egse.dpu.fitsgen.SynopticsInterp1d.T_TEB_FEE">T_TEB_FEE</a></code></li>
<li><code><a title="egse.dpu.fitsgen.SynopticsInterp1d.T_TEB_TOU" href="#egse.dpu.fitsgen.SynopticsInterp1d.T_TEB_TOU">T_TEB_TOU</a></code></li>
<li><code><a title="egse.dpu.fitsgen.SynopticsInterp1d.T_TRP1" href="#egse.dpu.fitsgen.SynopticsInterp1d.T_TRP1">T_TRP1</a></code></li>
<li><code><a title="egse.dpu.fitsgen.SynopticsInterp1d.T_TRP10" href="#egse.dpu.fitsgen.SynopticsInterp1d.T_TRP10">T_TRP10</a></code></li>
<li><code><a title="egse.dpu.fitsgen.SynopticsInterp1d.T_TRP2" href="#egse.dpu.fitsgen.SynopticsInterp1d.T_TRP2">T_TRP2</a></code></li>
<li><code><a title="egse.dpu.fitsgen.SynopticsInterp1d.T_TRP21" href="#egse.dpu.fitsgen.SynopticsInterp1d.T_TRP21">T_TRP21</a></code></li>
<li><code><a title="egse.dpu.fitsgen.SynopticsInterp1d.T_TRP22" href="#egse.dpu.fitsgen.SynopticsInterp1d.T_TRP22">T_TRP22</a></code></li>
<li><code><a title="egse.dpu.fitsgen.SynopticsInterp1d.T_TRP3" href="#egse.dpu.fitsgen.SynopticsInterp1d.T_TRP3">T_TRP3</a></code></li>
<li><code><a title="egse.dpu.fitsgen.SynopticsInterp1d.T_TRP31" href="#egse.dpu.fitsgen.SynopticsInterp1d.T_TRP31">T_TRP31</a></code></li>
<li><code><a title="egse.dpu.fitsgen.SynopticsInterp1d.T_TRP4" href="#egse.dpu.fitsgen.SynopticsInterp1d.T_TRP4">T_TRP4</a></code></li>
<li><code><a title="egse.dpu.fitsgen.SynopticsInterp1d.T_TRP41" href="#egse.dpu.fitsgen.SynopticsInterp1d.T_TRP41">T_TRP41</a></code></li>
<li><code><a title="egse.dpu.fitsgen.SynopticsInterp1d.T_TRP5" href="#egse.dpu.fitsgen.SynopticsInterp1d.T_TRP5">T_TRP5</a></code></li>
<li><code><a title="egse.dpu.fitsgen.SynopticsInterp1d.T_TRP6" href="#egse.dpu.fitsgen.SynopticsInterp1d.T_TRP6">T_TRP6</a></code></li>
<li><code><a title="egse.dpu.fitsgen.SynopticsInterp1d.T_TRP7" href="#egse.dpu.fitsgen.SynopticsInterp1d.T_TRP7">T_TRP7</a></code></li>
<li><code><a title="egse.dpu.fitsgen.SynopticsInterp1d.T_TRP8" href="#egse.dpu.fitsgen.SynopticsInterp1d.T_TRP8">T_TRP8</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="egse.dpu.fitsgen.SynopticsLeaveGaps" href="#egse.dpu.fitsgen.SynopticsLeaveGaps">SynopticsLeaveGaps</a></code></h4>
<ul class="">
<li><code><a title="egse.dpu.fitsgen.SynopticsLeaveGaps.OGSHTTR" href="#egse.dpu.fitsgen.SynopticsLeaveGaps.OGSHTTR">OGSHTTR</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>