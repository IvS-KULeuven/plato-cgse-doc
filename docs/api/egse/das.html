<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>egse.das API documentation</title>
<meta name="description" content="The Data Acquisition System (DAS) is a small application that performs measurements on
different devices …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>egse.das</code></h1>
</header>
<section id="section-intro">
<p>The Data Acquisition System (DAS) is a small application that performs measurements on
different devices.</p>
<p>For the Keithley DAQ6510, the DAS reads the configuration for the Keithley DAQ6510 from the
Configuration Manager and then configures the device. When no Configuration Manager is
available, the DAS can also be started with a filename to read the configuration from. The file
should have the YAML format.</p>
<pre><code>Setup:
    DAQ6510:
        Sensors:
            Temperature:
                TRANSDUCER: FRTD
                &quot;RTD:FOUR&quot;: PT100
                UNIT: KELVIN
</code></pre>
<p>The Data Acquisition System can be started as follows:</p>
<pre><code>$ das
Usage: das.py [OPTIONS] COMMAND [ARGS]...

Options:
  --verbose  print out more info to the terminal.
  --debug    set the logging output to DEBUG mode.
  --help     Show this message and exit.

Commands:
  daq6510  Run the Data Acquisition System for the DAQ6510.
  tcs      Run the Data Acquisition System for the TCS EGSE.
  cdaq     Run the Data Acquisition System for the CDAQ EGSE
</code></pre>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
The Data Acquisition System (DAS) is a small application that performs measurements on
different devices.

For the Keithley DAQ6510, the DAS reads the configuration for the Keithley DAQ6510 from the
Configuration Manager and then configures the device. When no Configuration Manager is
available, the DAS can also be started with a filename to read the configuration from. The file
should have the YAML format.

```
Setup:
    DAQ6510:
        Sensors:
            Temperature:
                TRANSDUCER: FRTD
                &#34;RTD:FOUR&#34;: PT100
                UNIT: KELVIN
```

The Data Acquisition System can be started as follows:

```
$ das
Usage: das.py [OPTIONS] COMMAND [ARGS]...

Options:
  --verbose  print out more info to the terminal.
  --debug    set the logging output to DEBUG mode.
  --help     Show this message and exit.

Commands:
  daq6510  Run the Data Acquisition System for the DAQ6510.
  tcs      Run the Data Acquisition System for the TCS EGSE.
  cdaq     Run the Data Acquisition System for the CDAQ EGSE
```

&#34;&#34;&#34;
import itertools
import logging
import multiprocessing

import rich

multiprocessing.current_process().name = &#34;das&#34;

import re
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import List

import click
import invoke
from prometheus_client import Gauge, Summary
from prometheus_client import start_http_server

from egse.confman import ConfigurationManagerProxy
from egse.control import Failure
from egse.settings import Settings
from egse.setup import Setup
from egse.storage import StorageProxy
from egse.storage import is_storage_manager_active
from egse.storage.persistence import CSV
from egse.system import SignalCatcher
from egse.system import flatten_dict
from egse.tcs.tcs import TCSProxy
from egse.tcs.tcs import is_tcs_cs_active
from egse.tempcontrol.keithley.daq6510 import DAQ6510Proxy
from egse.tempcontrol.keithley.daq6510 import count_number_of_channels
from egse.tempcontrol.keithley.daq6510 import get_channel_names
from egse.tempcontrol.keithley.daq6510_cs import is_daq6510_cs_active
from egse.tempcontrol.srs.ptc10 import ptc10Proxy
from egse.tempcontrol.srs.ptc10_cs import is_ptc10_cs_active

from egse.powermeter.ni.cdaq9184 import cdaq9184Proxy
from egse.powermeter.ni.cdaq9184_cs import is_cdaq9184_cs_active

LOGGER = logging.getLogger(__name__)

DAS = Settings.load(&#34;Data Acquisition System&#34;)


def load_setup_from_input_file(input_file: str):
    &#34;&#34;&#34;Loads a Setup YAML file from disk.&#34;&#34;&#34;
    input_file = Path(input_file).resolve()

    if not input_file.exists():
        click.echo(f&#34;ERROR: Input file ({input_file}) doesn&#39;t exists.&#34;)
        return None

    return Setup.from_yaml_file(input_file)


def load_setup_from_configuration_manager():
    &#34;&#34;&#34;Loads a Setup YAML file from the Configuration Manager.&#34;&#34;&#34;
    with ConfigurationManagerProxy() as cm:
        setup = cm.get_setup()

    return setup


class Config:
    def __init__(self):
        self.verbose = False
        self.debug = False


pass_config = click.make_pass_decorator(Config, ensure=True)


@click.group()
@click.option(&#34;--verbose&#34;, is_flag=True, help=&#34;print out more info to the terminal.&#34;)
@click.option(&#34;--debug&#34;, is_flag=True, help=&#34;set the logging output to DEBUG mode.&#34;)
@pass_config
def cli(config, verbose, debug):
    config.verbose = verbose
    config.debug = debug


@cli.command()
@click.option(
    &#34;--count&#34;, default=None, help=&#34;how many samples should be taken for each measurement&#34;
)
@click.option(
    &#34;--interval&#34;, default=None, help=&#34;what is the time interval between scans [seconds]&#34;
)
@click.option(
    &#34;--delay&#34;, default=None, help=&#34;what is the time delay between measurements [seconds]&#34;
)
@click.option(
    &#34;--channel-list&#34;,
    default=None,
    help=&#39;a channel_list as understood by the device, e.g. &#34;(@101:105)&#34;&#39;,
)
@click.option(
    &#34;--background/--no-background&#34;, &#34;-bg/-no-bg&#34;, default=False,
    help=&#34;start the data acquisition in the background&#34;
)
@click.argument(&#34;input_file&#34;, type=str, required=False)
@pass_config
def daq6510(config, count, interval, delay, channel_list, background: bool, input_file: str):
    &#34;&#34;&#34;
    Run the Data Acquisition System for the DAQ6510.

    INPUT_FILE: YAML file containing the Setup for the DAQ6510 [optional]

    Note: When this command runs in the background, send an INTERRUPT SIGNAL with the kill command
    to terminate. Never send a KILL SIGNAL (9) because then the process will not properly be
    unregistered from the storage manager.

    $ kill -INT &lt;PID&gt;

    &#34;&#34;&#34;

    if background:
        cmd = &#34;das daq6510&#34;
        cmd += f&#34; --count {count}&#34;
        cmd += f&#34; --interval {interval}&#34;
        cmd += f&#34; --delay {delay}&#34;
        cmd += f&#34; --channel-list &#39;{channel_list}&#39;&#34; if channel_list else &#34;&#34;
        cmd += f&#34; {input_file}&#34; if input_file else &#34;&#34;
        LOGGER.info(f&#34;Invoking background command: {cmd}&#34;)
        invoke.run(cmd, disown=True)
        return

    multiprocessing.current_process().name = &#34;das-daq6510&#34;

    if config.debug:
        logging.basicConfig(level=logging.DEBUG, format=Settings.LOG_FORMAT_FULL)

    if not is_daq6510_cs_active():
        LOGGER.error(&#34;The DAQ6510 Control Server is not running, start the &#39;daq6510_cs&#39; command &#34;
                     &#34;before running the data acquisition.&#34;)
        return

    if not is_storage_manager_active():
        LOGGER.error(&#34;The storage manager is not running, start the core services &#34;
                     &#34;before running the data acquisition.&#34;)
        return

    if input_file:
        setup = load_setup_from_input_file(input_file)
    else:
        setup = load_setup_from_configuration_manager()

    if setup is None:
        LOGGER.error(&#34;ERROR: Could not load setup.&#34;)
        sys.exit(1)

    if config.verbose:
        LOGGER.info(setup)

    if &#34;DAQ6510&#34; not in setup.gse:
        LOGGER.error(&#34;ERROR: no DAQ6510 entry in the loaded Setup.&#34;)
        sys.exit(1)

    if not channel_list:
        channel_list = setup.gse.DAQ6510.channels

    if not count:
        count = setup.gse.DAQ6510.route.scan.COUNT.SCAN

    if not interval:
        interval = setup.gse.DAQ6510.route.scan.INTERVAL

    if not delay:
        delay = setup.gse.DAQ6510.route.delay

    count, interval, delay = int(count), int(interval), int(delay)

    channel_count = count_number_of_channels(channel_list)
    channel_names = get_channel_names(channel_list)

    DAQ_METRICS = {}
    for channel in channel_names:
        DAQ_METRICS[channel] = Gauge(f&#34;daq_channel_{channel}&#34;,
                                     f&#34;The current measure for the sensor connected to channel {channel} on the DAQ6510&#34;)

    start_http_server(DAS.METRICS_PORT_DAQ6510)

    # Initialize some variables that will be used for registration to the Storage Manager

    origin = &#34;DAS-DAQ6510&#34;
    persistence_class = CSV
    prep = {
        &#34;mode&#34;: &#34;a&#34;,
        &#34;ending&#34;: &#34;\n&#34;,
        &#34;column_names&#34;: [&#34;Timestamp&#34;, *channel_names],
    }

    killer = SignalCatcher()

    with DAQ6510Proxy() as daq, StorageProxy() as storage:
        daq.reset()

        storage.register({&#34;origin&#34;: origin, &#34;persistence_class&#34;: persistence_class, &#34;prep&#34;: prep})

        storage.save({&#34;origin&#34;: origin, &#34;data&#34;: f&#34;# columns: {channel_names}&#34;})

        for sensor in setup.gse.DAQ6510.sensors:
            for function in setup.gse.DAQ6510.sensors[sensor]:
                sense = {
                    function.upper(): [
                        (key, value)
                        for key, value in flatten_dict(
                            setup.gse.DAQ6510.sensors[sensor][function]
                        ).items()
                        if key != &#34;channels&#34;
                    ]
                }
                function_channel_list = setup.gse.DAQ6510.sensors[sensor][function].channels
                if config.verbose:
                    LOGGER.info(f&#34;{sense=}&#34;)
                    LOGGER.info(f&#34;{function_channel_list=}&#34;)
                daq.configure_sensors(channel_list=function_channel_list, sense=sense)

        LOGGER.info(f&#34;global: {channel_list=}, {channel_count=}&#34;)

        daq.setup_measurements(channel_list=channel_list)

        while True:
            try:
                response = daq.perform_measurement(
                    channel_list=channel_list, count=count, interval=interval
                )

                if killer.term_signal_received:
                    break

                if not response:
                    LOGGER.warning(&#34;Received an empty response from the DAQ6510, &#34;
                                   &#34;check the connection with the device.&#34;)
                    LOGGER.warning(f&#34;Response: {response=}&#34;)
                    time.sleep(1.0)
                    continue

                if isinstance(response, Failure):
                    LOGGER.warning(f&#34;Received a Failure from the DAQ6510 Control Server:&#34;)
                    LOGGER.warning(f&#34;Response: {response}&#34;)
                    time.sleep(1.0)
                    continue

                # Process and save the response

                # LOGGER.debug(f&#34;{response=}&#34;)

                dts = response[0][1].strip()
                datetime_string = datetime.strptime(
                    dts[:-3],
                    &#34;%m/%d/%Y %H:%M:%S.%f&#34;).strftime(&#34;%Y-%m-%dT%H:%M:%S,%f&#34;)

                data = {measure[0]: float(measure[2]) for measure in response}

                data.update({&#34;Timestamp&#34;: datetime_string})

                # FIXME: we probably need to do something with the units...

                units = [measure[3] for measure in response]

                # LOGGER.debug(f&#34;{data=}&#34;)

                storage.save({&#34;origin&#34;: origin, &#34;data&#34;: data})

                # Now extract channels from the response to update the metrics

                for channel in [measure[0] for measure in response]:
                    DAQ_METRICS[channel].set(data[channel])

                # wait for the next measurement to be done (delay)

                time.sleep(delay)

            except KeyboardInterrupt:
                LOGGER.debug(&#34;Interrupt received, terminating...&#34;)
                break
            except Exception as exc:
                LOGGER.warning(f&#34;DAS Exception: {exc}&#34;, exc_info=True)
                LOGGER.warning(&#34;Got a corrupt response from the DAQ6510. &#34;
                               &#34;Check log messages for &#39;DAS Exception&#39;.&#34;)
                time.sleep(1.0)
                continue

        storage.unregister({&#34;origin&#34;: origin})

@cli.command()
@click.option(
    &#34;--background/--no-background&#34;, &#34;-bg/-no-bg&#34;, default=False,
    help=&#34;start the data acquisition in the background&#34;
)
@click.option(
    &#34;--user_regulation&#34;, default=None, help=&#34;activate ptc10 regulation with the given parameter as temperature setpoint (float)&#34;
)
@click.option(
    &#34;--auto_regulation&#34;, is_flag=True, default=None, help=&#34;if this option is given, activate ptc10 regulation with survival mode (i.e. with T_min_NOP as temperature setpoint)&#34;
)
@click.argument(&#34;input_file&#34;, type=str, required=False)
@pass_config
def ptc10(config, user_regulation: float, auto_regulation: int, background: bool, input_file: str):
    &#34;&#34;&#34;
    Run the Data Acquisition System for the DAQ6510.

    INPUT_FILE: YAML file containing the Setup for the DAQ6510 [optional]

    Note: When this command runs in the background, send an INTERRUPT SIGNAL with the kill command
    to terminate. Never send a KILL SIGNAL (9) because then the process will not properly be
    unregistered from the storage manager.

    $ kill -INT &lt;PID&gt;

    &#34;&#34;&#34;

    if background:
        cmd = &#34;das ptc10&#34;
        cm += f&#34;--user_regulation {user_regulation}&#34;
        cm += f&#34;--auto_regulation {auto_regulation}&#34;
        cmd += f&#34; {input_file}&#34; if input_file else &#34;&#34;
        LOGGER.info(f&#34;Invoking background command: {cmd}&#34;)
        invoke.run(cmd, disown=True)
        return

    multiprocessing.current_process().name = &#34;das-ptc10&#34;

    if config.debug:
        logging.basicConfig(level=logging.DEBUG, format=Settings.LOG_FORMAT_FULL)

    if not is_ptc10_cs_active():
        LOGGER.error(&#34;The PTC10 Control Server is not running, start the &#39;ptc10_cs&#39; command &#34;
                     &#34;before running the data acquisition.&#34;)
        return

    if not is_storage_manager_active():
        LOGGER.error(&#34;The storage manager is not running, start the core services &#34;
                     &#34;before running the data acquisition.&#34;)
        return

    if input_file:
        setup = load_setup_from_input_file(input_file)
    else:
        setup = load_setup_from_configuration_manager()

    if setup is None:
        LOGGER.error(&#34;ERROR: Could not load setup.&#34;)
        sys.exit(1)

    if config.verbose:
        LOGGER.info(setup)

    if &#34;PTC10&#34; not in setup.gse:
        LOGGER.error(&#34;ERROR: no PTC10 entry in the loaded Setup.&#34;)
        sys.exit(1)

    channel_names = list(setup.gse.PTC10.channel_names.values())

    # Creation of list NAMES.
    # NAMES will be [&#39;TRP2&#39;, &#39;TRP3&#39;, &#39;TRP4&#39;, &#39;H1_ampere&#39;, &#39;H2_ampere&#39;, &#39;H3_ampere&#39;, &#39;H1_watt&#39;, &#39;H2_watt&#39;, &#39;H3_watt&#39;, &#39;H1_volt&#39;, &#39;H2_volt&#39;, &#39;H3_volt&#39;]
    NAMES = channel_names[:3]
    Units = [&#34;ampere&#34;, &#34;watt&#34;, &#34;volt&#34;]
    for unit in Units:
        NAMES.extend([name + &#34;_&#34; + unit for name in list(setup.gse.PTC10.channel_names.values())[-3:]])

    #Creation of Prometheus METRICS in a dictionnary

    # METRICS for inputs first (TRP 2-3-4)
    PTC_METRICS_inputs = {NAMES[channel] : Gauge(f&#34;ptc_channel_input_{channel + 1}&#34;,
                                                           f&#39;The current measure for the sensor connected to channel &#34;{NAMES[channel]}&#34; on the PTC10&#39;) for channel in [0, 1, 2]}

    #Then METRICS for outputs (3 outputs * 3 units = 9 METRICS for outputs)
    PTC_METRICS_outputs = {}
    i = 0
    for unit in Units:
        for channel in [3, 4, 5]:
            PTC_METRICS_outputs[NAMES[channel + i]] = Gauge(f&#34;ptc_channel_output_{channel - 2}_{unit}&#34;,
                                                            f&#39;The current {unit} for the heater connected to channel &#34;{NAMES[channel][:2]}&#34; on the PTC10&#39;)
        i += 3

    #Then merger in 1 dictionnary of METRICS for inputs and outputs
    PTC_METRICS = {**PTC_METRICS_inputs, **PTC_METRICS_outputs}  # merger of PTC_METRICS_inputs and PTC_METRICS_outputs dictionnaries

    start_http_server(DAS.METRICS_PORT_PTC10)

    # Initialize some variables that will be used for registration to the Storage Manager

    origin = &#34;DAS-PTC10&#34;
    persistence_class = CSV
    prep = {
        &#34;mode&#34;: &#34;a&#34;,
        &#34;ending&#34;: &#34;\n&#34;,
        &#34;column_names&#34;: [&#34;Timestamp&#34;, *NAMES],
    }

    killer = SignalCatcher()

    with ptc10Proxy() as ptc, StorageProxy() as storage:

        storage.register({&#34;origin&#34;: origin, &#34;persistence_class&#34;: persistence_class, &#34;prep&#34;: prep})
        storage.save({&#34;origin&#34;: origin, &#34;data&#34;: f&#34;# columns: {NAMES}&#34;})

        Old_channel_names = ptc.get_names()  # Old_channel_names is a tuple of 2 lists with the names of inputs (3 sensors)
                                             # in the first list and the names of outputs (3 heaters) in the second list
        Old_channel_names = Old_channel_names[0] + Old_channel_names[1]  # Now Old_channel_names is a long list of 6 elements (3 inputs/sensors
                                                                         # and 3 outputs/heaters)


        for old_name, new_name in zip(Old_channel_names, channel_names):
            ptc.set_name(old_name,new_name)

        for output_ch, input in zip([1, 2, 3], setup.gse.PTC10.heater_input):
            new_input = setup.gse.PTC10.heater_input[input]
            ptc.set_heater_input(output_ch, new_input)

        ptc.output_unit(setup.gse.PTC10.heater_unit) #Configure units before limits (because configuring units modifies limits)

        low_limit = setup.gse.PTC10.heater_limit.low
        high_limit = setup.gse.PTC10.heater_limit.high
        for output_ch in [1, 2, 3]:
            ptc.output_limit(output_ch, low_limit, high_limit)

        i = 0
        for pid in setup.gse.PTC10.PID:
            PID_list = list(setup.gse.PTC10.PID[pid].values())
            i += 1
            ptc.set_PID(input_ch=i, output_ch=i, PID=PID_list)

        R = setup.gse.PTC10.heater_Ohm #Resistance of the heater in Ohm

        while True:
            try:

                if user_regulation and auto_regulation:
                    print(&#34;Only 1 option is expected (user_regulation OR auto_regulation) but both were given.\nTerminating... &#34;)
                    break

                if user_regulation: # If user_regulation is given, then run the function set_stable() with &#34;user_regulation&#34; as temperature setpoint
                    try:
                        ptc.set_stable(float(user_regulation))
                    except ValueError:
                        print(&#34;ValueError: user_regulation must be a number.\nTerminating...&#34;)
                        break
                if auto_regulation: # If auto_regulation is given, then run the function set_survival() (i.e. with T_min_NOP as temperature setpoint)
                    ptc.set_survival()

                response = ptc.read_temperature() + ptc.read_heater()[0]

                # Calculation of power : P = RI²
                for i in [3, 4, 5]:
                    response.append(R * response[i]**2)

                # Calculation of voltage : U = RI
                for i in [3, 4, 5]:
                    response.append(R * response[i])

                data = {NAMES[i]: response[i] for i in range(len(NAMES))}

                if killer.term_signal_received:
                    break

                if not response:
                    LOGGER.warning(&#34;Received an empty response from the PTC10, &#34;
                                   &#34;check the connection with the device.&#34;)
                    LOGGER.warning(f&#34;Response: {response=}&#34;)
                    time.sleep(1.0)
                    continue

                if isinstance(response, Failure):
                    LOGGER.warning(f&#34;Received a Failure from the PTC10 Control Server:&#34;)
                    LOGGER.warning(f&#34;Response: {response}&#34;)
                    time.sleep(1.0)
                    continue

                # Process and save the response

                LOGGER.debug(f&#34;{response=}&#34;)

                LOGGER.debug(data)

                storage.save({&#34;origin&#34;: origin, &#34;data&#34;: data})

                # Now extract channels from the response to update the metrics

                for name in NAMES:
                    PTC_METRICS[name].set(data[name])


                # wait for the next measurement to be done

                # time.sleep(2)

            except KeyboardInterrupt:
                LOGGER.debug(&#34;Interrupt received, terminating...&#34;)
                break
            except Exception as exc:
                LOGGER.warning(f&#34;DAS Exception: {exc}&#34;, exc_info=True)
                LOGGER.warning(&#34;Got a corrupt response from the PTC10. &#34;
                               &#34;Check log messages for &#39;DAS Exception&#39;.&#34;)
                time.sleep(1.0)
                continue

        ptc.disable_all()
        storage.unregister({&#34;origin&#34;: origin})


@cli.command()
@click.option(
    &#34;--use-all-hk&#34;, is_flag=True,
    help=(&#34;Use get_all_housekeeping() method to read telemetry from the TCS EGSE. &#34;
          &#34;The device must not be in remote control mode for this.&#34;)
)
@click.option(
    &#34;--interval&#34;, default=10, help=&#34;what is the time delay between measurements [seconds]&#34;
)
@click.option(
    &#34;--background/--no-background&#34;, &#34;-bg/-no-bg&#34;, default=False,
    help=&#34;start the data acquisition in the background&#34;
)
@pass_config
def tcs(config, use_all_hk, interval, background):
    &#34;&#34;&#34;
    Run the Data Acquisition System for the TCS EGSE.

    Note: When this command runs in the background, send an INTERRUPT SIGNAL with the kill command
    to terminate. Never send a KILL SIGNAL (9) because then the process will not properly be
    unregistered from the storage manager.

    $ kill -INT &lt;PID&gt;

    &#34;&#34;&#34;

    rich.print(
        &#34;[red]WARNING[/red]: This function of the DAS has been deprecated and is replaced by the &#34;
        &#34;TCSTelemetry process which is automatically started by the tcs_cs. The `das tcs` will be &#34;
        &#34;removed shortly.&#34;)

    return

    if background:
        cmd = &#34;das tcs&#34;
        cmd += &#34; --use-all-hk&#34; if use_all_hk else &#34;&#34;
        cmd += f&#34; --interval {interval}&#34;
        LOGGER.info(f&#34;Invoking background command: {cmd}&#34;)
        invoke.run(cmd, disown=True)
        return

    multiprocessing.current_process().name = &#34;das-tcs&#34;

    if config.debug:
        logging.basicConfig(level=logging.DEBUG, format=Settings.LOG_FORMAT_FULL)

    start_http_server(DAS.METRICS_PORT_TCS)

    if not is_tcs_cs_active():
        LOGGER.error(&#34;The TCS Control Server is not running, start the &#39;tcs_cs&#39; command &#34;
                     &#34;before running the data acquisition.&#34;)
        return

    if not is_storage_manager_active():
        LOGGER.error(&#34;The storage manager is not running, start the core services &#34;
                     &#34;before running the data acquisition.&#34;)
        return

    killer = SignalCatcher()

    with TCSProxy() as tcs_proxy, StorageProxy() as storage:

        # Not all HK parameters are always sent with the periodic telemetry, so we
        # use the know parameters names. Since we further on pass the values as a dictionary
        # to the storage, values which are not in `names` are silently ignored.

        names = patterns.keys()

        # For each of the names, create also a Timestamp column for that name

        columns = list(itertools.chain.from_iterable((x + &#39;_ts&#39;, x) for x in names))

        # Initialize some variables that will be used for registration to the Storage Manager
        # Use the names in the header of the CSV file as column names.

        origin = &#34;DAS-TCS&#34;
        persistence_class = CSV
        prep = {
            &#34;mode&#34;: &#34;a&#34;,
            &#34;ending&#34;: &#34;\n&#34;,
            &#34;header&#34;: &#34;TCS EGSE First Connection Tests&#34;,
            &#34;column_names&#34;: columns,
        }

        storage.register({&#34;origin&#34;: origin, &#34;persistence_class&#34;: persistence_class, &#34;prep&#34;: prep})

        while True:
            try:
                data = tcs_proxy.get_all_housekeeping() if use_all_hk else tcs_proxy.get_data()
                if killer.term_signal_received:
                    break

                if isinstance(data, Failure):
                    LOGGER.warning(f&#34;Received a Failure from the TCS EGSE Control Server:&#34;)
                    LOGGER.warning(f&#34;Response: {data}&#34;)
                    time.sleep(1.0)
                    continue

                LOGGER.debug(f&#34;received {len(data or [])} data items&#34;)

                if data is None:
                    continue

                data = process_data(data)
                storage.save({&#34;origin&#34;: origin, &#34;data&#34;: data})

                time.sleep(interval)

            except KeyboardInterrupt:
                LOGGER.debug(&#34;Interrupt received, terminating...&#34;)
                break

        storage.unregister({&#34;origin&#34;: origin})


TCS_AMBIENT_RTD = Gauge(
    &#34;tcs_ambient_rtd&#34;, &#34;The current ambient temperature on the TCS EGSE&#34;
)
TCS_INTERNAL_RTD = Gauge(
    &#34;tcs_internal_rtd&#34;, &#34;The current internal temperature off the TCS EGSE&#34;
)
TCS_FEE_RTD_1 = Gauge(
    &#34;tcs_fee_rtd_1&#34;, &#34;The TRP22 sensor for control channel 1&#34;
)
TCS_FEE_RTD_2 = Gauge(
    &#34;tcs_fee_rtd_2&#34;, &#34;The TRP22 sensor for control channel 2&#34;
)
TCS_FEE_RTD_3 = Gauge(
    &#34;tcs_fee_rtd_3&#34;, &#34;The TRP22 sensor for control channel 3&#34;
)
TCS_TOU_RTD_1 = Gauge(
    &#34;tcs_tou_rtd_1&#34;, &#34;The TRP1 sensor for control channel 1&#34;
)
TCS_TOU_RTD_2 = Gauge(
    &#34;tcs_tou_rtd_2&#34;, &#34;The TRP1 sensor for control channel 2&#34;
)
TCS_TOU_RTD_3 = Gauge(
    &#34;tcs_tou_rtd_3&#34;, &#34;The TRP1 sensor for control channel 3&#34;
)
TCS_CH1_IOUT = Gauge(
    &#34;tcs_ch1_iout&#34;, &#34;channel 1 iout&#34;
)
TCS_CH1_POUT = Gauge(
    &#34;tcs_ch1_pout&#34;, &#34;channel 1 pout&#34;
)
TCS_CH1_VOUT = Gauge(
    &#34;tcs_ch1_vout&#34;, &#34;channel 1 pout&#34;
)
TCS_CH2_IOUT = Gauge(
    &#34;tcs_ch2_iout&#34;, &#34;channel 2 iout&#34;
)
TCS_CH2_POUT = Gauge(
    &#34;tcs_ch2_pout&#34;, &#34;channel 2 pout&#34;
)
TCS_CH2_VOUT = Gauge(
    &#34;tcs_ch2_vout&#34;, &#34;channel 2 pout&#34;
)


def process_data(data: List) -&gt; dict:
    &#34;&#34;&#34;
    Process the output of the `get_data()` and the `get_all_housekeeping()` commands. Telemetry
    parameters can occur multiple times, only the last entry is retained.

    This function also updates the metrics that are requested by Prometheus.

    Args:
        data: the data as returned by get_data() and get_all_housekeeping().

    Returns:
        an up-to-date dictionary with the parameter values and their timestamps.
    &#34;&#34;&#34;

    processed_data = {}

    # Create a proper dictionary with the last updated telemetry values.
    # We expect 3 entries: name, date, and value

    for item in data:

        if len(item) == 3:
            value = extract_value(item[0], item[2])
            processed_data.update({f&#34;{item[0]}_ts&#34;: item[1], item[0]: value})
        else:
            click.echo(&#34;WARNING: incorrect format in data response from TCS EGSE.&#34;)

    # Fill in the metrics that will be monitored by Prometheus

    for name, metric in (
            (&#34;ambient_rtd&#34;, TCS_AMBIENT_RTD),
            (&#34;internal_rtd&#34;, TCS_INTERNAL_RTD),
            (&#34;fee_rtd_1&#34;, TCS_FEE_RTD_1),
            (&#34;fee_rtd_2&#34;, TCS_FEE_RTD_2),
            (&#34;fee_rtd_3&#34;, TCS_FEE_RTD_3),
            (&#34;tou_rtd_1&#34;, TCS_TOU_RTD_1),
            (&#34;tou_rtd_2&#34;, TCS_TOU_RTD_2),
            (&#34;tou_rtd_3&#34;, TCS_TOU_RTD_3),
            (&#34;ch1_iout&#34;, TCS_CH1_IOUT),
            (&#34;ch1_pout&#34;, TCS_CH1_POUT),
            (&#34;ch1_vout&#34;, TCS_CH1_VOUT),
            (&#34;ch2_iout&#34;, TCS_CH2_IOUT),
            (&#34;ch2_pout&#34;, TCS_CH2_POUT),
            (&#34;ch2_vout&#34;, TCS_CH2_VOUT),
    ):
        if name in processed_data:
            metric.set(processed_data[name])

    return processed_data


# Define different regex patterns, e.g. for temperature, time, power, etc.

temperature_pattern = re.compile(r&#39;(.*) ºC&#39;)
seconds_pattern = re.compile(r&#39;(.*) s&#39;)
milliseconds_pattern = re.compile(r&#39;(.*) ms&#39;)
current_pattern = re.compile(r&#39;(.*) A \[(.*) Apk\]&#39;)
voltage_pattern = re.compile(r&#39;(.*) V&#39;)
voltage_peak_pattern = re.compile(r&#39;(.*) V \[(.*) Vpk\]&#39;)
power_pattern = re.compile(r&#39;(.*) mW \[(.*) mWavg\]&#39;)
storage_pattern = re.compile(r&#39;\[(.*)\]&#39;)
match_all_pattern = re.compile(r&#39;(.*)&#39;)

# Assign parsing patterns to each of the parameters that need specific parsing.

patterns = {
    &#39;ambient_rtd&#39;: temperature_pattern,
    &#39;ch1_clkheater_period&#39;: milliseconds_pattern,
    &#39;ch1_clkheater_ticks&#39;: seconds_pattern,
    &#39;ch1_iout&#39;: current_pattern,
    &#39;ch1_pid_proctime&#39;: seconds_pattern,
    &#39;ch1_pid_sp&#39;: temperature_pattern,
    &#39;ch1_pid_ts&#39;: seconds_pattern,
    &#39;ch1_pout&#39;: power_pattern,
    &#39;ch1_pwm_ontime&#39;: milliseconds_pattern,
    &#39;ch1_pwm_proctime&#39;: seconds_pattern,
    &#39;ch1_tav&#39;: temperature_pattern,
    &#39;ch1_vdc&#39;: voltage_pattern,
    &#39;ch1_vout&#39;: voltage_peak_pattern,
    &#39;ch2_clkheater_period&#39;: milliseconds_pattern,
    &#39;ch2_clkheater_ticks&#39;: seconds_pattern,
    &#39;ch2_iout&#39;: current_pattern,
    &#39;ch2_pid_proctime&#39;: seconds_pattern,
    &#39;ch2_pid_sp&#39;: temperature_pattern,
    &#39;ch2_pid_ts&#39;: seconds_pattern,
    &#39;ch2_pout&#39;: power_pattern,
    &#39;ch2_pwm_ontime&#39;: milliseconds_pattern,
    &#39;ch2_pwm_proctime&#39;: seconds_pattern,
    &#39;ch2_tav&#39;: temperature_pattern,
    &#39;ch2_vdc&#39;: voltage_pattern,
    &#39;ch2_vout&#39;: voltage_peak_pattern,
    &#39;fee_rtd_1&#39;: temperature_pattern,
    &#39;fee_rtd_2&#39;: temperature_pattern,
    &#39;fee_rtd_3&#39;: temperature_pattern,
    &#39;fee_rtd_tav&#39;: temperature_pattern,
    &#39;internal_rtd&#39;: temperature_pattern,
    &#39;ni9401_external_clkheater_period&#39;: seconds_pattern,
    &#39;ni9401_external_clkheater_timeout&#39;: seconds_pattern,
    &#39;psu_vdc&#39;: voltage_pattern,
    &#39;spare_rtd_1&#39;: temperature_pattern,
    &#39;spare_rtd_2&#39;: temperature_pattern,
    &#39;spare_rtd_3&#39;: temperature_pattern,
    &#39;spare_rtd_tav&#39;: temperature_pattern,
    &#39;storage_mmi&#39;: storage_pattern,
    &#39;storage_realtime&#39;: storage_pattern,
    &#39;tou_rtd_1&#39;: temperature_pattern,
    &#39;tou_rtd_2&#39;: temperature_pattern,
    &#39;tou_rtd_3&#39;: temperature_pattern,
    &#39;tou_rtd_tav&#39;: temperature_pattern,
}


def extract_value(key, value):
    &#34;&#34;&#34;
    Extract the actual value from the string containing the value and unit plus potential
    additional info. Parsing is done with dedicated regular expressions per parameter, e.g.
    parsing a temperature takes the &#39;ºC&#39; into account when extracting the actual value.

    Args:
        key (str): name of the parameter
        value (str): the value as returned by the TCS EGSE
    &#34;&#34;&#34;

    if key not in patterns:
        return value

    match = patterns[key].search(value)
    if match is not None:
        value = match.group(1)
    return value

@cli.command()
@click.option(
    &#34;--use-all-hk&#34;, is_flag=True,
    help=(&#34;Use get_all_housekeeping() method to read telemetry from the CDAQ. &#34;
          &#34;The device must not be in remote control mode for this.&#34;)
)
@click.option(
    &#34;--interval&#34;, default=1, help=&#34;what is the time delay between measurements [seconds]&#34;
)
@click.option(
    &#34;--background/--no-background&#34;, &#34;-bg/-no-bg&#34;, default=False,
    help=&#34;start the data acquisition in the background&#34;
)
@pass_config
def cdaq(config, use_all_hk, interval, background):
    &#34;&#34;&#34;
    Run the Data Acquisition System for the CDAQ.

    INPUT_FILE: YAML file containing the Setup for the CDAQ [optional]

    Note: When this command runs in the background, send an INTERRUPT SIGNAL with the kill command
    to terminate. Never send a KILL SIGNAL (9) because then the process will not properly be
    unregistered from the storage manager.

    $ kill -INT &lt;PID&gt;

    &#34;&#34;&#34;

    if background:
        cmd = &#34;das cdaq&#34;
        cmd += &#34; --use-all-hk&#34; if use_all_hk else &#34;&#34;
        cmd += f&#34; --interval {interval}&#34;
        LOGGER.info(f&#34;Invoking background command: {cmd}&#34;)
        invoke.run(cmd, disown=True)
        return

    multiprocessing.current_process().name = &#34;das-cdaq&#34;

    if config.debug:
        logging.basicConfig(level=logging.DEBUG, format=Settings.LOG_FORMAT_FULL)

    if not is_cdaq9184_cs_active():
        LOGGER.error(&#34;The cdaq Control Server is not running, start the &#39;cdaq_cs&#39; command &#34;
                     &#34;before running the data acquisition.&#34;)
        return

    if not is_storage_manager_active():
        LOGGER.error(&#34;The storage manager is not running, start the core services &#34;
                     &#34;before running the data acquisition.&#34;)
        return

    # Channel names for tall the HK, Here we only take the mean values (dismissing stddev values)

    channel_names = [&#34;Photodiode_1&#34;, &#34;Photodiode_2&#34;, &#34;Collimator_Temp_1&#34;, &#34;Collimator_Temp_2&#34;, &#34;Sphere_Temp&#34;]

    # GRAFANA/PROMETHEUS METRICS
    DAQ_METRICS = {}
    for channel in channel_names:
        DAQ_METRICS[channel] = Gauge(f&#34;cdaq_{channel}&#34;,
                                     f&#34;The current measure for the sensor connected to channel {channel} on the CDAQ&#34;)


    start_http_server(DAS.METRICS_PORT_CDAQ)

    # Initialize some variables that will be used for registration to the Storage Manager
    # the channel name units for the photodiodes are V, and C for the temperature sensors

    origin = &#34;DAS-CDAQ&#34;
    persistence_class = CSV
    prep = {
        &#34;mode&#34;: &#34;a&#34;,
        &#34;ending&#34;: &#34;\n&#34;,
        &#34;header&#34;: &#34;CDAQ First Connection Tests&#34;,
        &#34;column_names&#34;: [&#34;Daytime&#34;, &#34;Timestamp&#34;, *channel_names],
    }

    killer = SignalCatcher()

    with cdaq9184Proxy() as cdaq, StorageProxy() as storage:

        # Use the names in the header of the CSV file as column names.

        storage.register({&#34;origin&#34;: origin, &#34;persistence_class&#34;: persistence_class, &#34;prep&#34;: prep})

        while True:
            try:
                response = cdaq.read_values()
                # Here we only take the mean values (dismissing stddev values)
                values = [v for i, v in enumerate(response[2:]) if i % 2 == 0]
                del response[2:]
                response = response + values

                if killer.term_signal_received:
                    break
                if not response:
                    LOGGER.warning(&#34;Received an empty response from the CDAQ, &#34;
                       &#34;check the connection with the device.&#34;)
                    LOGGER.warning(f&#34;Response: {response=}&#34;)
                    time.sleep(1.0)
                    continue
                if isinstance(response, Failure):
                    LOGGER.warning(f&#34;Received a Failure from the CDAQ Control Server:&#34;)
                    LOGGER.warning(f&#34;Response: {response}&#34;)
                    time.sleep(1.0)
                    continue

                LOGGER.info(f&#34;Response: {response=}&#34;)
                storage.save({&#34;origin&#34;: origin, &#34;data&#34;: response})

#demo

                for i in range(len(channel_names)):
                    DAQ_METRICS[channel_names[i]].set(values[i])

                time.sleep(interval)

            except KeyboardInterrupt:
                LOGGER.debug(&#34;Interrupt received, terminating...&#34;)
                break

            except Exception as exc:
                LOGGER.warning(f&#34;DAS Exception: {exc}&#34;, exc_info=True)
                LOGGER.warning(&#34;Got a corrupt response from the CDAQ. &#34;
                               &#34;Check log messages for &#39;DAS Exception&#39;.&#34;)
                time.sleep(1.0)
                continue

        storage.unregister({&#34;origin&#34;: origin})


if __name__ == &#34;__main__&#34;:
    cli()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="egse.das.extract_value"><code class="name flex">
<span>def <span class="ident">extract_value</span></span>(<span>key, value)</span>
</code></dt>
<dd>
<div class="desc"><p>Extract the actual value from the string containing the value and unit plus potential
additional info. Parsing is done with dedicated regular expressions per parameter, e.g.
parsing a temperature takes the 'ºC' into account when extracting the actual value.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>key</code></strong> :&ensp;<code>str</code></dt>
<dd>name of the parameter</dd>
<dt><strong><code>value</code></strong> :&ensp;<code>str</code></dt>
<dd>the value as returned by the TCS EGSE</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_value(key, value):
    &#34;&#34;&#34;
    Extract the actual value from the string containing the value and unit plus potential
    additional info. Parsing is done with dedicated regular expressions per parameter, e.g.
    parsing a temperature takes the &#39;ºC&#39; into account when extracting the actual value.

    Args:
        key (str): name of the parameter
        value (str): the value as returned by the TCS EGSE
    &#34;&#34;&#34;

    if key not in patterns:
        return value

    match = patterns[key].search(value)
    if match is not None:
        value = match.group(1)
    return value</code></pre>
</details>
</dd>
<dt id="egse.das.load_setup_from_configuration_manager"><code class="name flex">
<span>def <span class="ident">load_setup_from_configuration_manager</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads a Setup YAML file from the Configuration Manager.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_setup_from_configuration_manager():
    &#34;&#34;&#34;Loads a Setup YAML file from the Configuration Manager.&#34;&#34;&#34;
    with ConfigurationManagerProxy() as cm:
        setup = cm.get_setup()

    return setup</code></pre>
</details>
</dd>
<dt id="egse.das.load_setup_from_input_file"><code class="name flex">
<span>def <span class="ident">load_setup_from_input_file</span></span>(<span>input_file: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads a Setup YAML file from disk.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_setup_from_input_file(input_file: str):
    &#34;&#34;&#34;Loads a Setup YAML file from disk.&#34;&#34;&#34;
    input_file = Path(input_file).resolve()

    if not input_file.exists():
        click.echo(f&#34;ERROR: Input file ({input_file}) doesn&#39;t exists.&#34;)
        return None

    return Setup.from_yaml_file(input_file)</code></pre>
</details>
</dd>
<dt id="egse.das.process_data"><code class="name flex">
<span>def <span class="ident">process_data</span></span>(<span>data: List[~T]) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Process the output of the <code>get_data()</code> and the <code>get_all_housekeeping()</code> commands. Telemetry
parameters can occur multiple times, only the last entry is retained.</p>
<p>This function also updates the metrics that are requested by Prometheus.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>the data as returned by get_data() and get_all_housekeeping().</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>an up-to-date dictionary with the parameter values and their timestamps.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_data(data: List) -&gt; dict:
    &#34;&#34;&#34;
    Process the output of the `get_data()` and the `get_all_housekeeping()` commands. Telemetry
    parameters can occur multiple times, only the last entry is retained.

    This function also updates the metrics that are requested by Prometheus.

    Args:
        data: the data as returned by get_data() and get_all_housekeeping().

    Returns:
        an up-to-date dictionary with the parameter values and their timestamps.
    &#34;&#34;&#34;

    processed_data = {}

    # Create a proper dictionary with the last updated telemetry values.
    # We expect 3 entries: name, date, and value

    for item in data:

        if len(item) == 3:
            value = extract_value(item[0], item[2])
            processed_data.update({f&#34;{item[0]}_ts&#34;: item[1], item[0]: value})
        else:
            click.echo(&#34;WARNING: incorrect format in data response from TCS EGSE.&#34;)

    # Fill in the metrics that will be monitored by Prometheus

    for name, metric in (
            (&#34;ambient_rtd&#34;, TCS_AMBIENT_RTD),
            (&#34;internal_rtd&#34;, TCS_INTERNAL_RTD),
            (&#34;fee_rtd_1&#34;, TCS_FEE_RTD_1),
            (&#34;fee_rtd_2&#34;, TCS_FEE_RTD_2),
            (&#34;fee_rtd_3&#34;, TCS_FEE_RTD_3),
            (&#34;tou_rtd_1&#34;, TCS_TOU_RTD_1),
            (&#34;tou_rtd_2&#34;, TCS_TOU_RTD_2),
            (&#34;tou_rtd_3&#34;, TCS_TOU_RTD_3),
            (&#34;ch1_iout&#34;, TCS_CH1_IOUT),
            (&#34;ch1_pout&#34;, TCS_CH1_POUT),
            (&#34;ch1_vout&#34;, TCS_CH1_VOUT),
            (&#34;ch2_iout&#34;, TCS_CH2_IOUT),
            (&#34;ch2_pout&#34;, TCS_CH2_POUT),
            (&#34;ch2_vout&#34;, TCS_CH2_VOUT),
    ):
        if name in processed_data:
            metric.set(processed_data[name])

    return processed_data</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="egse.das.Config"><code class="flex name class">
<span>class <span class="ident">Config</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Config:
    def __init__(self):
        self.verbose = False
        self.debug = False</code></pre>
</details>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="egse" href="index.html">egse</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="egse.das.extract_value" href="#egse.das.extract_value">extract_value</a></code></li>
<li><code><a title="egse.das.load_setup_from_configuration_manager" href="#egse.das.load_setup_from_configuration_manager">load_setup_from_configuration_manager</a></code></li>
<li><code><a title="egse.das.load_setup_from_input_file" href="#egse.das.load_setup_from_input_file">load_setup_from_input_file</a></code></li>
<li><code><a title="egse.das.process_data" href="#egse.das.process_data">process_data</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="egse.das.Config" href="#egse.das.Config">Config</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>