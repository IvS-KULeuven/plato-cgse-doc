[#stories]
== Stories
:imagesdir: ../images

This section contains short stories of debugging sessions and working with the system that are otherwise difficult to explain in the previous sections.

=== Seemingly unrelated Process Manager Crash

Related to issue: no issue

The filter wheel control server (`fw8smc4_cs`) didn't start when using the process manager GUI (`pm_ui`). When the button was clicked nothing happened, but we got some error messages in the logger:

image::stories-process-manager-crash.png[]

There was no clear error message except that the PM was already registered at the storage manager. We thought that could be due to the fact the pm_ui crashed already several times before, or maybe a STORAGE_MNEMONIC in the Settings that was not set correctly. So we checked the Settings YAML file, and also the local settings file. The command to do that is:

----
python -m egse.settings
----

We looked at the Setups and inspected the source of the filter wheel control server, but could not relate the problem to anything in the code. Especially, since the control server could be started without problem from the terminal.

----
fw8smc4_cs start
----

So we started to look into all kinds of settings that were important when starting a control server from the process manager, e.g. the DEVICE_SETTINGS variable that is expected in the module and should contain the definition of the ControlServer. XXXXX: xref to document/section explaining this. We fixed a number of these things, see for example PR #1963, but still the problem remained.

We were still confused about the message that the PM was already registered at the Storage Manager. Why does this appear here? Why would the process manager try to register again. Looking at the debug messages before the error, we saw that the process manager actually restarted. That's of course the reason why it tries to register to the Storage, and the fact that it is already registered is probably due to the fact that the process manager crashed. So, instructing the `pm_cs` to start the `fw8smc4_cs` (by clicking the button in the `pm_ui`) crashes the process manager. And it is of course immediately restarted by the Systemd services.

So, we checked the logging messages of the Systemd for the `pm_cs.services`:

[%nowrap]
----
May 24 14:46:35 plato-arrakis pm_cs[2526576]: libximc.so: cannot open shared object file: No such file or directory
May 24 14:46:35 plato-arrakis pm_cs[2526576]: Can't load libximc library. Please add all shared libraries to the appropriate places. It is decribed in detail in developers' documentation. On Linux make sure you installed libximc-dev package.
May 24 14:46:35 plato-arrakis pm_cs[2526576]: make sure that the architecture of the system and the interpreter is the same
May 24 14:46:35 plato-arrakis pm_cs[2526576]: System Exit with code None.
----

The problem seems to be that the `pm_cs` can not load the `libximc` shared library which is needed for the filter wheel control server. This works in the terminal, because there the `LD_LIBRARY_PATH` is set, but it is not known to the process manager control server. To fix this, the environment variable must be set in the `/cgse/env.txt` file that is used in the services file to start the `pm_cs.services`. The content of the files:

((PYTHONPATH))
(((PLATO_LOCAL_SETTINGS)))
((PLATO_CONF_DATA_LOCATION))
((PLATO_CONF_REPO_LOCATION))
((PLATO_DATA_STORAGE_LOCATION))
((PLATO_LOG_FILE_LOCATION))
((LD_LIBRARY_PATH))

[%nowrap]
----
[plato-data@plato-arrakis ~]$ cat /cgse/env.txt
PYTHONPATH=/cgse/lib/python/
PLATO_LOCAL_SETTINGS=/cgse/local_settings.yaml
PLATO_CONF_DATA_LOCATION=/data/IAS/conf
PLATO_CONF_REPO_LOCATION=/home/plato-data/git/plato-cgse-conf
PLATO_DATA_STORAGE_LOCATION=/data/IAS
PLATO_LOG_FILE_LOCATION=/data/IAS/log
LD_LIBRARY_PATH=/home/plato-data/git/plato-common-egse/src/egse/lib/ximc/libximc.framework
----

and

[%nowrap]
----
[plato-data@plato-arrakis ~]$ cat /etc/systemd/system/pm_cs.service
[Unit]
Description=Process Manager Control Server
After=network-online.target cm_cs.service

[Service]
Type=simple
Restart=always
RestartSec=3
User=plato-data
Group=plato-data
EnvironmentFile=/cgse/env.txt
WorkingDirectory=/home/plato-data/workdir
ExecStartPre=/bin/sleep 3
ExecStart=/cgse/bin/pm_cs start

[Install]
Alias=pm_cs.service
WantedBy=multi-user.target
----

After this fix, the `fw8smc4_cs` could be started from the process manager GUI.

=== A non-closed socket is deeply buried in the code

So, I let myself work on a side track today and yesterday. I was doing some monitoring tests with the dummy control server and noticed that the Dummy CS would not properly terminate. It was hanging and it was not clear why.

I went through the whole chain of calls to `close()` functions on all sockets to find out if there was one still missing and if maybe I should use the `linger=0` option etc. I placed log messages before and after all these actions, used different methods to stop and start sub processes, ... It took me way too long to find out...

When I put logging statements around the `term()` call for the ZeroMQ context of the control server, I noticed these message never appeared. Thinking therefore the problem I was trying to solve would appear before these statements was again wrong. The reason they did not appear was that I close down all ZeroMQ logging handlers before I terminate the context, the logging level for the standard logger was set to INFO instead of DEBUG.
[source,python]
----
close_all_zmq_handlers()

logging.debug("Terminating the ZeroMQ Context.")
self.zcontext.term()
logging.debug("-bye-")
----
So, I will have to use `logging.info` there instead.

Since the control server was hanging when terminating the ZeroMQ context,  checking if some port number was still open sounded like a good idea. I found that a connection to the configuration manager commanding port (6000) was still open for the `dummy_cs` process (PID 36619 at that time):
----
$  lsof -i -P -n |grep 36619
python3.8 36619  rik   27u     IPv4 0x283fbd58b2f1ab9d        0t0                 TCP 127.0.0.1:58869->127.0.0.1:6000 (ESTABLISHED)
----
So, somewhere there must be a connection to the configuration control server that was not closed properly. I started searching the code and finally came to this piece of art (ðŸ™„) in the `control.py` module.
[source,python]
----
endpoint = proxy().get_endpoint()
if not is_control_server_active(endpoint, timeout=0.5):
    MODULE_LOGGER.warning(f"The {endpoint} endpoint is not responding, {listener['name']} not un-registered.")
    return
----
This is in the `unregister_as_listener()` method of the `ControlServer` class and the idea behind this code was to quickly check if the CM CS was active before sending a command to it that takes more time (because we use 5 retries). Turns out to be a bad idea for several reasons.

1. We want to use the `is_control_server_active()` function, but this needs the `endpoint` for the CS to contact,
2. we don't have the endpoint, but we can fetch it from the CS using the Proxy (!), so we used the Proxy to get the `endpoint`,
3. then we check if the CS is active, basically opening another Proxy and doing a `ping()`, when its not active, we return otherwise we proceed.
4. In both cases we forget to close/disconnect the Proxy (!)

NOTE: When possible, you should always use the Proxy as a context manager, the socket will then automatically be closed when leaving the context!

So, why not directly using the Proxy with a short timeout. The Proxy does a `ping()` when entering the context. So, I came up with this snippet for now:
[source,python]
----
try:
    with proxy(timeout=100) as x:
        pass
except ConnectionError as exc:
    MODULE_LOGGER.warning(
        f"The {proxy.__class__.__name__} endpoint is not responding, {listener['name']} not un-registered."    )
    return
----
Now, the next question is if we do need this at all. In the `register_as_listener()` method, we don't first check if the control server is active, so why did I think it is necessary here? There are some comments in the code that try to clarify this, but I will spill the beans, the `unregister...` method is usually called when shutting down the services and it might be that the configuration manager is already shut down at the time we want to unregister.

[NOTE]
====
This solution has a drawback. Not all subclasses of Proxy allow to pass the timeout during construction, so when the `proxy` variable
contains a sub-class that doesn't have a timeout argument, as `NameError` will be raised.

----
NameError: name 'timeout' is not defined
----

Actually, most of the Proxy subclasses do not have the timeout parameter, so we will leave that out.
====
